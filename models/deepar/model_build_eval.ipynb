{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87711f77-5d3d-449a-b73e-aabe009f375a",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "389c1fc3-0900-43f7-a5fa-1c1e6e5f123a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fd585bc-007e-4745-aa65-2251e4bf0049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3da1f01d-4a2f-43eb-8344-31e2afbcb59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c2a5e80-0a8d-42fc-aba1-52a04435a25c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_bucket = \"sand-test-central-481423469601-us-east-1\"\n",
    "s3_prefix = \"deepar_model\"\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "#role = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd1ea93-57cf-475a-8a15-1534eae01e31",
   "metadata": {},
   "source": [
    "### Trip Start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ed3bab-0934-4870-8118-5f40b0ea2943",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27e3dd5a-2db3-4b00-930a-b7e52947e4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "s3_data_path = \"s3://{}/{}/data_start_poc\".format(s3_bucket, s3_prefix)\n",
    "s3_output_path = \"s3://{}/{}/output_start_poc\".format(s3_bucket, s3_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "843c20d6-8fb9-4bbf-a71e-9b1a30ae19f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = sagemaker.image_uris.retrieve(\"forecasting-deepar\", region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8f17a69-3ae0-45ea-827f-25d632d73023",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    image_uri = image_name,\n",
    "    sagemaker_session = sagemaker_session,\n",
    "    role = role,\n",
    "    instance_count = 1,\n",
    "    instance_type = \"ml.c5.2xlarge\",\n",
    "    base_job_name = \"deepar-poc-start\",\n",
    "    output_path = s3_output_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62e4ddd5-c6f9-4020-93cb-34041454b689",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = \"15min\"\n",
    "context_length = 4 * 24 * 3\n",
    "prediction_length = 4 * 24 * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9ca48c4-2da9-44ad-9a59-037c8436ddc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"time_freq\": freq,\n",
    "    \"epochs\": \"400\",\n",
    "    \"early_stopping_patience\": \"40\",\n",
    "    \"mini_batch_size\": \"64\",\n",
    "    \"learning_rate\": \"5E-4\",\n",
    "    \"context_length\": str(context_length),\n",
    "    \"prediction_length\": str(prediction_length),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab3ae3e7-ae61-4b8f-9272-e1e8084c26c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba2b73c2-3331-41dd-8829-a439c086fb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-19 16:11:43 Starting - Starting the training job...\n",
      "2022-10-19 16:12:06 Starting - Preparing the instances for trainingProfilerReport-1666195902: InProgress\n",
      ".........\n",
      "2022-10-19 16:13:28 Downloading - Downloading input data...\n",
      "2022-10-19 16:14:07 Training - Downloading the training image..............\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.8/site-packages/mxnet/model.py:78: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if num_device is 1 and 'dist' not in kvstore:\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.8/site-packages/jsonref.py:8: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "  from collections import Mapping, MutableMapping, Sequence\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:16:25 INFO 139946820503360] Reading default configuration from /opt/amazon/lib/python3.8/site-packages/algorithm/resources/default-input.json: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '', 'embedding_dimension': '10', 'learning_rate': '0.001', 'likelihood': 'student-t', 'mini_batch_size': '128', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]'}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:16:25 INFO 139946820503360] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'context_length': '288', 'early_stopping_patience': '40', 'epochs': '400', 'learning_rate': '5E-4', 'mini_batch_size': '64', 'prediction_length': '288', 'time_freq': '15min'}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:16:25 INFO 139946820503360] Final configuration: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '40', 'embedding_dimension': '10', 'learning_rate': '5E-4', 'likelihood': 'student-t', 'mini_batch_size': '64', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', 'context_length': '288', 'epochs': '400', 'prediction_length': '288', 'time_freq': '15min'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:16:25 INFO 139946820503360] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:16:25 INFO 139946820503360] Using early stopping with patience 40\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:16:25 INFO 139946820503360] random_seed is None\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:16:25 INFO 139946820503360] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/train_poc_start.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:16:25 INFO 139946820503360] [num_dynamic_feat=auto] `dynamic_feat` field was NOT found in the file `/opt/ml/input/data/train/train_poc_start.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:16:28 INFO 139946820503360] Training set statistics:\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:16:28 INFO 139946820503360] Integer time series\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:16:28 INFO 139946820503360] number of time series: 451\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:16:28 INFO 139946820503360] number of observations: 28714289\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:16:28 INFO 139946820503360] mean target length: 63668.046563192904\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:16:28 INFO 139946820503360] min/mean/max target: 0.0/0.20894886166256807/45.0\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:16:28 INFO 139946820503360] mean abs(target): 0.20894886166256807\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:16:28 INFO 139946820503360] contains missing values: no\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:16:28 INFO 139946820503360] Small number of time series. Doing 2 passes over dataset with prob 0.7095343680709535 per epoch.\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:16:28 INFO 139946820503360] Test set statistics:\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:16:28 INFO 139946820503360] Integer time series\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:16:28 INFO 139946820503360] number of time series: 451\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:16:28 INFO 139946820503360] number of observations: 129888\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:16:28 INFO 139946820503360] mean target length: 288.0\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:16:28 INFO 139946820503360] min/mean/max target: 0.0/0.4611049519586105/28.0\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:16:28 INFO 139946820503360] mean abs(target): 0.4611049519586105\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:16:28 INFO 139946820503360] contains missing values: no\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.8/site-packages/algorithm/core/date_feature_set.py:44: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)\n",
      "  return index.weekofyear / 51.0 - 0.5\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:16:28 INFO 139946820503360] #memory_usage::<batchbuffer> = 83.69873046875 mb\u001b[0m\n",
      "\u001b[34m/opt/amazon/python3.8/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:16:28 INFO 139946820503360] nvidia-smi: took 0.029 seconds to run.\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:16:28 INFO 139946820503360] nvidia-smi identified 0 GPUs.\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:16:28 INFO 139946820503360] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:16:28 INFO 139946820503360] Create Store: local\u001b[0m\n",
      "\n",
      "2022-10-19 16:16:27 Training - Training image download completed. Training in progress.\u001b[34m#metrics {\"StartTime\": 1666196188.7698762, \"EndTime\": 1666196193.579149, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 4806.876182556152, \"count\": 1, \"min\": 4806.876182556152, \"max\": 4806.876182556152}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:16:33 INFO 139946820503360] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:16:36 INFO 139946820503360] #memory_usage::<model> = 257 mb\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196193.5792212, \"EndTime\": 1666196196.3704376, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 7600.427627563477, \"count\": 1, \"min\": 7600.427627563477, \"max\": 7600.427627563477}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:16:46 INFO 139946820503360] Epoch[0] Batch[0] avg_epoch_loss=-0.549856\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:16:46 INFO 139946820503360] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=-0.5498555898666382\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:16:51 INFO 139946820503360] Epoch[0] Batch[5] avg_epoch_loss=-0.825337\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:16:51 INFO 139946820503360] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=-0.8253374894460043\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:16:51 INFO 139946820503360] Epoch[0] Batch [5]#011Speed: 70.95 samples/sec#011loss=-0.825337\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:16:56 INFO 139946820503360] Epoch[0] Batch[10] avg_epoch_loss=-0.869132\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:16:56 INFO 139946820503360] #quality_metric: host=algo-1, epoch=0, batch=10 train loss <loss>=-0.9216853737831116\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:16:56 INFO 139946820503360] Epoch[0] Batch [10]#011Speed: 62.49 samples/sec#011loss=-0.921685\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:16:56 INFO 139946820503360] processed a total of 687 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196196.3705091, \"EndTime\": 1666196216.3918, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"epochs\": {\"sum\": 400.0, \"count\": 1, \"min\": 400, \"max\": 400}, \"update.time\": {\"sum\": 20021.21925354004, \"count\": 1, \"min\": 20021.21925354004, \"max\": 20021.21925354004}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:16:56 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=34.31342293883335 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:16:56 INFO 139946820503360] #progress_metric: host=algo-1, completed 0.25 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:16:56 INFO 139946820503360] #quality_metric: host=algo-1, epoch=0, train loss <loss>=-0.8691319823265076\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:16:56 INFO 139946820503360] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:16:56 INFO 139946820503360] Saved checkpoint to \"/opt/ml/model/state_3f959b80-77ef-43b6-874b-98ba2b897139-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196216.391871, \"EndTime\": 1666196216.6590176, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 266.80850982666016, \"count\": 1, \"min\": 266.80850982666016, \"max\": 266.80850982666016}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:17:04 INFO 139946820503360] Epoch[1] Batch[0] avg_epoch_loss=-0.999015\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:17:04 INFO 139946820503360] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=-0.9990153312683105\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:17:08 INFO 139946820503360] Epoch[1] Batch[5] avg_epoch_loss=-1.133799\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:17:08 INFO 139946820503360] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=-1.1337993840376537\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:17:08 INFO 139946820503360] Epoch[1] Batch [5]#011Speed: 71.23 samples/sec#011loss=-1.133799\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:17:13 INFO 139946820503360] Epoch[1] Batch[10] avg_epoch_loss=-1.249871\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:17:13 INFO 139946820503360] #quality_metric: host=algo-1, epoch=1, batch=10 train loss <loss>=-1.3891574025154114\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:17:13 INFO 139946820503360] Epoch[1] Batch [10]#011Speed: 67.88 samples/sec#011loss=-1.389157\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:17:13 INFO 139946820503360] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196216.659083, \"EndTime\": 1666196233.6535344, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16994.395971298218, \"count\": 1, \"min\": 16994.395971298218, \"max\": 16994.395971298218}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:17:13 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=38.77727993080588 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:17:13 INFO 139946820503360] #progress_metric: host=algo-1, completed 0.5 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:17:13 INFO 139946820503360] #quality_metric: host=algo-1, epoch=1, train loss <loss>=-1.2498712106184526\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:17:13 INFO 139946820503360] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:17:13 INFO 139946820503360] Saved checkpoint to \"/opt/ml/model/state_783c6e4a-ee20-45fe-a369-dd11f8cae3d9-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196233.6535983, \"EndTime\": 1666196233.8532517, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 199.34391975402832, \"count\": 1, \"min\": 199.34391975402832, \"max\": 199.34391975402832}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:17:20 INFO 139946820503360] Epoch[2] Batch[0] avg_epoch_loss=-0.931696\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:17:20 INFO 139946820503360] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=-0.9316962361335754\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:17:25 INFO 139946820503360] Epoch[2] Batch[5] avg_epoch_loss=-1.456340\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:17:25 INFO 139946820503360] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=-1.4563397268454235\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:17:25 INFO 139946820503360] Epoch[2] Batch [5]#011Speed: 73.18 samples/sec#011loss=-1.456340\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:17:28 INFO 139946820503360] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196233.8533156, \"EndTime\": 1666196248.8740368, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15020.659923553467, \"count\": 1, \"min\": 15020.659923553467, \"max\": 15020.659923553467}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:17:28 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=41.74220341326814 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:17:28 INFO 139946820503360] #progress_metric: host=algo-1, completed 0.75 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:17:28 INFO 139946820503360] #quality_metric: host=algo-1, epoch=2, train loss <loss>=-1.4659965574741363\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:17:28 INFO 139946820503360] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:17:29 INFO 139946820503360] Saved checkpoint to \"/opt/ml/model/state_46689901-2c28-409e-b815-b4db85dae587-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196248.8741148, \"EndTime\": 1666196249.0678124, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 193.33863258361816, \"count\": 1, \"min\": 193.33863258361816, \"max\": 193.33863258361816}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:17:36 INFO 139946820503360] Epoch[3] Batch[0] avg_epoch_loss=-1.397026\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:17:36 INFO 139946820503360] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=-1.3970259428024292\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:17:41 INFO 139946820503360] Epoch[3] Batch[5] avg_epoch_loss=-1.780041\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:17:41 INFO 139946820503360] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=-1.7800408005714417\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:17:41 INFO 139946820503360] Epoch[3] Batch [5]#011Speed: 70.12 samples/sec#011loss=-1.780041\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:17:45 INFO 139946820503360] processed a total of 615 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196249.0678792, \"EndTime\": 1666196265.1436954, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16075.756311416626, \"count\": 1, \"min\": 16075.756311416626, \"max\": 16075.756311416626}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:17:45 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=38.25611642164902 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:17:45 INFO 139946820503360] #progress_metric: host=algo-1, completed 1.0 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:17:45 INFO 139946820503360] #quality_metric: host=algo-1, epoch=3, train loss <loss>=-1.6941924214363098\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:17:45 INFO 139946820503360] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:17:45 INFO 139946820503360] Saved checkpoint to \"/opt/ml/model/state_e0aa82ac-fb93-47e1-86c7-dd18bf35e903-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196265.1437669, \"EndTime\": 1666196265.3469396, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 202.7420997619629, \"count\": 1, \"min\": 202.7420997619629, \"max\": 202.7420997619629}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:17:52 INFO 139946820503360] Epoch[4] Batch[0] avg_epoch_loss=-2.031576\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:17:52 INFO 139946820503360] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=-2.031575918197632\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:17:56 INFO 139946820503360] Epoch[4] Batch[5] avg_epoch_loss=-1.825534\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:17:56 INFO 139946820503360] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=-1.825534204641978\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:17:56 INFO 139946820503360] Epoch[4] Batch [5]#011Speed: 72.53 samples/sec#011loss=-1.825534\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:18:01 INFO 139946820503360] Epoch[4] Batch[10] avg_epoch_loss=-1.978677\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:18:01 INFO 139946820503360] #quality_metric: host=algo-1, epoch=4, batch=10 train loss <loss>=-2.162448215484619\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:18:01 INFO 139946820503360] Epoch[4] Batch [10]#011Speed: 67.19 samples/sec#011loss=-2.162448\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:18:01 INFO 139946820503360] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196265.346998, \"EndTime\": 1666196281.6126413, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16265.581846237183, \"count\": 1, \"min\": 16265.581846237183, \"max\": 16265.581846237183}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:18:01 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.022934492016724 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:18:01 INFO 139946820503360] #progress_metric: host=algo-1, completed 1.25 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:18:01 INFO 139946820503360] #quality_metric: host=algo-1, epoch=4, train loss <loss>=-1.9786769368431785\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:18:01 INFO 139946820503360] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:18:01 INFO 139946820503360] Saved checkpoint to \"/opt/ml/model/state_162cb51d-6ebd-4e7b-805b-7d865e93d686-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196281.6127026, \"EndTime\": 1666196281.8467724, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 233.7646484375, \"count\": 1, \"min\": 233.7646484375, \"max\": 233.7646484375}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:18:09 INFO 139946820503360] Epoch[5] Batch[0] avg_epoch_loss=-1.855661\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:18:09 INFO 139946820503360] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=-1.8556607961654663\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:18:13 INFO 139946820503360] Epoch[5] Batch[5] avg_epoch_loss=-2.085543\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:18:13 INFO 139946820503360] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=-2.0855430165926614\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:18:13 INFO 139946820503360] Epoch[5] Batch [5]#011Speed: 70.22 samples/sec#011loss=-2.085543\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:18:18 INFO 139946820503360] Epoch[5] Batch[10] avg_epoch_loss=-1.832798\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:18:18 INFO 139946820503360] #quality_metric: host=algo-1, epoch=5, batch=10 train loss <loss>=-1.5295041680336\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:18:18 INFO 139946820503360] Epoch[5] Batch [10]#011Speed: 70.98 samples/sec#011loss=-1.529504\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:18:18 INFO 139946820503360] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196281.8468487, \"EndTime\": 1666196298.2844408, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16437.530755996704, \"count\": 1, \"min\": 16437.530755996704, \"max\": 16437.530755996704}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:18:18 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=39.30007070188354 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:18:18 INFO 139946820503360] #progress_metric: host=algo-1, completed 1.5 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:18:18 INFO 139946820503360] #quality_metric: host=algo-1, epoch=5, train loss <loss>=-1.8327980854294517\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:18:18 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:18:25 INFO 139946820503360] Epoch[6] Batch[0] avg_epoch_loss=-2.304695\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:18:25 INFO 139946820503360] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=-2.3046951293945312\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:18:29 INFO 139946820503360] Epoch[6] Batch[5] avg_epoch_loss=-2.242182\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:18:29 INFO 139946820503360] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=-2.242181976636251\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:18:29 INFO 139946820503360] Epoch[6] Batch [5]#011Speed: 74.37 samples/sec#011loss=-2.242182\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:18:33 INFO 139946820503360] processed a total of 613 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196298.2845087, \"EndTime\": 1666196313.3821027, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15097.208976745605, \"count\": 1, \"min\": 15097.208976745605, \"max\": 15097.208976745605}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:18:33 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.60312623632178 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:18:33 INFO 139946820503360] #progress_metric: host=algo-1, completed 1.75 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:18:33 INFO 139946820503360] #quality_metric: host=algo-1, epoch=6, train loss <loss>=-2.386896276473999\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:18:33 INFO 139946820503360] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:18:33 INFO 139946820503360] Saved checkpoint to \"/opt/ml/model/state_ee83460a-5933-46c6-ae1b-297e335439db-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196313.382219, \"EndTime\": 1666196313.6130738, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 230.4682731628418, \"count\": 1, \"min\": 230.4682731628418, \"max\": 230.4682731628418}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:18:40 INFO 139946820503360] Epoch[7] Batch[0] avg_epoch_loss=-2.275921\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:18:40 INFO 139946820503360] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=-2.275921106338501\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:18:45 INFO 139946820503360] Epoch[7] Batch[5] avg_epoch_loss=-2.284137\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:18:45 INFO 139946820503360] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=-2.2841370503107705\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:18:45 INFO 139946820503360] Epoch[7] Batch [5]#011Speed: 71.92 samples/sec#011loss=-2.284137\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:18:48 INFO 139946820503360] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196313.6131375, \"EndTime\": 1666196328.9547007, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15341.492652893066, \"count\": 1, \"min\": 15341.492652893066, \"max\": 15341.492652893066}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:18:48 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=41.39056911260473 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:18:48 INFO 139946820503360] #progress_metric: host=algo-1, completed 2.0 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:18:48 INFO 139946820503360] #quality_metric: host=algo-1, epoch=7, train loss <loss>=-2.2592574834823607\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:18:48 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:18:56 INFO 139946820503360] Epoch[8] Batch[0] avg_epoch_loss=-2.881529\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:18:56 INFO 139946820503360] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=-2.8815293312072754\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:19:00 INFO 139946820503360] Epoch[8] Batch[5] avg_epoch_loss=-2.358161\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:19:00 INFO 139946820503360] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=-2.358160972595215\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:19:00 INFO 139946820503360] Epoch[8] Batch [5]#011Speed: 72.96 samples/sec#011loss=-2.358161\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:19:04 INFO 139946820503360] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196328.954823, \"EndTime\": 1666196344.408092, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15452.712297439575, \"count\": 1, \"min\": 15452.712297439575, \"max\": 15452.712297439575}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:19:04 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=41.35166013670936 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:19:04 INFO 139946820503360] #progress_metric: host=algo-1, completed 2.25 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:19:04 INFO 139946820503360] #quality_metric: host=algo-1, epoch=8, train loss <loss>=-2.3413448691368104\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:19:04 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:19:11 INFO 139946820503360] Epoch[9] Batch[0] avg_epoch_loss=-1.865889\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:19:11 INFO 139946820503360] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=-1.865889310836792\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:19:15 INFO 139946820503360] Epoch[9] Batch[5] avg_epoch_loss=-2.439886\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:19:15 INFO 139946820503360] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=-2.439885894457499\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:19:15 INFO 139946820503360] Epoch[9] Batch [5]#011Speed: 75.14 samples/sec#011loss=-2.439886\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:19:19 INFO 139946820503360] processed a total of 604 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196344.4081724, \"EndTime\": 1666196359.239668, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 14831.177711486816, \"count\": 1, \"min\": 14831.177711486816, \"max\": 14831.177711486816}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:19:19 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.72476669532267 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:19:19 INFO 139946820503360] #progress_metric: host=algo-1, completed 2.5 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:19:19 INFO 139946820503360] #quality_metric: host=algo-1, epoch=9, train loss <loss>=-2.3305223762989042\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:19:19 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:19:26 INFO 139946820503360] Epoch[10] Batch[0] avg_epoch_loss=-2.411281\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:19:26 INFO 139946820503360] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=-2.411280632019043\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:19:30 INFO 139946820503360] Epoch[10] Batch[5] avg_epoch_loss=-2.687964\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:19:30 INFO 139946820503360] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=-2.6879638036092124\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:19:30 INFO 139946820503360] Epoch[10] Batch [5]#011Speed: 71.21 samples/sec#011loss=-2.687964\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:19:35 INFO 139946820503360] Epoch[10] Batch[10] avg_epoch_loss=-2.678261\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:19:35 INFO 139946820503360] #quality_metric: host=algo-1, epoch=10, batch=10 train loss <loss>=-2.666616916656494\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:19:35 INFO 139946820503360] Epoch[10] Batch [10]#011Speed: 67.65 samples/sec#011loss=-2.666617\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:19:35 INFO 139946820503360] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196359.2397301, \"EndTime\": 1666196375.6201482, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16379.999876022339, \"count\": 1, \"min\": 16379.999876022339, \"max\": 16379.999876022339}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:19:35 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=39.62130364512932 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:19:35 INFO 139946820503360] #progress_metric: host=algo-1, completed 2.75 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:19:35 INFO 139946820503360] #quality_metric: host=algo-1, epoch=10, train loss <loss>=-2.6782606731761587\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:19:35 INFO 139946820503360] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:19:35 INFO 139946820503360] Saved checkpoint to \"/opt/ml/model/state_57bb2d1a-0e32-47a5-927a-c1c355097312-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196375.6201987, \"EndTime\": 1666196375.8022838, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 181.76651000976562, \"count\": 1, \"min\": 181.76651000976562, \"max\": 181.76651000976562}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:19:43 INFO 139946820503360] Epoch[11] Batch[0] avg_epoch_loss=-2.704697\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:19:43 INFO 139946820503360] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=-2.7046971321105957\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:19:47 INFO 139946820503360] Epoch[11] Batch[5] avg_epoch_loss=-2.438746\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:19:47 INFO 139946820503360] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=-2.4387462536493936\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:19:47 INFO 139946820503360] Epoch[11] Batch [5]#011Speed: 69.92 samples/sec#011loss=-2.438746\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:19:52 INFO 139946820503360] Epoch[11] Batch[10] avg_epoch_loss=-2.579535\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:19:52 INFO 139946820503360] #quality_metric: host=algo-1, epoch=11, batch=10 train loss <loss>=-2.748480939865112\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:19:52 INFO 139946820503360] Epoch[11] Batch [10]#011Speed: 69.41 samples/sec#011loss=-2.748481\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:19:52 INFO 139946820503360] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196375.8023524, \"EndTime\": 1666196392.371161, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16568.75252723694, \"count\": 1, \"min\": 16568.75252723694, \"max\": 16568.75252723694}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:19:52 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=38.988842713227186 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:19:52 INFO 139946820503360] #progress_metric: host=algo-1, completed 3.0 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:19:52 INFO 139946820503360] #quality_metric: host=algo-1, epoch=11, train loss <loss>=-2.579534747383811\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:19:52 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:19:59 INFO 139946820503360] Epoch[12] Batch[0] avg_epoch_loss=-2.051842\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:19:59 INFO 139946820503360] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=-2.051842451095581\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:20:04 INFO 139946820503360] Epoch[12] Batch[5] avg_epoch_loss=-2.216056\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:20:04 INFO 139946820503360] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=-2.216055989265442\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:20:04 INFO 139946820503360] Epoch[12] Batch [5]#011Speed: 71.66 samples/sec#011loss=-2.216056\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:20:07 INFO 139946820503360] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196392.3712234, \"EndTime\": 1666196407.7701266, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15398.627519607544, \"count\": 1, \"min\": 15398.627519607544, \"max\": 15398.627519607544}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:20:07 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=41.10731210969454 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:20:07 INFO 139946820503360] #progress_metric: host=algo-1, completed 3.25 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:20:07 INFO 139946820503360] #quality_metric: host=algo-1, epoch=12, train loss <loss>=-2.2904998540878294\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:20:07 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:20:15 INFO 139946820503360] Epoch[13] Batch[0] avg_epoch_loss=-2.321189\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:20:15 INFO 139946820503360] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=-2.3211891651153564\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:20:19 INFO 139946820503360] Epoch[13] Batch[5] avg_epoch_loss=-2.505307\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:20:19 INFO 139946820503360] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=-2.5053069988886514\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:20:19 INFO 139946820503360] Epoch[13] Batch [5]#011Speed: 70.20 samples/sec#011loss=-2.505307\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:20:23 INFO 139946820503360] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196407.7701917, \"EndTime\": 1666196423.1140273, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15343.374490737915, \"count\": 1, \"min\": 15343.374490737915, \"max\": 15343.374490737915}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:20:23 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.408087315593036 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:20:23 INFO 139946820503360] #progress_metric: host=algo-1, completed 3.5 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:20:23 INFO 139946820503360] #quality_metric: host=algo-1, epoch=13, train loss <loss>=-2.539473366737366\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:20:23 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:20:30 INFO 139946820503360] Epoch[14] Batch[0] avg_epoch_loss=-2.335353\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:20:30 INFO 139946820503360] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=-2.335353374481201\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:20:35 INFO 139946820503360] Epoch[14] Batch[5] avg_epoch_loss=-2.567639\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:20:35 INFO 139946820503360] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=-2.5676387945810952\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:20:35 INFO 139946820503360] Epoch[14] Batch [5]#011Speed: 69.92 samples/sec#011loss=-2.567639\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:20:38 INFO 139946820503360] processed a total of 603 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196423.1140895, \"EndTime\": 1666196438.559923, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15445.366859436035, \"count\": 1, \"min\": 15445.366859436035, \"max\": 15445.366859436035}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:20:38 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=39.04057523245337 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:20:38 INFO 139946820503360] #progress_metric: host=algo-1, completed 3.75 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:20:38 INFO 139946820503360] #quality_metric: host=algo-1, epoch=14, train loss <loss>=-2.6365824699401856\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:20:38 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:20:45 INFO 139946820503360] Epoch[15] Batch[0] avg_epoch_loss=-2.655271\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:20:45 INFO 139946820503360] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=-2.655270576477051\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:20:50 INFO 139946820503360] Epoch[15] Batch[5] avg_epoch_loss=-2.687702\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:20:50 INFO 139946820503360] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=-2.687702020009359\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:20:50 INFO 139946820503360] Epoch[15] Batch [5]#011Speed: 74.38 samples/sec#011loss=-2.687702\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:20:53 INFO 139946820503360] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196438.55999, \"EndTime\": 1666196453.7056224, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15145.307302474976, \"count\": 1, \"min\": 15145.307302474976, \"max\": 15145.307302474976}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:20:53 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=41.06856739070944 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:20:53 INFO 139946820503360] #progress_metric: host=algo-1, completed 4.0 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:20:53 INFO 139946820503360] #quality_metric: host=algo-1, epoch=15, train loss <loss>=-2.792270874977112\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:20:53 INFO 139946820503360] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:20:53 INFO 139946820503360] Saved checkpoint to \"/opt/ml/model/state_77698514-13a1-48cf-bfd3-259cbf9312af-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196453.7056887, \"EndTime\": 1666196453.9384649, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 232.38229751586914, \"count\": 1, \"min\": 232.38229751586914, \"max\": 232.38229751586914}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:21:01 INFO 139946820503360] Epoch[16] Batch[0] avg_epoch_loss=-2.389941\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:21:01 INFO 139946820503360] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=-2.3899409770965576\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:21:05 INFO 139946820503360] Epoch[16] Batch[5] avg_epoch_loss=-2.588247\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:21:05 INFO 139946820503360] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=-2.588246504465739\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:21:05 INFO 139946820503360] Epoch[16] Batch [5]#011Speed: 71.54 samples/sec#011loss=-2.588247\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:21:09 INFO 139946820503360] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196453.9385288, \"EndTime\": 1666196469.016686, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15078.099012374878, \"count\": 1, \"min\": 15078.099012374878, \"max\": 15078.099012374878}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:21:09 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=42.37909301229358 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:21:09 INFO 139946820503360] #progress_metric: host=algo-1, completed 4.25 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:21:09 INFO 139946820503360] #quality_metric: host=algo-1, epoch=16, train loss <loss>=-2.622062063217163\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:21:09 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:21:16 INFO 139946820503360] Epoch[17] Batch[0] avg_epoch_loss=-2.373829\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:21:16 INFO 139946820503360] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=-2.373828887939453\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:21:20 INFO 139946820503360] Epoch[17] Batch[5] avg_epoch_loss=-2.749847\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:21:20 INFO 139946820503360] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=-2.749847412109375\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:21:20 INFO 139946820503360] Epoch[17] Batch [5]#011Speed: 76.52 samples/sec#011loss=-2.749847\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:21:24 INFO 139946820503360] Epoch[17] Batch[10] avg_epoch_loss=-2.945447\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:21:24 INFO 139946820503360] #quality_metric: host=algo-1, epoch=17, batch=10 train loss <loss>=-3.18016676902771\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:21:24 INFO 139946820503360] Epoch[17] Batch [10]#011Speed: 67.84 samples/sec#011loss=-3.180167\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:21:24 INFO 139946820503360] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196469.0167456, \"EndTime\": 1666196484.9656112, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15948.456048965454, \"count\": 1, \"min\": 15948.456048965454, \"max\": 15948.456048965454}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:21:24 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=41.19497499404086 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:21:24 INFO 139946820503360] #progress_metric: host=algo-1, completed 4.5 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:21:24 INFO 139946820503360] #quality_metric: host=algo-1, epoch=17, train loss <loss>=-2.9454471197995273\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:21:24 INFO 139946820503360] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:21:25 INFO 139946820503360] Saved checkpoint to \"/opt/ml/model/state_8a83c9a9-d896-473d-a94a-82b17aa4c611-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196484.965675, \"EndTime\": 1666196485.1981058, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 232.11097717285156, \"count\": 1, \"min\": 232.11097717285156, \"max\": 232.11097717285156}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:21:32 INFO 139946820503360] Epoch[18] Batch[0] avg_epoch_loss=-2.882968\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:21:32 INFO 139946820503360] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=-2.8829681873321533\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:21:37 INFO 139946820503360] Epoch[18] Batch[5] avg_epoch_loss=-2.652378\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:21:37 INFO 139946820503360] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=-2.6523783206939697\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:21:37 INFO 139946820503360] Epoch[18] Batch [5]#011Speed: 72.17 samples/sec#011loss=-2.652378\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:21:40 INFO 139946820503360] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196485.198172, \"EndTime\": 1666196500.5896564, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15391.427040100098, \"count\": 1, \"min\": 15391.427040100098, \"max\": 15391.427040100098}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:21:40 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=41.45138306638339 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:21:40 INFO 139946820503360] #progress_metric: host=algo-1, completed 4.75 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:21:40 INFO 139946820503360] #quality_metric: host=algo-1, epoch=18, train loss <loss>=-2.6515800952911377\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:21:40 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:21:47 INFO 139946820503360] Epoch[19] Batch[0] avg_epoch_loss=-2.477574\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:21:47 INFO 139946820503360] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=-2.477574110031128\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:21:52 INFO 139946820503360] Epoch[19] Batch[5] avg_epoch_loss=-2.755589\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:21:52 INFO 139946820503360] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=-2.75558869043986\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:21:52 INFO 139946820503360] Epoch[19] Batch [5]#011Speed: 72.39 samples/sec#011loss=-2.755589\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:21:55 INFO 139946820503360] processed a total of 596 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196500.5897255, \"EndTime\": 1666196515.8362036, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15246.162176132202, \"count\": 1, \"min\": 15246.162176132202, \"max\": 15246.162176132202}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:21:55 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=39.091564222757775 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:21:55 INFO 139946820503360] #progress_metric: host=algo-1, completed 5.0 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:21:55 INFO 139946820503360] #quality_metric: host=algo-1, epoch=19, train loss <loss>=-2.6452284812927247\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:21:55 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:22:02 INFO 139946820503360] Epoch[20] Batch[0] avg_epoch_loss=-2.225464\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:22:02 INFO 139946820503360] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=-2.225464105606079\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:22:07 INFO 139946820503360] Epoch[20] Batch[5] avg_epoch_loss=-2.863070\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:22:07 INFO 139946820503360] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=-2.863069693247477\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:22:07 INFO 139946820503360] Epoch[20] Batch [5]#011Speed: 70.47 samples/sec#011loss=-2.863070\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:22:11 INFO 139946820503360] Epoch[20] Batch[10] avg_epoch_loss=-3.052160\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:22:11 INFO 139946820503360] #quality_metric: host=algo-1, epoch=20, batch=10 train loss <loss>=-3.279069423675537\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:22:11 INFO 139946820503360] Epoch[20] Batch [10]#011Speed: 74.08 samples/sec#011loss=-3.279069\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:22:11 INFO 139946820503360] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196515.836268, \"EndTime\": 1666196531.8581684, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16021.471977233887, \"count\": 1, \"min\": 16021.471977233887, \"max\": 16021.471977233887}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:22:11 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.38304785464229 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:22:11 INFO 139946820503360] #progress_metric: host=algo-1, completed 5.25 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:22:11 INFO 139946820503360] #quality_metric: host=algo-1, epoch=20, train loss <loss>=-3.052160479805686\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:22:11 INFO 139946820503360] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:22:12 INFO 139946820503360] Saved checkpoint to \"/opt/ml/model/state_1d4c41d1-28ba-4361-87e9-3a6b47ce0dcd-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196531.8582382, \"EndTime\": 1666196532.0223463, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 163.71631622314453, \"count\": 1, \"min\": 163.71631622314453, \"max\": 163.71631622314453}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:22:18 INFO 139946820503360] Epoch[21] Batch[0] avg_epoch_loss=-1.959496\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:22:18 INFO 139946820503360] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=-1.9594964981079102\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:22:23 INFO 139946820503360] Epoch[21] Batch[5] avg_epoch_loss=-1.384346\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:22:23 INFO 139946820503360] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=-1.3843456705411274\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:22:23 INFO 139946820503360] Epoch[21] Batch [5]#011Speed: 73.07 samples/sec#011loss=-1.384346\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:22:28 INFO 139946820503360] Epoch[21] Batch[10] avg_epoch_loss=-1.742981\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:22:28 INFO 139946820503360] #quality_metric: host=algo-1, epoch=21, batch=10 train loss <loss>=-2.17334406375885\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:22:28 INFO 139946820503360] Epoch[21] Batch [10]#011Speed: 65.82 samples/sec#011loss=-2.173344\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:22:28 INFO 139946820503360] processed a total of 689 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196532.0224116, \"EndTime\": 1666196548.1534145, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16130.946397781372, \"count\": 1, \"min\": 16130.946397781372, \"max\": 16130.946397781372}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:22:28 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=42.712681607664656 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:22:28 INFO 139946820503360] #progress_metric: host=algo-1, completed 5.5 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:22:28 INFO 139946820503360] #quality_metric: host=algo-1, epoch=21, train loss <loss>=-1.7429813038219104\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:22:28 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:22:35 INFO 139946820503360] Epoch[22] Batch[0] avg_epoch_loss=-2.066750\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:22:35 INFO 139946820503360] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=-2.0667500495910645\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:22:39 INFO 139946820503360] Epoch[22] Batch[5] avg_epoch_loss=-2.121577\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:22:39 INFO 139946820503360] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=-2.1215773026148477\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:22:39 INFO 139946820503360] Epoch[22] Batch [5]#011Speed: 74.00 samples/sec#011loss=-2.121577\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:22:43 INFO 139946820503360] processed a total of 604 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196548.1534803, \"EndTime\": 1666196563.188591, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15034.823179244995, \"count\": 1, \"min\": 15034.823179244995, \"max\": 15034.823179244995}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:22:43 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.173140464086934 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:22:43 INFO 139946820503360] #progress_metric: host=algo-1, completed 5.75 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:22:43 INFO 139946820503360] #quality_metric: host=algo-1, epoch=22, train loss <loss>=-2.2842061281204225\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:22:43 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:22:50 INFO 139946820503360] Epoch[23] Batch[0] avg_epoch_loss=-1.967796\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:22:50 INFO 139946820503360] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=-1.9677960872650146\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:22:54 INFO 139946820503360] Epoch[23] Batch[5] avg_epoch_loss=-2.510186\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:22:54 INFO 139946820503360] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=-2.5101859966913858\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:22:54 INFO 139946820503360] Epoch[23] Batch [5]#011Speed: 74.41 samples/sec#011loss=-2.510186\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:22:58 INFO 139946820503360] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196563.1886594, \"EndTime\": 1666196578.2921467, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15103.158235549927, \"count\": 1, \"min\": 15103.158235549927, \"max\": 15103.158235549927}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:22:58 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=41.3817766459712 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:22:58 INFO 139946820503360] #progress_metric: host=algo-1, completed 6.0 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:22:58 INFO 139946820503360] #quality_metric: host=algo-1, epoch=23, train loss <loss>=-2.4751391053199767\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:22:58 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:23:05 INFO 139946820503360] Epoch[24] Batch[0] avg_epoch_loss=-2.719187\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:23:05 INFO 139946820503360] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=-2.719186544418335\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:23:10 INFO 139946820503360] Epoch[24] Batch[5] avg_epoch_loss=-2.652435\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:23:10 INFO 139946820503360] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=-2.6524346272150674\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:23:10 INFO 139946820503360] Epoch[24] Batch [5]#011Speed: 73.73 samples/sec#011loss=-2.652435\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:23:13 INFO 139946820503360] processed a total of 592 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196578.2922235, \"EndTime\": 1666196593.5244808, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15231.93383216858, \"count\": 1, \"min\": 15231.93383216858, \"max\": 15231.93383216858}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:23:13 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=38.86549665569534 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:23:13 INFO 139946820503360] #progress_metric: host=algo-1, completed 6.25 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:23:13 INFO 139946820503360] #quality_metric: host=algo-1, epoch=24, train loss <loss>=-2.9802523374557497\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:23:13 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:23:20 INFO 139946820503360] Epoch[25] Batch[0] avg_epoch_loss=-2.610307\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:23:20 INFO 139946820503360] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=-2.61030650138855\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:23:25 INFO 139946820503360] Epoch[25] Batch[5] avg_epoch_loss=-2.688317\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:23:25 INFO 139946820503360] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=-2.68831733862559\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:23:25 INFO 139946820503360] Epoch[25] Batch [5]#011Speed: 74.83 samples/sec#011loss=-2.688317\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:23:29 INFO 139946820503360] Epoch[25] Batch[10] avg_epoch_loss=-3.011254\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:23:29 INFO 139946820503360] #quality_metric: host=algo-1, epoch=25, batch=10 train loss <loss>=-3.3987776756286623\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:23:29 INFO 139946820503360] Epoch[25] Batch [10]#011Speed: 67.92 samples/sec#011loss=-3.398778\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:23:29 INFO 139946820503360] processed a total of 672 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196593.5245378, \"EndTime\": 1666196609.7167683, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16191.807508468628, \"count\": 1, \"min\": 16191.807508468628, \"max\": 16191.807508468628}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:23:29 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=41.5022527260251 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:23:29 INFO 139946820503360] #progress_metric: host=algo-1, completed 6.5 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:23:29 INFO 139946820503360] #quality_metric: host=algo-1, epoch=25, train loss <loss>=-3.0112538554451684\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:23:29 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:23:36 INFO 139946820503360] Epoch[26] Batch[0] avg_epoch_loss=-2.334345\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:23:36 INFO 139946820503360] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=-2.3343448638916016\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:23:41 INFO 139946820503360] Epoch[26] Batch[5] avg_epoch_loss=-2.654081\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:23:41 INFO 139946820503360] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=-2.654080549875895\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:23:41 INFO 139946820503360] Epoch[26] Batch [5]#011Speed: 70.84 samples/sec#011loss=-2.654081\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:23:45 INFO 139946820503360] Epoch[26] Batch[10] avg_epoch_loss=-2.713755\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:23:45 INFO 139946820503360] #quality_metric: host=algo-1, epoch=26, batch=10 train loss <loss>=-2.785363292694092\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:23:45 INFO 139946820503360] Epoch[26] Batch [10]#011Speed: 68.82 samples/sec#011loss=-2.785363\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:23:45 INFO 139946820503360] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196609.7168243, \"EndTime\": 1666196625.9786549, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16261.502742767334, \"count\": 1, \"min\": 16261.502742767334, \"max\": 16261.502742767334}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:23:45 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.094480884108066 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:23:45 INFO 139946820503360] #progress_metric: host=algo-1, completed 6.75 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:23:45 INFO 139946820503360] #quality_metric: host=algo-1, epoch=26, train loss <loss>=-2.7137545238841665\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:23:45 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:23:53 INFO 139946820503360] Epoch[27] Batch[0] avg_epoch_loss=-2.764101\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:23:53 INFO 139946820503360] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=-2.7641005516052246\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:23:57 INFO 139946820503360] Epoch[27] Batch[5] avg_epoch_loss=-2.776960\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:23:57 INFO 139946820503360] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=-2.7769600550333657\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:23:57 INFO 139946820503360] Epoch[27] Batch [5]#011Speed: 75.54 samples/sec#011loss=-2.776960\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:24:01 INFO 139946820503360] Epoch[27] Batch[10] avg_epoch_loss=-3.015201\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:24:01 INFO 139946820503360] #quality_metric: host=algo-1, epoch=27, batch=10 train loss <loss>=-3.301090860366821\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:24:01 INFO 139946820503360] Epoch[27] Batch [10]#011Speed: 69.96 samples/sec#011loss=-3.301091\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:24:01 INFO 139946820503360] processed a total of 666 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196625.9787142, \"EndTime\": 1666196641.915382, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15936.351299285889, \"count\": 1, \"min\": 15936.351299285889, \"max\": 15936.351299285889}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:24:01 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=41.79098544468556 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:24:01 INFO 139946820503360] #progress_metric: host=algo-1, completed 7.0 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:24:01 INFO 139946820503360] #quality_metric: host=algo-1, epoch=27, train loss <loss>=-3.0152013301849365\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:24:01 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:24:09 INFO 139946820503360] Epoch[28] Batch[0] avg_epoch_loss=-2.537352\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:24:09 INFO 139946820503360] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=-2.5373520851135254\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:24:13 INFO 139946820503360] Epoch[28] Batch[5] avg_epoch_loss=-2.994411\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:24:13 INFO 139946820503360] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=-2.9944109121958413\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:24:13 INFO 139946820503360] Epoch[28] Batch [5]#011Speed: 73.00 samples/sec#011loss=-2.994411\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:24:18 INFO 139946820503360] Epoch[28] Batch[10] avg_epoch_loss=-3.191002\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:24:18 INFO 139946820503360] #quality_metric: host=algo-1, epoch=28, batch=10 train loss <loss>=-3.4269116878509522\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:24:18 INFO 139946820503360] Epoch[28] Batch [10]#011Speed: 70.99 samples/sec#011loss=-3.426912\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:24:18 INFO 139946820503360] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196641.9154503, \"EndTime\": 1666196658.215481, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16299.681663513184, \"count\": 1, \"min\": 16299.681663513184, \"max\": 16299.681663513184}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:24:18 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=39.75517527909419 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:24:18 INFO 139946820503360] #progress_metric: host=algo-1, completed 7.25 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:24:18 INFO 139946820503360] #quality_metric: host=algo-1, epoch=28, train loss <loss>=-3.1910021738572554\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:24:18 INFO 139946820503360] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:24:18 INFO 139946820503360] Saved checkpoint to \"/opt/ml/model/state_a7fa82af-7d55-4fe5-9456-cd9c7a16df0b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196658.2155397, \"EndTime\": 1666196658.4028604, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 186.98668479919434, \"count\": 1, \"min\": 186.98668479919434, \"max\": 186.98668479919434}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:24:25 INFO 139946820503360] Epoch[29] Batch[0] avg_epoch_loss=-1.973781\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:24:25 INFO 139946820503360] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=-1.9737805128097534\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:24:29 INFO 139946820503360] Epoch[29] Batch[5] avg_epoch_loss=-2.121810\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:24:29 INFO 139946820503360] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=-2.121810336907705\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:24:29 INFO 139946820503360] Epoch[29] Batch [5]#011Speed: 74.13 samples/sec#011loss=-2.121810\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:24:33 INFO 139946820503360] processed a total of 613 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196658.4029164, \"EndTime\": 1666196673.5107381, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15107.76662826538, \"count\": 1, \"min\": 15107.76662826538, \"max\": 15107.76662826538}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:24:33 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.574891077736524 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:24:33 INFO 139946820503360] #progress_metric: host=algo-1, completed 7.5 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:24:33 INFO 139946820503360] #quality_metric: host=algo-1, epoch=29, train loss <loss>=-2.2826927065849305\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:24:33 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:24:40 INFO 139946820503360] Epoch[30] Batch[0] avg_epoch_loss=-2.267387\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:24:40 INFO 139946820503360] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=-2.2673873901367188\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:24:45 INFO 139946820503360] Epoch[30] Batch[5] avg_epoch_loss=-2.377524\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:24:45 INFO 139946820503360] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=-2.3775237798690796\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:24:45 INFO 139946820503360] Epoch[30] Batch [5]#011Speed: 70.00 samples/sec#011loss=-2.377524\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:24:49 INFO 139946820503360] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196673.5108063, \"EndTime\": 1666196689.03247, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15521.181583404541, \"count\": 1, \"min\": 15521.181583404541, \"max\": 15521.181583404541}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:24:49 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.718286139339696 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:24:49 INFO 139946820503360] #progress_metric: host=algo-1, completed 7.75 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:24:49 INFO 139946820503360] #quality_metric: host=algo-1, epoch=30, train loss <loss>=-2.4746368646621706\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:24:49 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:24:56 INFO 139946820503360] Epoch[31] Batch[0] avg_epoch_loss=-2.726509\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:24:56 INFO 139946820503360] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=-2.726508855819702\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:25:00 INFO 139946820503360] Epoch[31] Batch[5] avg_epoch_loss=-2.796763\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:25:00 INFO 139946820503360] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=-2.796762704849243\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:25:00 INFO 139946820503360] Epoch[31] Batch [5]#011Speed: 70.81 samples/sec#011loss=-2.796763\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:25:05 INFO 139946820503360] Epoch[31] Batch[10] avg_epoch_loss=-2.974649\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:25:05 INFO 139946820503360] #quality_metric: host=algo-1, epoch=31, batch=10 train loss <loss>=-3.1881116390228272\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:25:05 INFO 139946820503360] Epoch[31] Batch [10]#011Speed: 66.78 samples/sec#011loss=-3.188112\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:25:05 INFO 139946820503360] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196689.0325384, \"EndTime\": 1666196705.4142451, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16381.273984909058, \"count\": 1, \"min\": 16381.273984909058, \"max\": 16381.273984909058}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:25:05 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=39.92342735386066 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:25:05 INFO 139946820503360] #progress_metric: host=algo-1, completed 8.0 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:25:05 INFO 139946820503360] #quality_metric: host=algo-1, epoch=31, train loss <loss>=-2.974648584019054\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:25:05 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:25:12 INFO 139946820503360] Epoch[32] Batch[0] avg_epoch_loss=-2.944198\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:25:12 INFO 139946820503360] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=-2.9441981315612793\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:25:16 INFO 139946820503360] Epoch[32] Batch[5] avg_epoch_loss=-2.756482\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:25:16 INFO 139946820503360] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=-2.7564820845921836\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:25:16 INFO 139946820503360] Epoch[32] Batch [5]#011Speed: 71.21 samples/sec#011loss=-2.756482\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:25:21 INFO 139946820503360] Epoch[32] Batch[10] avg_epoch_loss=-2.700063\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:25:21 INFO 139946820503360] #quality_metric: host=algo-1, epoch=32, batch=10 train loss <loss>=-2.6323610305786134\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:25:21 INFO 139946820503360] Epoch[32] Batch [10]#011Speed: 65.42 samples/sec#011loss=-2.632361\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:25:21 INFO 139946820503360] processed a total of 670 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196705.4143028, \"EndTime\": 1666196721.7463536, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16331.739664077759, \"count\": 1, \"min\": 16331.739664077759, \"max\": 16331.739664077759}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:25:21 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=41.02414866623517 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:25:21 INFO 139946820503360] #progress_metric: host=algo-1, completed 8.25 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:25:21 INFO 139946820503360] #quality_metric: host=algo-1, epoch=32, train loss <loss>=-2.7000634236769243\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:25:21 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:25:29 INFO 139946820503360] Epoch[33] Batch[0] avg_epoch_loss=-2.960377\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:25:29 INFO 139946820503360] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=-2.9603769779205322\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:25:33 INFO 139946820503360] Epoch[33] Batch[5] avg_epoch_loss=-2.935061\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:25:33 INFO 139946820503360] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=-2.9350609381993613\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:25:33 INFO 139946820503360] Epoch[33] Batch [5]#011Speed: 73.75 samples/sec#011loss=-2.935061\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:25:36 INFO 139946820503360] processed a total of 604 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196721.7464273, \"EndTime\": 1666196736.8861468, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15139.427661895752, \"count\": 1, \"min\": 15139.427661895752, \"max\": 15139.427661895752}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:25:36 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=39.89555896169105 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:25:36 INFO 139946820503360] #progress_metric: host=algo-1, completed 8.5 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:25:36 INFO 139946820503360] #quality_metric: host=algo-1, epoch=33, train loss <loss>=-3.1093088388442993\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:25:36 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:25:43 INFO 139946820503360] Epoch[34] Batch[0] avg_epoch_loss=-3.148990\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:25:43 INFO 139946820503360] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=-3.148989677429199\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:25:48 INFO 139946820503360] Epoch[34] Batch[5] avg_epoch_loss=-3.090092\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:25:48 INFO 139946820503360] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=-3.0900920232137046\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:25:48 INFO 139946820503360] Epoch[34] Batch [5]#011Speed: 72.15 samples/sec#011loss=-3.090092\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:25:53 INFO 139946820503360] Epoch[34] Batch[10] avg_epoch_loss=-3.226157\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:25:53 INFO 139946820503360] #quality_metric: host=algo-1, epoch=34, batch=10 train loss <loss>=-3.3894340991973877\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:25:53 INFO 139946820503360] Epoch[34] Batch [10]#011Speed: 66.70 samples/sec#011loss=-3.389434\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:25:53 INFO 139946820503360] processed a total of 662 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196736.8862166, \"EndTime\": 1666196753.1839316, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16297.295570373535, \"count\": 1, \"min\": 16297.295570373535, \"max\": 16297.295570373535}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:25:53 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.62001125838039 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:25:53 INFO 139946820503360] #progress_metric: host=algo-1, completed 8.75 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:25:53 INFO 139946820503360] #quality_metric: host=algo-1, epoch=34, train loss <loss>=-3.2261566032062876\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:25:53 INFO 139946820503360] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:25:53 INFO 139946820503360] Saved checkpoint to \"/opt/ml/model/state_576314a5-1451-4ed6-9c4d-51ce8713f382-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196753.1839938, \"EndTime\": 1666196753.4151733, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 230.865478515625, \"count\": 1, \"min\": 230.865478515625, \"max\": 230.865478515625}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:26:00 INFO 139946820503360] Epoch[35] Batch[0] avg_epoch_loss=-2.382925\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:26:00 INFO 139946820503360] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=-2.3829245567321777\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:26:05 INFO 139946820503360] Epoch[35] Batch[5] avg_epoch_loss=-2.940786\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:26:05 INFO 139946820503360] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=-2.9407863219579062\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:26:05 INFO 139946820503360] Epoch[35] Batch [5]#011Speed: 69.72 samples/sec#011loss=-2.940786\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:26:08 INFO 139946820503360] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196753.4152408, \"EndTime\": 1666196768.8739996, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15458.699703216553, \"count\": 1, \"min\": 15458.699703216553, \"max\": 15458.699703216553}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:26:08 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=41.012243838378566 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:26:08 INFO 139946820503360] #progress_metric: host=algo-1, completed 9.0 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:26:08 INFO 139946820503360] #quality_metric: host=algo-1, epoch=35, train loss <loss>=-2.9666149377822877\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:26:08 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:26:15 INFO 139946820503360] Epoch[36] Batch[0] avg_epoch_loss=-3.663047\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:26:15 INFO 139946820503360] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=-3.6630473136901855\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:26:20 INFO 139946820503360] Epoch[36] Batch[5] avg_epoch_loss=-2.999013\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:26:20 INFO 139946820503360] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=-2.9990132252375283\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:26:20 INFO 139946820503360] Epoch[36] Batch [5]#011Speed: 74.76 samples/sec#011loss=-2.999013\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:26:24 INFO 139946820503360] Epoch[36] Batch[10] avg_epoch_loss=-3.155307\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:26:24 INFO 139946820503360] #quality_metric: host=algo-1, epoch=36, batch=10 train loss <loss>=-3.342859411239624\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:26:24 INFO 139946820503360] Epoch[36] Batch [10]#011Speed: 69.66 samples/sec#011loss=-3.342859\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:26:24 INFO 139946820503360] processed a total of 674 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196768.8740675, \"EndTime\": 1666196784.8589463, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15984.469413757324, \"count\": 1, \"min\": 15984.469413757324, \"max\": 15984.469413757324}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:26:24 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=42.165699290805485 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:26:24 INFO 139946820503360] #progress_metric: host=algo-1, completed 9.25 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:26:24 INFO 139946820503360] #quality_metric: host=algo-1, epoch=36, train loss <loss>=-3.155306946147572\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:26:24 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:26:32 INFO 139946820503360] Epoch[37] Batch[0] avg_epoch_loss=-2.998681\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:26:32 INFO 139946820503360] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=-2.99868106842041\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:26:36 INFO 139946820503360] Epoch[37] Batch[5] avg_epoch_loss=-2.812115\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:26:36 INFO 139946820503360] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=-2.8121145566304526\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:26:36 INFO 139946820503360] Epoch[37] Batch [5]#011Speed: 75.48 samples/sec#011loss=-2.812115\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:26:40 INFO 139946820503360] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196784.8590062, \"EndTime\": 1666196800.0712368, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15211.95363998413, \"count\": 1, \"min\": 15211.95363998413, \"max\": 15211.95363998413}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:26:40 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=41.94037632594243 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:26:40 INFO 139946820503360] #progress_metric: host=algo-1, completed 9.5 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:26:40 INFO 139946820503360] #quality_metric: host=algo-1, epoch=37, train loss <loss>=-2.9343243837356567\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:26:40 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:26:47 INFO 139946820503360] Epoch[38] Batch[0] avg_epoch_loss=-3.014573\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:26:47 INFO 139946820503360] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=-3.014572858810425\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:26:51 INFO 139946820503360] Epoch[38] Batch[5] avg_epoch_loss=-3.221649\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:26:51 INFO 139946820503360] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=-3.221649090449015\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:26:51 INFO 139946820503360] Epoch[38] Batch [5]#011Speed: 71.42 samples/sec#011loss=-3.221649\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:26:56 INFO 139946820503360] Epoch[38] Batch[10] avg_epoch_loss=-3.386173\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:26:56 INFO 139946820503360] #quality_metric: host=algo-1, epoch=38, batch=10 train loss <loss>=-3.5836013317108155\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:26:56 INFO 139946820503360] Epoch[38] Batch [10]#011Speed: 70.53 samples/sec#011loss=-3.583601\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:26:56 INFO 139946820503360] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196800.0713077, \"EndTime\": 1666196816.2777045, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16205.977201461792, \"count\": 1, \"min\": 16205.977201461792, \"max\": 16205.977201461792}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:26:56 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.478671605184196 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:26:56 INFO 139946820503360] #progress_metric: host=algo-1, completed 9.75 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:26:56 INFO 139946820503360] #quality_metric: host=algo-1, epoch=38, train loss <loss>=-3.3861728364771064\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:26:56 INFO 139946820503360] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:26:56 INFO 139946820503360] Saved checkpoint to \"/opt/ml/model/state_af97feee-1fb5-496f-81b6-42c21bb1ca0d-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196816.2777667, \"EndTime\": 1666196816.5102594, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 232.18369483947754, \"count\": 1, \"min\": 232.18369483947754, \"max\": 232.18369483947754}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:27:04 INFO 139946820503360] Epoch[39] Batch[0] avg_epoch_loss=-2.835573\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:27:04 INFO 139946820503360] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=-2.8355729579925537\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:27:08 INFO 139946820503360] Epoch[39] Batch[5] avg_epoch_loss=-2.971790\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:27:08 INFO 139946820503360] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=-2.9717900355656943\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:27:08 INFO 139946820503360] Epoch[39] Batch [5]#011Speed: 71.66 samples/sec#011loss=-2.971790\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:27:12 INFO 139946820503360] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196816.5103934, \"EndTime\": 1666196832.2114267, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15700.971841812134, \"count\": 1, \"min\": 15700.971841812134, \"max\": 15700.971841812134}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:27:12 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=39.487448677652495 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:27:12 INFO 139946820503360] #progress_metric: host=algo-1, completed 10.0 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:27:12 INFO 139946820503360] #quality_metric: host=algo-1, epoch=39, train loss <loss>=-2.964392614364624\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:27:12 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:27:19 INFO 139946820503360] Epoch[40] Batch[0] avg_epoch_loss=-3.183498\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:27:19 INFO 139946820503360] #quality_metric: host=algo-1, epoch=40, batch=0 train loss <loss>=-3.1834983825683594\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:27:23 INFO 139946820503360] Epoch[40] Batch[5] avg_epoch_loss=-3.024589\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:27:23 INFO 139946820503360] #quality_metric: host=algo-1, epoch=40, batch=5 train loss <loss>=-3.0245887438456216\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:27:23 INFO 139946820503360] Epoch[40] Batch [5]#011Speed: 71.94 samples/sec#011loss=-3.024589\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:27:28 INFO 139946820503360] Epoch[40] Batch[10] avg_epoch_loss=-3.301014\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:27:28 INFO 139946820503360] #quality_metric: host=algo-1, epoch=40, batch=10 train loss <loss>=-3.632724714279175\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:27:28 INFO 139946820503360] Epoch[40] Batch [10]#011Speed: 67.22 samples/sec#011loss=-3.632725\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:27:28 INFO 139946820503360] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196832.2116144, \"EndTime\": 1666196848.5028055, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16290.830373764038, \"count\": 1, \"min\": 16290.830373764038, \"max\": 16290.830373764038}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:27:28 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=39.960769165058046 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:27:28 INFO 139946820503360] #progress_metric: host=algo-1, completed 10.25 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:27:28 INFO 139946820503360] #quality_metric: host=algo-1, epoch=40, train loss <loss>=-3.3010141849517822\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:27:28 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:27:35 INFO 139946820503360] Epoch[41] Batch[0] avg_epoch_loss=-3.013735\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:27:35 INFO 139946820503360] #quality_metric: host=algo-1, epoch=41, batch=0 train loss <loss>=-3.013735055923462\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:27:40 INFO 139946820503360] Epoch[41] Batch[5] avg_epoch_loss=-3.002804\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:27:40 INFO 139946820503360] #quality_metric: host=algo-1, epoch=41, batch=5 train loss <loss>=-3.0028041203816733\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:27:40 INFO 139946820503360] Epoch[41] Batch [5]#011Speed: 70.65 samples/sec#011loss=-3.002804\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:27:44 INFO 139946820503360] Epoch[41] Batch[10] avg_epoch_loss=-3.205407\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:27:44 INFO 139946820503360] #quality_metric: host=algo-1, epoch=41, batch=10 train loss <loss>=-3.448529577255249\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:27:44 INFO 139946820503360] Epoch[41] Batch [10]#011Speed: 71.07 samples/sec#011loss=-3.448530\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:27:44 INFO 139946820503360] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196848.5029242, \"EndTime\": 1666196864.776074, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16272.857189178467, \"count\": 1, \"min\": 16272.857189178467, \"max\": 16272.857189178467}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:27:44 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.18940481759511 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:27:44 INFO 139946820503360] #progress_metric: host=algo-1, completed 10.5 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:27:44 INFO 139946820503360] #quality_metric: host=algo-1, epoch=41, train loss <loss>=-3.205406600778753\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:27:44 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:27:52 INFO 139946820503360] Epoch[42] Batch[0] avg_epoch_loss=-2.348323\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:27:52 INFO 139946820503360] #quality_metric: host=algo-1, epoch=42, batch=0 train loss <loss>=-2.348322629928589\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:27:56 INFO 139946820503360] Epoch[42] Batch[5] avg_epoch_loss=-2.108544\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:27:56 INFO 139946820503360] #quality_metric: host=algo-1, epoch=42, batch=5 train loss <loss>=-2.10854442914327\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:27:56 INFO 139946820503360] Epoch[42] Batch [5]#011Speed: 70.05 samples/sec#011loss=-2.108544\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:28:00 INFO 139946820503360] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196864.7761347, \"EndTime\": 1666196880.2917254, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15515.282154083252, \"count\": 1, \"min\": 15515.282154083252, \"max\": 15515.282154083252}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:28:00 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=41.249393422810876 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:28:00 INFO 139946820503360] #progress_metric: host=algo-1, completed 10.75 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:28:00 INFO 139946820503360] #quality_metric: host=algo-1, epoch=42, train loss <loss>=-2.237104058265686\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:28:00 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:28:07 INFO 139946820503360] Epoch[43] Batch[0] avg_epoch_loss=-2.594074\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:28:07 INFO 139946820503360] #quality_metric: host=algo-1, epoch=43, batch=0 train loss <loss>=-2.59407377243042\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:28:11 INFO 139946820503360] Epoch[43] Batch[5] avg_epoch_loss=-2.567740\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:28:11 INFO 139946820503360] #quality_metric: host=algo-1, epoch=43, batch=5 train loss <loss>=-2.567739566167196\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:28:11 INFO 139946820503360] Epoch[43] Batch [5]#011Speed: 75.85 samples/sec#011loss=-2.567740\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:28:16 INFO 139946820503360] Epoch[43] Batch[10] avg_epoch_loss=-2.833877\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:28:16 INFO 139946820503360] #quality_metric: host=algo-1, epoch=43, batch=10 train loss <loss>=-3.1532424449920655\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:28:16 INFO 139946820503360] Epoch[43] Batch [10]#011Speed: 65.50 samples/sec#011loss=-3.153242\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:28:16 INFO 139946820503360] processed a total of 662 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196880.2917936, \"EndTime\": 1666196896.8439214, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16551.637887954712, \"count\": 1, \"min\": 16551.637887954712, \"max\": 16551.637887954712}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:28:16 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=39.99578595263288 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:28:16 INFO 139946820503360] #progress_metric: host=algo-1, completed 11.0 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:28:16 INFO 139946820503360] #quality_metric: host=algo-1, epoch=43, train loss <loss>=-2.8338772383603184\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:28:16 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:28:24 INFO 139946820503360] Epoch[44] Batch[0] avg_epoch_loss=-3.369307\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:28:24 INFO 139946820503360] #quality_metric: host=algo-1, epoch=44, batch=0 train loss <loss>=-3.369307041168213\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:28:28 INFO 139946820503360] Epoch[44] Batch[5] avg_epoch_loss=-3.034275\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:28:28 INFO 139946820503360] #quality_metric: host=algo-1, epoch=44, batch=5 train loss <loss>=-3.034275492032369\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:28:28 INFO 139946820503360] Epoch[44] Batch [5]#011Speed: 70.04 samples/sec#011loss=-3.034275\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:28:32 INFO 139946820503360] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196896.8439949, \"EndTime\": 1666196912.228701, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15384.313344955444, \"count\": 1, \"min\": 15384.313344955444, \"max\": 15384.313344955444}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:28:32 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.88529833186995 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:28:32 INFO 139946820503360] #progress_metric: host=algo-1, completed 11.25 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:28:32 INFO 139946820503360] #quality_metric: host=algo-1, epoch=44, train loss <loss>=-3.12432005405426\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:28:32 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:28:39 INFO 139946820503360] Epoch[45] Batch[0] avg_epoch_loss=-3.898159\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:28:39 INFO 139946820503360] #quality_metric: host=algo-1, epoch=45, batch=0 train loss <loss>=-3.8981587886810303\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:28:43 INFO 139946820503360] Epoch[45] Batch[5] avg_epoch_loss=-2.881566\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:28:43 INFO 139946820503360] #quality_metric: host=algo-1, epoch=45, batch=5 train loss <loss>=-2.8815660874048867\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:28:43 INFO 139946820503360] Epoch[45] Batch [5]#011Speed: 71.63 samples/sec#011loss=-2.881566\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:28:48 INFO 139946820503360] Epoch[45] Batch[10] avg_epoch_loss=-2.898647\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:28:48 INFO 139946820503360] #quality_metric: host=algo-1, epoch=45, batch=10 train loss <loss>=-2.9191450119018554\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:28:48 INFO 139946820503360] Epoch[45] Batch [10]#011Speed: 63.17 samples/sec#011loss=-2.919145\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:28:48 INFO 139946820503360] processed a total of 679 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196912.2287683, \"EndTime\": 1666196928.568752, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16339.534521102905, \"count\": 1, \"min\": 16339.534521102905, \"max\": 16339.534521102905}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:28:48 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=41.55542519241873 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:28:48 INFO 139946820503360] #progress_metric: host=algo-1, completed 11.5 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:28:48 INFO 139946820503360] #quality_metric: host=algo-1, epoch=45, train loss <loss>=-2.898647416721691\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:28:48 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:28:55 INFO 139946820503360] Epoch[46] Batch[0] avg_epoch_loss=-3.550005\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:28:55 INFO 139946820503360] #quality_metric: host=algo-1, epoch=46, batch=0 train loss <loss>=-3.5500051975250244\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:29:00 INFO 139946820503360] Epoch[46] Batch[5] avg_epoch_loss=-3.275031\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:29:00 INFO 139946820503360] #quality_metric: host=algo-1, epoch=46, batch=5 train loss <loss>=-3.2750306526819863\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:29:00 INFO 139946820503360] Epoch[46] Batch [5]#011Speed: 71.55 samples/sec#011loss=-3.275031\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:29:04 INFO 139946820503360] Epoch[46] Batch[10] avg_epoch_loss=-3.528647\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:29:04 INFO 139946820503360] #quality_metric: host=algo-1, epoch=46, batch=10 train loss <loss>=-3.8329865455627443\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:29:04 INFO 139946820503360] Epoch[46] Batch [10]#011Speed: 68.99 samples/sec#011loss=-3.832987\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:29:04 INFO 139946820503360] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196928.5688143, \"EndTime\": 1666196944.8751879, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16306.095600128174, \"count\": 1, \"min\": 16306.095600128174, \"max\": 16306.095600128174}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:29:04 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=39.55556079325351 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:29:04 INFO 139946820503360] #progress_metric: host=algo-1, completed 11.75 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:29:04 INFO 139946820503360] #quality_metric: host=algo-1, epoch=46, train loss <loss>=-3.5286469676277856\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:29:04 INFO 139946820503360] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:29:05 INFO 139946820503360] Saved checkpoint to \"/opt/ml/model/state_f6e0fbdf-a370-4d17-a50f-5bd7ff7d255b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196944.8752446, \"EndTime\": 1666196945.0800762, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 204.5149803161621, \"count\": 1, \"min\": 204.5149803161621, \"max\": 204.5149803161621}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:29:12 INFO 139946820503360] Epoch[47] Batch[0] avg_epoch_loss=-3.193155\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:29:12 INFO 139946820503360] #quality_metric: host=algo-1, epoch=47, batch=0 train loss <loss>=-3.19315505027771\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:29:16 INFO 139946820503360] Epoch[47] Batch[5] avg_epoch_loss=-3.308369\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:29:16 INFO 139946820503360] #quality_metric: host=algo-1, epoch=47, batch=5 train loss <loss>=-3.308369000752767\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:29:16 INFO 139946820503360] Epoch[47] Batch [5]#011Speed: 71.30 samples/sec#011loss=-3.308369\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:29:20 INFO 139946820503360] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196945.0801425, \"EndTime\": 1666196960.356193, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15275.986909866333, \"count\": 1, \"min\": 15275.986909866333, \"max\": 15275.986909866333}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:29:20 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.389928753572555 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:29:20 INFO 139946820503360] #progress_metric: host=algo-1, completed 12.0 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:29:20 INFO 139946820503360] #quality_metric: host=algo-1, epoch=47, train loss <loss>=-3.3097286462783813\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:29:20 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:29:27 INFO 139946820503360] Epoch[48] Batch[0] avg_epoch_loss=-3.082878\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:29:27 INFO 139946820503360] #quality_metric: host=algo-1, epoch=48, batch=0 train loss <loss>=-3.0828776359558105\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:29:31 INFO 139946820503360] Epoch[48] Batch[5] avg_epoch_loss=-3.179460\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:29:31 INFO 139946820503360] #quality_metric: host=algo-1, epoch=48, batch=5 train loss <loss>=-3.179460287094116\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:29:31 INFO 139946820503360] Epoch[48] Batch [5]#011Speed: 75.36 samples/sec#011loss=-3.179460\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:29:36 INFO 139946820503360] Epoch[48] Batch[10] avg_epoch_loss=-3.271007\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:29:36 INFO 139946820503360] #quality_metric: host=algo-1, epoch=48, batch=10 train loss <loss>=-3.380863094329834\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:29:36 INFO 139946820503360] Epoch[48] Batch [10]#011Speed: 71.77 samples/sec#011loss=-3.380863\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:29:36 INFO 139946820503360] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196960.3562593, \"EndTime\": 1666196976.1022503, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15745.584487915039, \"count\": 1, \"min\": 15745.584487915039, \"max\": 15745.584487915039}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:29:36 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=41.78926123451339 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:29:36 INFO 139946820503360] #progress_metric: host=algo-1, completed 12.25 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:29:36 INFO 139946820503360] #quality_metric: host=algo-1, epoch=48, train loss <loss>=-3.271007017655806\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:29:36 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:29:42 INFO 139946820503360] Epoch[49] Batch[0] avg_epoch_loss=-3.480982\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:29:42 INFO 139946820503360] #quality_metric: host=algo-1, epoch=49, batch=0 train loss <loss>=-3.4809823036193848\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:29:47 INFO 139946820503360] Epoch[49] Batch[5] avg_epoch_loss=-3.344608\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:29:47 INFO 139946820503360] #quality_metric: host=algo-1, epoch=49, batch=5 train loss <loss>=-3.3446080684661865\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:29:47 INFO 139946820503360] Epoch[49] Batch [5]#011Speed: 71.08 samples/sec#011loss=-3.344608\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:29:52 INFO 139946820503360] Epoch[49] Batch[10] avg_epoch_loss=-3.355367\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:29:52 INFO 139946820503360] #quality_metric: host=algo-1, epoch=49, batch=10 train loss <loss>=-3.3682783603668214\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:29:52 INFO 139946820503360] Epoch[49] Batch [10]#011Speed: 67.66 samples/sec#011loss=-3.368278\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:29:52 INFO 139946820503360] processed a total of 685 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196976.1023104, \"EndTime\": 1666196992.115075, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16012.46976852417, \"count\": 1, \"min\": 16012.46976852417, \"max\": 16012.46976852417}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:29:52 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=42.77892966906485 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:29:52 INFO 139946820503360] #progress_metric: host=algo-1, completed 12.5 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:29:52 INFO 139946820503360] #quality_metric: host=algo-1, epoch=49, train loss <loss>=-3.3553672920573843\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:29:52 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:29:59 INFO 139946820503360] Epoch[50] Batch[0] avg_epoch_loss=-2.999099\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:29:59 INFO 139946820503360] #quality_metric: host=algo-1, epoch=50, batch=0 train loss <loss>=-2.999099016189575\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:30:03 INFO 139946820503360] Epoch[50] Batch[5] avg_epoch_loss=-3.324259\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:30:03 INFO 139946820503360] #quality_metric: host=algo-1, epoch=50, batch=5 train loss <loss>=-3.324258883794149\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:30:03 INFO 139946820503360] Epoch[50] Batch [5]#011Speed: 69.44 samples/sec#011loss=-3.324259\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:30:07 INFO 139946820503360] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666196992.1151373, \"EndTime\": 1666197007.4545407, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15339.09821510315, \"count\": 1, \"min\": 15339.09821510315, \"max\": 15339.09821510315}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:30:07 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=41.52755453514214 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:30:07 INFO 139946820503360] #progress_metric: host=algo-1, completed 12.75 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:30:07 INFO 139946820503360] #quality_metric: host=algo-1, epoch=50, train loss <loss>=-3.407193970680237\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:30:07 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:30:14 INFO 139946820503360] Epoch[51] Batch[0] avg_epoch_loss=-3.921775\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:30:14 INFO 139946820503360] #quality_metric: host=algo-1, epoch=51, batch=0 train loss <loss>=-3.9217748641967773\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:30:19 INFO 139946820503360] Epoch[51] Batch[5] avg_epoch_loss=-3.184333\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:30:19 INFO 139946820503360] #quality_metric: host=algo-1, epoch=51, batch=5 train loss <loss>=-3.1843332846959433\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:30:19 INFO 139946820503360] Epoch[51] Batch [5]#011Speed: 72.74 samples/sec#011loss=-3.184333\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:30:22 INFO 139946820503360] processed a total of 615 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197007.4546206, \"EndTime\": 1666197022.8190298, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15363.938570022583, \"count\": 1, \"min\": 15363.938570022583, \"max\": 15363.938570022583}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:30:22 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.02855741052373 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:30:22 INFO 139946820503360] #progress_metric: host=algo-1, completed 13.0 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:30:22 INFO 139946820503360] #quality_metric: host=algo-1, epoch=51, train loss <loss>=-3.317367744445801\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:30:22 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:30:29 INFO 139946820503360] Epoch[52] Batch[0] avg_epoch_loss=-2.300983\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:30:29 INFO 139946820503360] #quality_metric: host=algo-1, epoch=52, batch=0 train loss <loss>=-2.30098295211792\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:30:34 INFO 139946820503360] Epoch[52] Batch[5] avg_epoch_loss=-2.496405\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:30:34 INFO 139946820503360] #quality_metric: host=algo-1, epoch=52, batch=5 train loss <loss>=-2.496405005455017\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:30:34 INFO 139946820503360] Epoch[52] Batch [5]#011Speed: 71.24 samples/sec#011loss=-2.496405\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:30:39 INFO 139946820503360] Epoch[52] Batch[10] avg_epoch_loss=-2.672142\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:30:39 INFO 139946820503360] #quality_metric: host=algo-1, epoch=52, batch=10 train loss <loss>=-2.8830260753631594\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:30:39 INFO 139946820503360] Epoch[52] Batch [10]#011Speed: 66.77 samples/sec#011loss=-2.883026\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:30:39 INFO 139946820503360] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197022.81909, \"EndTime\": 1666197039.179275, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16359.800815582275, \"count\": 1, \"min\": 16359.800815582275, \"max\": 16359.800815582275}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:30:39 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.22032171337031 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:30:39 INFO 139946820503360] #progress_metric: host=algo-1, completed 13.25 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:30:39 INFO 139946820503360] #quality_metric: host=algo-1, epoch=52, train loss <loss>=-2.6721418554132637\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:30:39 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:30:46 INFO 139946820503360] Epoch[53] Batch[0] avg_epoch_loss=-2.178040\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:30:46 INFO 139946820503360] #quality_metric: host=algo-1, epoch=53, batch=0 train loss <loss>=-2.1780402660369873\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:30:50 INFO 139946820503360] Epoch[53] Batch[5] avg_epoch_loss=-1.329371\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:30:50 INFO 139946820503360] #quality_metric: host=algo-1, epoch=53, batch=5 train loss <loss>=-1.3293713629245758\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:30:50 INFO 139946820503360] Epoch[53] Batch [5]#011Speed: 69.93 samples/sec#011loss=-1.329371\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:30:55 INFO 139946820503360] Epoch[53] Batch[10] avg_epoch_loss=-1.600072\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:30:55 INFO 139946820503360] #quality_metric: host=algo-1, epoch=53, batch=10 train loss <loss>=-1.9249122142791748\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:30:55 INFO 139946820503360] Epoch[53] Batch [10]#011Speed: 67.30 samples/sec#011loss=-1.924912\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:30:55 INFO 139946820503360] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197039.1793363, \"EndTime\": 1666197055.6531422, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16473.533153533936, \"count\": 1, \"min\": 16473.533153533936, \"max\": 16473.533153533936}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:30:55 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=39.699812993364 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:30:55 INFO 139946820503360] #progress_metric: host=algo-1, completed 13.5 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:30:55 INFO 139946820503360] #quality_metric: host=algo-1, epoch=53, train loss <loss>=-1.600071749903939\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:30:55 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:31:02 INFO 139946820503360] Epoch[54] Batch[0] avg_epoch_loss=-2.069011\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:31:02 INFO 139946820503360] #quality_metric: host=algo-1, epoch=54, batch=0 train loss <loss>=-2.0690114498138428\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:31:07 INFO 139946820503360] Epoch[54] Batch[5] avg_epoch_loss=-2.136842\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:31:07 INFO 139946820503360] #quality_metric: host=algo-1, epoch=54, batch=5 train loss <loss>=-2.1368417938550315\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:31:07 INFO 139946820503360] Epoch[54] Batch [5]#011Speed: 69.36 samples/sec#011loss=-2.136842\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:31:11 INFO 139946820503360] processed a total of 606 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197055.653212, \"EndTime\": 1666197071.0888603, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15435.397386550903, \"count\": 1, \"min\": 15435.397386550903, \"max\": 15435.397386550903}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:31:11 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=39.26016185531444 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:31:11 INFO 139946820503360] #progress_metric: host=algo-1, completed 13.75 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:31:11 INFO 139946820503360] #quality_metric: host=algo-1, epoch=54, train loss <loss>=-2.173251521587372\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:31:11 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:31:18 INFO 139946820503360] Epoch[55] Batch[0] avg_epoch_loss=-3.108959\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:31:18 INFO 139946820503360] #quality_metric: host=algo-1, epoch=55, batch=0 train loss <loss>=-3.108959436416626\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:31:22 INFO 139946820503360] Epoch[55] Batch[5] avg_epoch_loss=-2.605407\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:31:22 INFO 139946820503360] #quality_metric: host=algo-1, epoch=55, batch=5 train loss <loss>=-2.6054073174794516\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:31:22 INFO 139946820503360] Epoch[55] Batch [5]#011Speed: 73.39 samples/sec#011loss=-2.605407\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:31:26 INFO 139946820503360] processed a total of 605 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197071.088928, \"EndTime\": 1666197086.6279578, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15538.700103759766, \"count\": 1, \"min\": 15538.700103759766, \"max\": 15538.700103759766}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:31:26 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=38.93479906119646 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:31:26 INFO 139946820503360] #progress_metric: host=algo-1, completed 14.0 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:31:26 INFO 139946820503360] #quality_metric: host=algo-1, epoch=55, train loss <loss>=-2.8196568727493285\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:31:26 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:31:33 INFO 139946820503360] Epoch[56] Batch[0] avg_epoch_loss=-2.938986\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:31:33 INFO 139946820503360] #quality_metric: host=algo-1, epoch=56, batch=0 train loss <loss>=-2.938985586166382\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:31:38 INFO 139946820503360] Epoch[56] Batch[5] avg_epoch_loss=-2.832849\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:31:38 INFO 139946820503360] #quality_metric: host=algo-1, epoch=56, batch=5 train loss <loss>=-2.83284862836202\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:31:38 INFO 139946820503360] Epoch[56] Batch [5]#011Speed: 73.93 samples/sec#011loss=-2.832849\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:31:41 INFO 139946820503360] processed a total of 602 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197086.6280267, \"EndTime\": 1666197101.7891202, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15160.749673843384, \"count\": 1, \"min\": 15160.749673843384, \"max\": 15160.749673843384}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:31:41 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=39.70754417876709 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:31:41 INFO 139946820503360] #progress_metric: host=algo-1, completed 14.25 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:31:41 INFO 139946820503360] #quality_metric: host=algo-1, epoch=56, train loss <loss>=-2.748379349708557\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:31:41 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:31:48 INFO 139946820503360] Epoch[57] Batch[0] avg_epoch_loss=-2.944707\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:31:48 INFO 139946820503360] #quality_metric: host=algo-1, epoch=57, batch=0 train loss <loss>=-2.944707155227661\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:31:53 INFO 139946820503360] Epoch[57] Batch[5] avg_epoch_loss=-2.791040\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:31:53 INFO 139946820503360] #quality_metric: host=algo-1, epoch=57, batch=5 train loss <loss>=-2.7910399039586387\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:31:53 INFO 139946820503360] Epoch[57] Batch [5]#011Speed: 70.24 samples/sec#011loss=-2.791040\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:31:58 INFO 139946820503360] Epoch[57] Batch[10] avg_epoch_loss=-2.867390\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:31:58 INFO 139946820503360] #quality_metric: host=algo-1, epoch=57, batch=10 train loss <loss>=-2.959010362625122\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:31:58 INFO 139946820503360] Epoch[57] Batch [10]#011Speed: 64.68 samples/sec#011loss=-2.959010\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:31:58 INFO 139946820503360] processed a total of 674 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197101.7891884, \"EndTime\": 1666197118.2952998, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16505.792379379272, \"count\": 1, \"min\": 16505.792379379272, \"max\": 16505.792379379272}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:31:58 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.83389095163638 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:31:58 INFO 139946820503360] #progress_metric: host=algo-1, completed 14.5 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:31:58 INFO 139946820503360] #quality_metric: host=algo-1, epoch=57, train loss <loss>=-2.867390112443404\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:31:58 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:32:05 INFO 139946820503360] Epoch[58] Batch[0] avg_epoch_loss=-2.898675\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:32:05 INFO 139946820503360] #quality_metric: host=algo-1, epoch=58, batch=0 train loss <loss>=-2.898674964904785\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:32:09 INFO 139946820503360] Epoch[58] Batch[5] avg_epoch_loss=-3.039631\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:32:09 INFO 139946820503360] #quality_metric: host=algo-1, epoch=58, batch=5 train loss <loss>=-3.039630572001139\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:32:09 INFO 139946820503360] Epoch[58] Batch [5]#011Speed: 76.39 samples/sec#011loss=-3.039631\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:32:14 INFO 139946820503360] Epoch[58] Batch[10] avg_epoch_loss=-2.964599\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:32:14 INFO 139946820503360] #quality_metric: host=algo-1, epoch=58, batch=10 train loss <loss>=-2.874561238288879\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:32:14 INFO 139946820503360] Epoch[58] Batch [10]#011Speed: 70.92 samples/sec#011loss=-2.874561\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:32:14 INFO 139946820503360] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197118.2953718, \"EndTime\": 1666197134.505529, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16209.786415100098, \"count\": 1, \"min\": 16209.786415100098, \"max\": 16209.786415100098}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:32:14 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=39.667192510613354 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:32:14 INFO 139946820503360] #progress_metric: host=algo-1, completed 14.75 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:32:14 INFO 139946820503360] #quality_metric: host=algo-1, epoch=58, train loss <loss>=-2.9645990566773848\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:32:14 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:32:21 INFO 139946820503360] Epoch[59] Batch[0] avg_epoch_loss=-3.088664\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:32:21 INFO 139946820503360] #quality_metric: host=algo-1, epoch=59, batch=0 train loss <loss>=-3.0886635780334473\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:32:26 INFO 139946820503360] Epoch[59] Batch[5] avg_epoch_loss=-3.088413\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:32:26 INFO 139946820503360] #quality_metric: host=algo-1, epoch=59, batch=5 train loss <loss>=-3.0884132385253906\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:32:26 INFO 139946820503360] Epoch[59] Batch [5]#011Speed: 70.98 samples/sec#011loss=-3.088413\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:32:29 INFO 139946820503360] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197134.5055866, \"EndTime\": 1666197149.6378105, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15131.942749023438, \"count\": 1, \"min\": 15131.942749023438, \"max\": 15131.942749023438}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:32:29 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=42.29436941397909 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:32:29 INFO 139946820503360] #progress_metric: host=algo-1, completed 15.0 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:32:29 INFO 139946820503360] #quality_metric: host=algo-1, epoch=59, train loss <loss>=-3.109311318397522\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:32:29 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:32:37 INFO 139946820503360] Epoch[60] Batch[0] avg_epoch_loss=-3.486951\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:32:37 INFO 139946820503360] #quality_metric: host=algo-1, epoch=60, batch=0 train loss <loss>=-3.4869511127471924\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:32:41 INFO 139946820503360] Epoch[60] Batch[5] avg_epoch_loss=-3.539529\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:32:41 INFO 139946820503360] #quality_metric: host=algo-1, epoch=60, batch=5 train loss <loss>=-3.539529244105021\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:32:41 INFO 139946820503360] Epoch[60] Batch [5]#011Speed: 71.18 samples/sec#011loss=-3.539529\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:32:45 INFO 139946820503360] processed a total of 606 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197149.6378767, \"EndTime\": 1666197165.1617336, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15523.50401878357, \"count\": 1, \"min\": 15523.50401878357, \"max\": 15523.50401878357}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:32:45 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=39.03697551730652 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:32:45 INFO 139946820503360] #progress_metric: host=algo-1, completed 15.25 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:32:45 INFO 139946820503360] #quality_metric: host=algo-1, epoch=60, train loss <loss>=-3.6495595455169676\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:32:45 INFO 139946820503360] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:32:45 INFO 139946820503360] Saved checkpoint to \"/opt/ml/model/state_a69bd03c-8118-4820-a218-87a3fa515d40-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197165.1619403, \"EndTime\": 1666197165.3919113, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 229.56061363220215, \"count\": 1, \"min\": 229.56061363220215, \"max\": 229.56061363220215}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:32:52 INFO 139946820503360] Epoch[61] Batch[0] avg_epoch_loss=-3.321157\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:32:52 INFO 139946820503360] #quality_metric: host=algo-1, epoch=61, batch=0 train loss <loss>=-3.321157455444336\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:32:56 INFO 139946820503360] Epoch[61] Batch[5] avg_epoch_loss=-3.158461\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:32:56 INFO 139946820503360] #quality_metric: host=algo-1, epoch=61, batch=5 train loss <loss>=-3.158460577329\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:32:56 INFO 139946820503360] Epoch[61] Batch [5]#011Speed: 73.41 samples/sec#011loss=-3.158461\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:33:01 INFO 139946820503360] Epoch[61] Batch[10] avg_epoch_loss=-3.799313\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:33:01 INFO 139946820503360] #quality_metric: host=algo-1, epoch=61, batch=10 train loss <loss>=-4.568336868286133\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:33:01 INFO 139946820503360] Epoch[61] Batch [10]#011Speed: 71.48 samples/sec#011loss=-4.568337\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:33:01 INFO 139946820503360] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197165.391976, \"EndTime\": 1666197181.4686606, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16076.62582397461, \"count\": 1, \"min\": 16076.62582397461, \"max\": 16076.62582397461}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:33:01 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=39.99569442373388 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:33:01 INFO 139946820503360] #progress_metric: host=algo-1, completed 15.5 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:33:01 INFO 139946820503360] #quality_metric: host=algo-1, epoch=61, train loss <loss>=-3.7993134368549693\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:33:01 INFO 139946820503360] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:33:01 INFO 139946820503360] Saved checkpoint to \"/opt/ml/model/state_06300681-fc1f-4378-a30b-ea516144e12e-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197181.4687338, \"EndTime\": 1666197181.6578028, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 188.58623504638672, \"count\": 1, \"min\": 188.58623504638672, \"max\": 188.58623504638672}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:33:08 INFO 139946820503360] Epoch[62] Batch[0] avg_epoch_loss=-3.139055\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:33:08 INFO 139946820503360] #quality_metric: host=algo-1, epoch=62, batch=0 train loss <loss>=-3.139055013656616\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:33:13 INFO 139946820503360] Epoch[62] Batch[5] avg_epoch_loss=-3.353124\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:33:13 INFO 139946820503360] #quality_metric: host=algo-1, epoch=62, batch=5 train loss <loss>=-3.353123664855957\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:33:13 INFO 139946820503360] Epoch[62] Batch [5]#011Speed: 72.79 samples/sec#011loss=-3.353124\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:33:17 INFO 139946820503360] Epoch[62] Batch[10] avg_epoch_loss=-3.448640\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:33:17 INFO 139946820503360] #quality_metric: host=algo-1, epoch=62, batch=10 train loss <loss>=-3.5632596015930176\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:33:17 INFO 139946820503360] Epoch[62] Batch [10]#011Speed: 70.66 samples/sec#011loss=-3.563260\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:33:17 INFO 139946820503360] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197181.6578689, \"EndTime\": 1666197197.8567111, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16198.783874511719, \"count\": 1, \"min\": 16198.783874511719, \"max\": 16198.783874511719}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:33:17 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=39.69412519802632 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:33:17 INFO 139946820503360] #progress_metric: host=algo-1, completed 15.75 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:33:17 INFO 139946820503360] #quality_metric: host=algo-1, epoch=62, train loss <loss>=-3.448639999736439\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:33:17 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:33:24 INFO 139946820503360] Epoch[63] Batch[0] avg_epoch_loss=-3.619744\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:33:24 INFO 139946820503360] #quality_metric: host=algo-1, epoch=63, batch=0 train loss <loss>=-3.619743585586548\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:33:29 INFO 139946820503360] Epoch[63] Batch[5] avg_epoch_loss=-3.077760\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:33:29 INFO 139946820503360] #quality_metric: host=algo-1, epoch=63, batch=5 train loss <loss>=-3.0777604977289834\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:33:29 INFO 139946820503360] Epoch[63] Batch [5]#011Speed: 69.75 samples/sec#011loss=-3.077760\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:33:34 INFO 139946820503360] Epoch[63] Batch[10] avg_epoch_loss=-3.309944\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:33:34 INFO 139946820503360] #quality_metric: host=algo-1, epoch=63, batch=10 train loss <loss>=-3.5885645866394045\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:33:34 INFO 139946820503360] Epoch[63] Batch [10]#011Speed: 61.13 samples/sec#011loss=-3.588565\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:33:34 INFO 139946820503360] processed a total of 703 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197197.8567708, \"EndTime\": 1666197214.4746857, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16617.619276046753, \"count\": 1, \"min\": 16617.619276046753, \"max\": 16617.619276046753}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:33:34 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=42.30427396887953 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:33:34 INFO 139946820503360] #progress_metric: host=algo-1, completed 16.0 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:33:34 INFO 139946820503360] #quality_metric: host=algo-1, epoch=63, train loss <loss>=-3.3099441745064477\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:33:34 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:33:41 INFO 139946820503360] Epoch[64] Batch[0] avg_epoch_loss=-2.580870\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:33:41 INFO 139946820503360] #quality_metric: host=algo-1, epoch=64, batch=0 train loss <loss>=-2.5808703899383545\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:33:46 INFO 139946820503360] Epoch[64] Batch[5] avg_epoch_loss=-3.011560\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:33:46 INFO 139946820503360] #quality_metric: host=algo-1, epoch=64, batch=5 train loss <loss>=-3.0115601619084678\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:33:46 INFO 139946820503360] Epoch[64] Batch [5]#011Speed: 72.40 samples/sec#011loss=-3.011560\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:33:49 INFO 139946820503360] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197214.4747462, \"EndTime\": 1666197229.608468, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15133.39376449585, \"count\": 1, \"min\": 15133.39376449585, \"max\": 15133.39376449585}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:33:49 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=42.15815495921698 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:33:49 INFO 139946820503360] #progress_metric: host=algo-1, completed 16.25 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:33:49 INFO 139946820503360] #quality_metric: host=algo-1, epoch=64, train loss <loss>=-2.981221318244934\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:33:49 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:33:56 INFO 139946820503360] Epoch[65] Batch[0] avg_epoch_loss=-3.126373\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:33:56 INFO 139946820503360] #quality_metric: host=algo-1, epoch=65, batch=0 train loss <loss>=-3.1263725757598877\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:34:01 INFO 139946820503360] Epoch[65] Batch[5] avg_epoch_loss=-3.346435\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:34:01 INFO 139946820503360] #quality_metric: host=algo-1, epoch=65, batch=5 train loss <loss>=-3.3464345137278237\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:34:01 INFO 139946820503360] Epoch[65] Batch [5]#011Speed: 72.10 samples/sec#011loss=-3.346435\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:34:06 INFO 139946820503360] Epoch[65] Batch[10] avg_epoch_loss=-3.499441\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:34:06 INFO 139946820503360] #quality_metric: host=algo-1, epoch=65, batch=10 train loss <loss>=-3.6830494880676268\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:34:06 INFO 139946820503360] Epoch[65] Batch [10]#011Speed: 64.03 samples/sec#011loss=-3.683049\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:34:06 INFO 139946820503360] processed a total of 678 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197229.6085343, \"EndTime\": 1666197246.0179071, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16408.958911895752, \"count\": 1, \"min\": 16408.958911895752, \"max\": 16408.958911895752}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:34:06 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=41.318646388656745 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:34:06 INFO 139946820503360] #progress_metric: host=algo-1, completed 16.5 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:34:06 INFO 139946820503360] #quality_metric: host=algo-1, epoch=65, train loss <loss>=-3.499441320245916\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:34:06 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:34:13 INFO 139946820503360] Epoch[66] Batch[0] avg_epoch_loss=-3.362530\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:34:13 INFO 139946820503360] #quality_metric: host=algo-1, epoch=66, batch=0 train loss <loss>=-3.36253023147583\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:34:17 INFO 139946820503360] Epoch[66] Batch[5] avg_epoch_loss=-3.095811\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:34:17 INFO 139946820503360] #quality_metric: host=algo-1, epoch=66, batch=5 train loss <loss>=-3.095811049143473\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:34:17 INFO 139946820503360] Epoch[66] Batch [5]#011Speed: 70.09 samples/sec#011loss=-3.095811\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:34:21 INFO 139946820503360] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197246.017971, \"EndTime\": 1666197261.3956785, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15377.346992492676, \"count\": 1, \"min\": 15377.346992492676, \"max\": 15377.346992492676}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:34:21 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.77399557181054 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:34:21 INFO 139946820503360] #progress_metric: host=algo-1, completed 16.75 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:34:21 INFO 139946820503360] #quality_metric: host=algo-1, epoch=66, train loss <loss>=-3.0366029024124144\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:34:21 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:34:28 INFO 139946820503360] Epoch[67] Batch[0] avg_epoch_loss=-2.567405\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:34:28 INFO 139946820503360] #quality_metric: host=algo-1, epoch=67, batch=0 train loss <loss>=-2.5674052238464355\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:34:33 INFO 139946820503360] Epoch[67] Batch[5] avg_epoch_loss=-2.794792\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:34:33 INFO 139946820503360] #quality_metric: host=algo-1, epoch=67, batch=5 train loss <loss>=-2.79479185740153\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:34:33 INFO 139946820503360] Epoch[67] Batch [5]#011Speed: 69.76 samples/sec#011loss=-2.794792\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:34:38 INFO 139946820503360] Epoch[67] Batch[10] avg_epoch_loss=-3.104884\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:34:38 INFO 139946820503360] #quality_metric: host=algo-1, epoch=67, batch=10 train loss <loss>=-3.476994037628174\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:34:38 INFO 139946820503360] Epoch[67] Batch [10]#011Speed: 64.84 samples/sec#011loss=-3.476994\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:34:38 INFO 139946820503360] processed a total of 679 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197261.3957477, \"EndTime\": 1666197278.066096, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16670.024394989014, \"count\": 1, \"min\": 16670.024394989014, \"max\": 16670.024394989014}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:34:38 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.73157265216171 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:34:38 INFO 139946820503360] #progress_metric: host=algo-1, completed 17.0 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:34:38 INFO 139946820503360] #quality_metric: host=algo-1, epoch=67, train loss <loss>=-3.10488375750455\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:34:38 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:34:45 INFO 139946820503360] Epoch[68] Batch[0] avg_epoch_loss=-3.420581\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:34:45 INFO 139946820503360] #quality_metric: host=algo-1, epoch=68, batch=0 train loss <loss>=-3.4205806255340576\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:34:49 INFO 139946820503360] Epoch[68] Batch[5] avg_epoch_loss=-3.330457\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:34:49 INFO 139946820503360] #quality_metric: host=algo-1, epoch=68, batch=5 train loss <loss>=-3.330457250277201\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:34:49 INFO 139946820503360] Epoch[68] Batch [5]#011Speed: 76.83 samples/sec#011loss=-3.330457\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:34:53 INFO 139946820503360] processed a total of 621 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197278.06616, \"EndTime\": 1666197293.01403, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 14947.590351104736, \"count\": 1, \"min\": 14947.590351104736, \"max\": 14947.590351104736}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:34:53 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=41.54486822949294 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:34:53 INFO 139946820503360] #progress_metric: host=algo-1, completed 17.25 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:34:53 INFO 139946820503360] #quality_metric: host=algo-1, epoch=68, train loss <loss>=-3.2936135292053224\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:34:53 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:35:00 INFO 139946820503360] Epoch[69] Batch[0] avg_epoch_loss=-3.588104\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:35:00 INFO 139946820503360] #quality_metric: host=algo-1, epoch=69, batch=0 train loss <loss>=-3.588103771209717\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:35:04 INFO 139946820503360] Epoch[69] Batch[5] avg_epoch_loss=-3.573078\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:35:04 INFO 139946820503360] #quality_metric: host=algo-1, epoch=69, batch=5 train loss <loss>=-3.5730781157811484\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:35:04 INFO 139946820503360] Epoch[69] Batch [5]#011Speed: 71.11 samples/sec#011loss=-3.573078\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:35:08 INFO 139946820503360] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197293.0141008, \"EndTime\": 1666197308.3782063, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15363.660335540771, \"count\": 1, \"min\": 15363.660335540771, \"max\": 15363.660335540771}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:35:08 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=41.135770039448936 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:35:08 INFO 139946820503360] #progress_metric: host=algo-1, completed 17.5 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:35:08 INFO 139946820503360] #quality_metric: host=algo-1, epoch=69, train loss <loss>=-3.3569703578948973\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:35:08 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:35:15 INFO 139946820503360] Epoch[70] Batch[0] avg_epoch_loss=-3.250376\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:35:15 INFO 139946820503360] #quality_metric: host=algo-1, epoch=70, batch=0 train loss <loss>=-3.250375986099243\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:35:20 INFO 139946820503360] Epoch[70] Batch[5] avg_epoch_loss=-3.373675\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:35:20 INFO 139946820503360] #quality_metric: host=algo-1, epoch=70, batch=5 train loss <loss>=-3.373674670855204\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:35:20 INFO 139946820503360] Epoch[70] Batch [5]#011Speed: 71.96 samples/sec#011loss=-3.373675\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:35:24 INFO 139946820503360] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197308.3782737, \"EndTime\": 1666197324.0895746, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15710.976600646973, \"count\": 1, \"min\": 15710.976600646973, \"max\": 15710.976600646973}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:35:24 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=39.271654745492796 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:35:24 INFO 139946820503360] #progress_metric: host=algo-1, completed 17.75 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:35:24 INFO 139946820503360] #quality_metric: host=algo-1, epoch=70, train loss <loss>=-3.359378147125244\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:35:24 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:35:31 INFO 139946820503360] Epoch[71] Batch[0] avg_epoch_loss=-3.483665\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:35:31 INFO 139946820503360] #quality_metric: host=algo-1, epoch=71, batch=0 train loss <loss>=-3.4836654663085938\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:35:35 INFO 139946820503360] Epoch[71] Batch[5] avg_epoch_loss=-3.343830\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:35:35 INFO 139946820503360] #quality_metric: host=algo-1, epoch=71, batch=5 train loss <loss>=-3.3438299099604287\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:35:35 INFO 139946820503360] Epoch[71] Batch [5]#011Speed: 69.87 samples/sec#011loss=-3.343830\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:35:39 INFO 139946820503360] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197324.0896444, \"EndTime\": 1666197339.4152362, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15325.241565704346, \"count\": 1, \"min\": 15325.241565704346, \"max\": 15325.241565704346}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:35:39 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.45587297426876 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:35:39 INFO 139946820503360] #progress_metric: host=algo-1, completed 18.0 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:35:39 INFO 139946820503360] #quality_metric: host=algo-1, epoch=71, train loss <loss>=-3.4164947271347046\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:35:39 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:35:46 INFO 139946820503360] Epoch[72] Batch[0] avg_epoch_loss=-3.691635\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:35:46 INFO 139946820503360] #quality_metric: host=algo-1, epoch=72, batch=0 train loss <loss>=-3.6916353702545166\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:35:51 INFO 139946820503360] Epoch[72] Batch[5] avg_epoch_loss=-3.441334\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:35:51 INFO 139946820503360] #quality_metric: host=algo-1, epoch=72, batch=5 train loss <loss>=-3.441333850224813\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:35:51 INFO 139946820503360] Epoch[72] Batch [5]#011Speed: 70.86 samples/sec#011loss=-3.441334\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:35:55 INFO 139946820503360] Epoch[72] Batch[10] avg_epoch_loss=-3.015317\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:35:55 INFO 139946820503360] #quality_metric: host=algo-1, epoch=72, batch=10 train loss <loss>=-2.5040962994098663\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:35:55 INFO 139946820503360] Epoch[72] Batch [10]#011Speed: 68.11 samples/sec#011loss=-2.504096\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:35:55 INFO 139946820503360] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197339.4153044, \"EndTime\": 1666197355.9203608, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16504.652500152588, \"count\": 1, \"min\": 16504.652500152588, \"max\": 16504.652500152588}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:35:55 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=39.140243515054316 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:35:55 INFO 139946820503360] #progress_metric: host=algo-1, completed 18.25 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:35:55 INFO 139946820503360] #quality_metric: host=algo-1, epoch=72, train loss <loss>=-3.0153167816725643\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:35:55 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:36:02 INFO 139946820503360] Epoch[73] Batch[0] avg_epoch_loss=-3.058140\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:36:02 INFO 139946820503360] #quality_metric: host=algo-1, epoch=73, batch=0 train loss <loss>=-3.0581398010253906\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:36:07 INFO 139946820503360] Epoch[73] Batch[5] avg_epoch_loss=-3.036544\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:36:07 INFO 139946820503360] #quality_metric: host=algo-1, epoch=73, batch=5 train loss <loss>=-3.03654412428538\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:36:07 INFO 139946820503360] Epoch[73] Batch [5]#011Speed: 70.66 samples/sec#011loss=-3.036544\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:36:12 INFO 139946820503360] Epoch[73] Batch[10] avg_epoch_loss=-3.144578\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:36:12 INFO 139946820503360] #quality_metric: host=algo-1, epoch=73, batch=10 train loss <loss>=-3.274218130111694\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:36:12 INFO 139946820503360] Epoch[73] Batch [10]#011Speed: 61.98 samples/sec#011loss=-3.274218\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:36:13 INFO 139946820503360] processed a total of 713 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197355.920431, \"EndTime\": 1666197373.3132908, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 17392.497301101685, \"count\": 1, \"min\": 17392.497301101685, \"max\": 17392.497301101685}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:36:13 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.994462632136376 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:36:13 INFO 139946820503360] #progress_metric: host=algo-1, completed 18.5 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:36:13 INFO 139946820503360] #quality_metric: host=algo-1, epoch=73, train loss <loss>=-3.421870708465576\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:36:13 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:36:20 INFO 139946820503360] Epoch[74] Batch[0] avg_epoch_loss=-3.477782\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:36:20 INFO 139946820503360] #quality_metric: host=algo-1, epoch=74, batch=0 train loss <loss>=-3.4777815341949463\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:36:24 INFO 139946820503360] Epoch[74] Batch[5] avg_epoch_loss=-3.186299\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:36:24 INFO 139946820503360] #quality_metric: host=algo-1, epoch=74, batch=5 train loss <loss>=-3.186299125353495\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:36:24 INFO 139946820503360] Epoch[74] Batch [5]#011Speed: 71.03 samples/sec#011loss=-3.186299\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:36:28 INFO 139946820503360] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197373.3133569, \"EndTime\": 1666197388.5545948, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15240.668058395386, \"count\": 1, \"min\": 15240.668058395386, \"max\": 15240.668058395386}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:36:28 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=41.53335446663302 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:36:28 INFO 139946820503360] #progress_metric: host=algo-1, completed 18.75 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:36:28 INFO 139946820503360] #quality_metric: host=algo-1, epoch=74, train loss <loss>=-3.3431554555892946\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:36:28 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:36:35 INFO 139946820503360] Epoch[75] Batch[0] avg_epoch_loss=-2.932777\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:36:35 INFO 139946820503360] #quality_metric: host=algo-1, epoch=75, batch=0 train loss <loss>=-2.932776927947998\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:36:40 INFO 139946820503360] Epoch[75] Batch[5] avg_epoch_loss=-3.463560\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:36:40 INFO 139946820503360] #quality_metric: host=algo-1, epoch=75, batch=5 train loss <loss>=-3.463559945424398\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:36:40 INFO 139946820503360] Epoch[75] Batch [5]#011Speed: 73.90 samples/sec#011loss=-3.463560\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:36:43 INFO 139946820503360] processed a total of 610 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197388.5546572, \"EndTime\": 1666197403.7065933, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15151.501417160034, \"count\": 1, \"min\": 15151.501417160034, \"max\": 15151.501417160034}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:36:43 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.25977041883734 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:36:43 INFO 139946820503360] #progress_metric: host=algo-1, completed 19.0 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:36:43 INFO 139946820503360] #quality_metric: host=algo-1, epoch=75, train loss <loss>=-3.4937278747558596\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:36:43 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:36:50 INFO 139946820503360] Epoch[76] Batch[0] avg_epoch_loss=-3.868268\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:36:50 INFO 139946820503360] #quality_metric: host=algo-1, epoch=76, batch=0 train loss <loss>=-3.868267774581909\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:36:55 INFO 139946820503360] Epoch[76] Batch[5] avg_epoch_loss=-3.191787\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:36:55 INFO 139946820503360] #quality_metric: host=algo-1, epoch=76, batch=5 train loss <loss>=-3.191786607106527\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:36:55 INFO 139946820503360] Epoch[76] Batch [5]#011Speed: 73.83 samples/sec#011loss=-3.191787\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:36:59 INFO 139946820503360] Epoch[76] Batch[10] avg_epoch_loss=-3.347394\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:36:59 INFO 139946820503360] #quality_metric: host=algo-1, epoch=76, batch=10 train loss <loss>=-3.5341231346130373\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:36:59 INFO 139946820503360] Epoch[76] Batch [10]#011Speed: 68.41 samples/sec#011loss=-3.534123\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:36:59 INFO 139946820503360] processed a total of 680 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197403.7066624, \"EndTime\": 1666197419.7241826, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16017.172813415527, \"count\": 1, \"min\": 16017.172813415527, \"max\": 16017.172813415527}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:36:59 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=42.454197275770305 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:36:59 INFO 139946820503360] #progress_metric: host=algo-1, completed 19.25 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:36:59 INFO 139946820503360] #quality_metric: host=algo-1, epoch=76, train loss <loss>=-3.347394119609486\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:36:59 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:37:07 INFO 139946820503360] Epoch[77] Batch[0] avg_epoch_loss=-3.140942\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:37:07 INFO 139946820503360] #quality_metric: host=algo-1, epoch=77, batch=0 train loss <loss>=-3.140941619873047\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:37:11 INFO 139946820503360] Epoch[77] Batch[5] avg_epoch_loss=-3.362218\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:37:11 INFO 139946820503360] #quality_metric: host=algo-1, epoch=77, batch=5 train loss <loss>=-3.362217585245768\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:37:11 INFO 139946820503360] Epoch[77] Batch [5]#011Speed: 75.89 samples/sec#011loss=-3.362218\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:37:15 INFO 139946820503360] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197419.7242434, \"EndTime\": 1666197435.0308857, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15306.361675262451, \"count\": 1, \"min\": 15306.361675262451, \"max\": 15306.361675262451}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:37:15 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=41.616450818951535 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:37:15 INFO 139946820503360] #progress_metric: host=algo-1, completed 19.5 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:37:15 INFO 139946820503360] #quality_metric: host=algo-1, epoch=77, train loss <loss>=-3.4481258630752563\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:37:15 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:37:22 INFO 139946820503360] Epoch[78] Batch[0] avg_epoch_loss=-3.025769\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:37:22 INFO 139946820503360] #quality_metric: host=algo-1, epoch=78, batch=0 train loss <loss>=-3.025768518447876\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:37:26 INFO 139946820503360] Epoch[78] Batch[5] avg_epoch_loss=-3.495236\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:37:26 INFO 139946820503360] #quality_metric: host=algo-1, epoch=78, batch=5 train loss <loss>=-3.4952355225880942\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:37:26 INFO 139946820503360] Epoch[78] Batch [5]#011Speed: 75.38 samples/sec#011loss=-3.495236\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:37:29 INFO 139946820503360] processed a total of 612 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197435.0309422, \"EndTime\": 1666197449.972735, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 14941.396236419678, \"count\": 1, \"min\": 14941.396236419678, \"max\": 14941.396236419678}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:37:29 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.95977712539573 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:37:29 INFO 139946820503360] #progress_metric: host=algo-1, completed 19.75 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:37:29 INFO 139946820503360] #quality_metric: host=algo-1, epoch=78, train loss <loss>=-3.37651447057724\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:37:29 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:37:36 INFO 139946820503360] Epoch[79] Batch[0] avg_epoch_loss=-4.006203\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:37:36 INFO 139946820503360] #quality_metric: host=algo-1, epoch=79, batch=0 train loss <loss>=-4.006202697753906\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:37:41 INFO 139946820503360] Epoch[79] Batch[5] avg_epoch_loss=-3.252856\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:37:41 INFO 139946820503360] #quality_metric: host=algo-1, epoch=79, batch=5 train loss <loss>=-3.2528560956319175\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:37:41 INFO 139946820503360] Epoch[79] Batch [5]#011Speed: 74.65 samples/sec#011loss=-3.252856\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:37:45 INFO 139946820503360] Epoch[79] Batch[10] avg_epoch_loss=-3.350687\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:37:45 INFO 139946820503360] #quality_metric: host=algo-1, epoch=79, batch=10 train loss <loss>=-3.4680850505828857\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:37:45 INFO 139946820503360] Epoch[79] Batch [10]#011Speed: 69.54 samples/sec#011loss=-3.468085\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:37:45 INFO 139946820503360] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197449.9727962, \"EndTime\": 1666197465.7971137, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15823.962926864624, \"count\": 1, \"min\": 15823.962926864624, \"max\": 15823.962926864624}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:37:45 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=41.96145185089873 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:37:45 INFO 139946820503360] #progress_metric: host=algo-1, completed 20.0 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:37:45 INFO 139946820503360] #quality_metric: host=algo-1, epoch=79, train loss <loss>=-3.3506874387914483\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:37:45 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:37:53 INFO 139946820503360] Epoch[80] Batch[0] avg_epoch_loss=-3.739349\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:37:53 INFO 139946820503360] #quality_metric: host=algo-1, epoch=80, batch=0 train loss <loss>=-3.739349126815796\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:37:57 INFO 139946820503360] Epoch[80] Batch[5] avg_epoch_loss=-3.484210\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:37:57 INFO 139946820503360] #quality_metric: host=algo-1, epoch=80, batch=5 train loss <loss>=-3.4842103322347007\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:37:57 INFO 139946820503360] Epoch[80] Batch [5]#011Speed: 71.03 samples/sec#011loss=-3.484210\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:38:01 INFO 139946820503360] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197465.797171, \"EndTime\": 1666197481.226811, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15429.3212890625, \"count\": 1, \"min\": 15429.3212890625, \"max\": 15429.3212890625}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:38:01 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.182846607135176 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:38:01 INFO 139946820503360] #progress_metric: host=algo-1, completed 20.25 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:38:01 INFO 139946820503360] #quality_metric: host=algo-1, epoch=80, train loss <loss>=-3.4050491094589233\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:38:01 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:38:08 INFO 139946820503360] Epoch[81] Batch[0] avg_epoch_loss=-3.311913\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:38:08 INFO 139946820503360] #quality_metric: host=algo-1, epoch=81, batch=0 train loss <loss>=-3.31191349029541\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:38:13 INFO 139946820503360] Epoch[81] Batch[5] avg_epoch_loss=-3.486903\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:38:13 INFO 139946820503360] #quality_metric: host=algo-1, epoch=81, batch=5 train loss <loss>=-3.486902674039205\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:38:13 INFO 139946820503360] Epoch[81] Batch [5]#011Speed: 70.15 samples/sec#011loss=-3.486903\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:38:16 INFO 139946820503360] processed a total of 606 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197481.2269256, \"EndTime\": 1666197496.8553746, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15628.028392791748, \"count\": 1, \"min\": 15628.028392791748, \"max\": 15628.028392791748}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:38:16 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=38.776222981645645 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:38:16 INFO 139946820503360] #progress_metric: host=algo-1, completed 20.5 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:38:16 INFO 139946820503360] #quality_metric: host=algo-1, epoch=81, train loss <loss>=-3.136754095554352\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:38:16 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:38:24 INFO 139946820503360] Epoch[82] Batch[0] avg_epoch_loss=-2.545428\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:38:24 INFO 139946820503360] #quality_metric: host=algo-1, epoch=82, batch=0 train loss <loss>=-2.5454280376434326\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:38:28 INFO 139946820503360] Epoch[82] Batch[5] avg_epoch_loss=-3.070198\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:38:28 INFO 139946820503360] #quality_metric: host=algo-1, epoch=82, batch=5 train loss <loss>=-3.0701976219813027\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:38:28 INFO 139946820503360] Epoch[82] Batch [5]#011Speed: 71.03 samples/sec#011loss=-3.070198\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:38:33 INFO 139946820503360] Epoch[82] Batch[10] avg_epoch_loss=-2.899567\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:38:33 INFO 139946820503360] #quality_metric: host=algo-1, epoch=82, batch=10 train loss <loss>=-2.6948110312223434\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:38:33 INFO 139946820503360] Epoch[82] Batch [10]#011Speed: 68.36 samples/sec#011loss=-2.694811\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:38:33 INFO 139946820503360] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197496.8554494, \"EndTime\": 1666197513.3545601, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16498.711347579956, \"count\": 1, \"min\": 16498.711347579956, \"max\": 16498.711347579956}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:38:33 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=39.27553023488348 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:38:33 INFO 139946820503360] #progress_metric: host=algo-1, completed 20.75 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:38:33 INFO 139946820503360] #quality_metric: host=algo-1, epoch=82, train loss <loss>=-2.8995673534545032\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:38:33 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:38:40 INFO 139946820503360] Epoch[83] Batch[0] avg_epoch_loss=-3.158204\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:38:40 INFO 139946820503360] #quality_metric: host=algo-1, epoch=83, batch=0 train loss <loss>=-3.1582038402557373\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:38:44 INFO 139946820503360] Epoch[83] Batch[5] avg_epoch_loss=-3.349322\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:38:44 INFO 139946820503360] #quality_metric: host=algo-1, epoch=83, batch=5 train loss <loss>=-3.349321722984314\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:38:44 INFO 139946820503360] Epoch[83] Batch [5]#011Speed: 75.40 samples/sec#011loss=-3.349322\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:38:48 INFO 139946820503360] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197513.3546374, \"EndTime\": 1666197528.3131473, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 14957.931280136108, \"count\": 1, \"min\": 14957.931280136108, \"max\": 14957.931280136108}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:38:48 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=41.58300471821612 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:38:48 INFO 139946820503360] #progress_metric: host=algo-1, completed 21.0 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:38:48 INFO 139946820503360] #quality_metric: host=algo-1, epoch=83, train loss <loss>=-3.4515294790267945\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:38:48 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:38:55 INFO 139946820503360] Epoch[84] Batch[0] avg_epoch_loss=-3.792001\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:38:55 INFO 139946820503360] #quality_metric: host=algo-1, epoch=84, batch=0 train loss <loss>=-3.7920010089874268\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:39:00 INFO 139946820503360] Epoch[84] Batch[5] avg_epoch_loss=-3.361944\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:39:00 INFO 139946820503360] #quality_metric: host=algo-1, epoch=84, batch=5 train loss <loss>=-3.361944238344828\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:39:00 INFO 139946820503360] Epoch[84] Batch [5]#011Speed: 71.49 samples/sec#011loss=-3.361944\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:39:03 INFO 139946820503360] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197528.313219, \"EndTime\": 1666197543.902771, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15589.190006256104, \"count\": 1, \"min\": 15589.190006256104, \"max\": 15589.190006256104}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:39:03 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=41.05377482498131 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:39:03 INFO 139946820503360] #progress_metric: host=algo-1, completed 21.25 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:39:03 INFO 139946820503360] #quality_metric: host=algo-1, epoch=84, train loss <loss>=-3.429871845245361\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:39:03 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:39:11 INFO 139946820503360] Epoch[85] Batch[0] avg_epoch_loss=-3.692774\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:39:11 INFO 139946820503360] #quality_metric: host=algo-1, epoch=85, batch=0 train loss <loss>=-3.6927738189697266\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:39:15 INFO 139946820503360] Epoch[85] Batch[5] avg_epoch_loss=-3.604936\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:39:15 INFO 139946820503360] #quality_metric: host=algo-1, epoch=85, batch=5 train loss <loss>=-3.604936361312866\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:39:15 INFO 139946820503360] Epoch[85] Batch [5]#011Speed: 73.31 samples/sec#011loss=-3.604936\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:39:20 INFO 139946820503360] Epoch[85] Batch[10] avg_epoch_loss=-3.387773\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:39:20 INFO 139946820503360] #quality_metric: host=algo-1, epoch=85, batch=10 train loss <loss>=-3.1271769523620607\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:39:20 INFO 139946820503360] Epoch[85] Batch [10]#011Speed: 68.77 samples/sec#011loss=-3.127177\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:39:20 INFO 139946820503360] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197543.902857, \"EndTime\": 1666197560.189455, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16286.16714477539, \"count\": 1, \"min\": 16286.16714477539, \"max\": 16286.16714477539}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:39:20 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.09515148247494 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:39:20 INFO 139946820503360] #progress_metric: host=algo-1, completed 21.5 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:39:20 INFO 139946820503360] #quality_metric: host=algo-1, epoch=85, train loss <loss>=-3.3877729936079546\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:39:20 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:39:27 INFO 139946820503360] Epoch[86] Batch[0] avg_epoch_loss=-3.722320\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:39:27 INFO 139946820503360] #quality_metric: host=algo-1, epoch=86, batch=0 train loss <loss>=-3.722320318222046\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:39:31 INFO 139946820503360] Epoch[86] Batch[5] avg_epoch_loss=-3.302082\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:39:31 INFO 139946820503360] #quality_metric: host=algo-1, epoch=86, batch=5 train loss <loss>=-3.3020817836125693\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:39:31 INFO 139946820503360] Epoch[86] Batch [5]#011Speed: 71.02 samples/sec#011loss=-3.302082\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:39:36 INFO 139946820503360] Epoch[86] Batch[10] avg_epoch_loss=-3.439703\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:39:36 INFO 139946820503360] #quality_metric: host=algo-1, epoch=86, batch=10 train loss <loss>=-3.6048487186431886\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:39:36 INFO 139946820503360] Epoch[86] Batch [10]#011Speed: 69.42 samples/sec#011loss=-3.604849\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:39:36 INFO 139946820503360] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197560.1895168, \"EndTime\": 1666197576.3475568, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16157.6669216156, \"count\": 1, \"min\": 16157.6669216156, \"max\": 16157.6669216156}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:39:36 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.84727833808053 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:39:36 INFO 139946820503360] #progress_metric: host=algo-1, completed 21.75 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:39:36 INFO 139946820503360] #quality_metric: host=algo-1, epoch=86, train loss <loss>=-3.439703117717396\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:39:36 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:39:43 INFO 139946820503360] Epoch[87] Batch[0] avg_epoch_loss=-3.470423\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:39:43 INFO 139946820503360] #quality_metric: host=algo-1, epoch=87, batch=0 train loss <loss>=-3.4704229831695557\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:39:47 INFO 139946820503360] Epoch[87] Batch[5] avg_epoch_loss=-3.519913\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:39:47 INFO 139946820503360] #quality_metric: host=algo-1, epoch=87, batch=5 train loss <loss>=-3.5199128786722818\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:39:47 INFO 139946820503360] Epoch[87] Batch [5]#011Speed: 75.83 samples/sec#011loss=-3.519913\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:39:51 INFO 139946820503360] Epoch[87] Batch[10] avg_epoch_loss=-3.217942\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:39:51 INFO 139946820503360] #quality_metric: host=algo-1, epoch=87, batch=10 train loss <loss>=-2.855576515197754\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:39:51 INFO 139946820503360] Epoch[87] Batch [10]#011Speed: 75.54 samples/sec#011loss=-2.855577\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:39:51 INFO 139946820503360] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197576.3476098, \"EndTime\": 1666197591.9810896, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15633.159637451172, \"count\": 1, \"min\": 15633.159637451172, \"max\": 15633.159637451172}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:39:51 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=41.00236577139268 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:39:51 INFO 139946820503360] #progress_metric: host=algo-1, completed 22.0 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:39:51 INFO 139946820503360] #quality_metric: host=algo-1, epoch=87, train loss <loss>=-3.2179418043656782\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:39:51 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:39:59 INFO 139946820503360] Epoch[88] Batch[0] avg_epoch_loss=-3.136774\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:39:59 INFO 139946820503360] #quality_metric: host=algo-1, epoch=88, batch=0 train loss <loss>=-3.1367735862731934\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:40:03 INFO 139946820503360] Epoch[88] Batch[5] avg_epoch_loss=-3.300712\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:40:03 INFO 139946820503360] #quality_metric: host=algo-1, epoch=88, batch=5 train loss <loss>=-3.3007115523020425\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:40:03 INFO 139946820503360] Epoch[88] Batch [5]#011Speed: 71.11 samples/sec#011loss=-3.300712\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:40:08 INFO 139946820503360] Epoch[88] Batch[10] avg_epoch_loss=-3.303744\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:40:08 INFO 139946820503360] #quality_metric: host=algo-1, epoch=88, batch=10 train loss <loss>=-3.307383346557617\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:40:08 INFO 139946820503360] Epoch[88] Batch [10]#011Speed: 67.03 samples/sec#011loss=-3.307383\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:40:08 INFO 139946820503360] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197591.981148, \"EndTime\": 1666197608.373819, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16392.349004745483, \"count\": 1, \"min\": 16392.349004745483, \"max\": 16392.349004745483}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:40:08 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.32346305241277 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:40:08 INFO 139946820503360] #progress_metric: host=algo-1, completed 22.25 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:40:08 INFO 139946820503360] #quality_metric: host=algo-1, epoch=88, train loss <loss>=-3.3037441860545766\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:40:08 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:40:15 INFO 139946820503360] Epoch[89] Batch[0] avg_epoch_loss=-3.085580\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:40:15 INFO 139946820503360] #quality_metric: host=algo-1, epoch=89, batch=0 train loss <loss>=-3.0855796337127686\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:40:20 INFO 139946820503360] Epoch[89] Batch[5] avg_epoch_loss=-3.349970\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:40:20 INFO 139946820503360] #quality_metric: host=algo-1, epoch=89, batch=5 train loss <loss>=-3.3499702612559\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:40:20 INFO 139946820503360] Epoch[89] Batch [5]#011Speed: 73.54 samples/sec#011loss=-3.349970\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:40:23 INFO 139946820503360] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197608.3738785, \"EndTime\": 1666197623.602046, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15227.834463119507, \"count\": 1, \"min\": 15227.834463119507, \"max\": 15227.834463119507}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:40:23 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=41.83103461389669 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:40:23 INFO 139946820503360] #progress_metric: host=algo-1, completed 22.5 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:40:23 INFO 139946820503360] #quality_metric: host=algo-1, epoch=89, train loss <loss>=-3.2986981868743896\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:40:23 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:40:30 INFO 139946820503360] Epoch[90] Batch[0] avg_epoch_loss=-2.644801\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:40:30 INFO 139946820503360] #quality_metric: host=algo-1, epoch=90, batch=0 train loss <loss>=-2.6448006629943848\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:40:35 INFO 139946820503360] Epoch[90] Batch[5] avg_epoch_loss=-2.849364\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:40:35 INFO 139946820503360] #quality_metric: host=algo-1, epoch=90, batch=5 train loss <loss>=-2.849364439646403\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:40:35 INFO 139946820503360] Epoch[90] Batch [5]#011Speed: 69.91 samples/sec#011loss=-2.849364\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:40:40 INFO 139946820503360] Epoch[90] Batch[10] avg_epoch_loss=-3.067531\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:40:40 INFO 139946820503360] #quality_metric: host=algo-1, epoch=90, batch=10 train loss <loss>=-3.3293310165405274\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:40:40 INFO 139946820503360] Epoch[90] Batch [10]#011Speed: 63.94 samples/sec#011loss=-3.329331\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:40:40 INFO 139946820503360] processed a total of 681 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197623.6021094, \"EndTime\": 1666197640.0749156, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16472.389459609985, \"count\": 1, \"min\": 16472.389459609985, \"max\": 16472.389459609985}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:40:40 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=41.34170457006652 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:40:40 INFO 139946820503360] #progress_metric: host=algo-1, completed 22.75 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:40:40 INFO 139946820503360] #quality_metric: host=algo-1, epoch=90, train loss <loss>=-3.0675310655073686\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:40:40 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:40:46 INFO 139946820503360] Epoch[91] Batch[0] avg_epoch_loss=-2.980754\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:40:46 INFO 139946820503360] #quality_metric: host=algo-1, epoch=91, batch=0 train loss <loss>=-2.9807536602020264\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:40:51 INFO 139946820503360] Epoch[91] Batch[5] avg_epoch_loss=-2.851936\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:40:51 INFO 139946820503360] #quality_metric: host=algo-1, epoch=91, batch=5 train loss <loss>=-2.851935545603434\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:40:51 INFO 139946820503360] Epoch[91] Batch [5]#011Speed: 72.29 samples/sec#011loss=-2.851936\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:40:56 INFO 139946820503360] Epoch[91] Batch[10] avg_epoch_loss=-2.921714\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:40:56 INFO 139946820503360] #quality_metric: host=algo-1, epoch=91, batch=10 train loss <loss>=-3.005448055267334\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:40:56 INFO 139946820503360] Epoch[91] Batch [10]#011Speed: 65.05 samples/sec#011loss=-3.005448\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:40:56 INFO 139946820503360] processed a total of 669 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197640.0749698, \"EndTime\": 1666197656.3169463, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16241.700172424316, \"count\": 1, \"min\": 16241.700172424316, \"max\": 16241.700172424316}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:40:56 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=41.19003928892864 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:40:56 INFO 139946820503360] #progress_metric: host=algo-1, completed 23.0 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:40:56 INFO 139946820503360] #quality_metric: host=algo-1, epoch=91, train loss <loss>=-2.921713959087025\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:40:56 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:41:03 INFO 139946820503360] Epoch[92] Batch[0] avg_epoch_loss=-3.149186\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:41:03 INFO 139946820503360] #quality_metric: host=algo-1, epoch=92, batch=0 train loss <loss>=-3.1491858959198\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:41:07 INFO 139946820503360] Epoch[92] Batch[5] avg_epoch_loss=-3.350258\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:41:07 INFO 139946820503360] #quality_metric: host=algo-1, epoch=92, batch=5 train loss <loss>=-3.350258151690165\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:41:07 INFO 139946820503360] Epoch[92] Batch [5]#011Speed: 70.65 samples/sec#011loss=-3.350258\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:41:12 INFO 139946820503360] Epoch[92] Batch[10] avg_epoch_loss=-3.471430\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:41:12 INFO 139946820503360] #quality_metric: host=algo-1, epoch=92, batch=10 train loss <loss>=-3.61683669090271\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:41:12 INFO 139946820503360] Epoch[92] Batch [10]#011Speed: 65.00 samples/sec#011loss=-3.616837\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:41:12 INFO 139946820503360] processed a total of 673 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197656.3170092, \"EndTime\": 1666197672.796072, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16478.793382644653, \"count\": 1, \"min\": 16478.793382644653, \"max\": 16478.793382644653}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:41:12 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.84014904429115 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:41:12 INFO 139946820503360] #progress_metric: host=algo-1, completed 23.25 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:41:12 INFO 139946820503360] #quality_metric: host=algo-1, epoch=92, train loss <loss>=-3.4714302149685947\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:41:12 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:41:19 INFO 139946820503360] Epoch[93] Batch[0] avg_epoch_loss=-2.860373\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:41:19 INFO 139946820503360] #quality_metric: host=algo-1, epoch=93, batch=0 train loss <loss>=-2.8603732585906982\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:41:24 INFO 139946820503360] Epoch[93] Batch[5] avg_epoch_loss=-3.432431\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:41:24 INFO 139946820503360] #quality_metric: host=algo-1, epoch=93, batch=5 train loss <loss>=-3.4324307839075723\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:41:24 INFO 139946820503360] Epoch[93] Batch [5]#011Speed: 72.37 samples/sec#011loss=-3.432431\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:41:29 INFO 139946820503360] Epoch[93] Batch[10] avg_epoch_loss=-3.644655\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:41:29 INFO 139946820503360] #quality_metric: host=algo-1, epoch=93, batch=10 train loss <loss>=-3.8993233680725097\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:41:29 INFO 139946820503360] Epoch[93] Batch [10]#011Speed: 66.94 samples/sec#011loss=-3.899323\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:41:29 INFO 139946820503360] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197672.7961345, \"EndTime\": 1666197689.1167724, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16320.314884185791, \"count\": 1, \"min\": 16320.314884185791, \"max\": 16320.314884185791}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:41:29 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.37890933310411 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:41:29 INFO 139946820503360] #progress_metric: host=algo-1, completed 23.5 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:41:29 INFO 139946820503360] #quality_metric: host=algo-1, epoch=93, train loss <loss>=-3.6446546858007256\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:41:29 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:41:36 INFO 139946820503360] Epoch[94] Batch[0] avg_epoch_loss=-3.449066\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:41:36 INFO 139946820503360] #quality_metric: host=algo-1, epoch=94, batch=0 train loss <loss>=-3.449066162109375\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:41:41 INFO 139946820503360] Epoch[94] Batch[5] avg_epoch_loss=-3.435463\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:41:41 INFO 139946820503360] #quality_metric: host=algo-1, epoch=94, batch=5 train loss <loss>=-3.4354629516601562\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:41:41 INFO 139946820503360] Epoch[94] Batch [5]#011Speed: 72.76 samples/sec#011loss=-3.435463\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:41:44 INFO 139946820503360] processed a total of 598 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197689.1168327, \"EndTime\": 1666197704.760811, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15643.716096878052, \"count\": 1, \"min\": 15643.716096878052, \"max\": 15643.716096878052}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:41:44 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=38.225932431892836 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:41:44 INFO 139946820503360] #progress_metric: host=algo-1, completed 23.75 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:41:44 INFO 139946820503360] #quality_metric: host=algo-1, epoch=94, train loss <loss>=-3.162704515457153\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:41:44 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:41:51 INFO 139946820503360] Epoch[95] Batch[0] avg_epoch_loss=-2.941742\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:41:51 INFO 139946820503360] #quality_metric: host=algo-1, epoch=95, batch=0 train loss <loss>=-2.941741943359375\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:41:55 INFO 139946820503360] Epoch[95] Batch[5] avg_epoch_loss=-3.410949\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:41:55 INFO 139946820503360] #quality_metric: host=algo-1, epoch=95, batch=5 train loss <loss>=-3.410948872566223\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:41:55 INFO 139946820503360] Epoch[95] Batch [5]#011Speed: 76.19 samples/sec#011loss=-3.410949\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:42:00 INFO 139946820503360] Epoch[95] Batch[10] avg_epoch_loss=-3.447737\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:42:00 INFO 139946820503360] #quality_metric: host=algo-1, epoch=95, batch=10 train loss <loss>=-3.4918824195861817\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:42:00 INFO 139946820503360] Epoch[95] Batch [10]#011Speed: 65.00 samples/sec#011loss=-3.491882\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:42:01 INFO 139946820503360] processed a total of 710 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197704.7608922, \"EndTime\": 1666197721.5618358, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16800.55284500122, \"count\": 1, \"min\": 16800.55284500122, \"max\": 16800.55284500122}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:42:01 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=42.26026279376409 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:42:01 INFO 139946820503360] #progress_metric: host=algo-1, completed 24.0 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:42:01 INFO 139946820503360] #quality_metric: host=algo-1, epoch=95, train loss <loss>=-3.6390803456306458\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:42:01 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:42:09 INFO 139946820503360] Epoch[96] Batch[0] avg_epoch_loss=-3.304120\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:42:09 INFO 139946820503360] #quality_metric: host=algo-1, epoch=96, batch=0 train loss <loss>=-3.3041200637817383\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:42:13 INFO 139946820503360] Epoch[96] Batch[5] avg_epoch_loss=-3.291071\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:42:13 INFO 139946820503360] #quality_metric: host=algo-1, epoch=96, batch=5 train loss <loss>=-3.2910714149475098\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:42:13 INFO 139946820503360] Epoch[96] Batch [5]#011Speed: 70.60 samples/sec#011loss=-3.291071\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:42:17 INFO 139946820503360] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197721.561905, \"EndTime\": 1666197737.004874, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15442.578077316284, \"count\": 1, \"min\": 15442.578077316284, \"max\": 15442.578077316284}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:42:17 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.472254318491466 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:42:17 INFO 139946820503360] #progress_metric: host=algo-1, completed 24.25 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:42:17 INFO 139946820503360] #quality_metric: host=algo-1, epoch=96, train loss <loss>=-3.2986212253570555\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:42:17 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:42:23 INFO 139946820503360] Epoch[97] Batch[0] avg_epoch_loss=-3.236245\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:42:23 INFO 139946820503360] #quality_metric: host=algo-1, epoch=97, batch=0 train loss <loss>=-3.2362449169158936\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:42:28 INFO 139946820503360] Epoch[97] Batch[5] avg_epoch_loss=-3.297712\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:42:28 INFO 139946820503360] #quality_metric: host=algo-1, epoch=97, batch=5 train loss <loss>=-3.2977121671040854\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:42:28 INFO 139946820503360] Epoch[97] Batch [5]#011Speed: 75.24 samples/sec#011loss=-3.297712\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:42:33 INFO 139946820503360] Epoch[97] Batch[10] avg_epoch_loss=-3.583343\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:42:33 INFO 139946820503360] #quality_metric: host=algo-1, epoch=97, batch=10 train loss <loss>=-3.926099491119385\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:42:33 INFO 139946820503360] Epoch[97] Batch [10]#011Speed: 62.59 samples/sec#011loss=-3.926099\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:42:33 INFO 139946820503360] processed a total of 695 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197737.0049431, \"EndTime\": 1666197753.1602159, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16154.954195022583, \"count\": 1, \"min\": 16154.954195022583, \"max\": 16154.954195022583}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:42:33 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=43.02061224858147 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:42:33 INFO 139946820503360] #progress_metric: host=algo-1, completed 24.5 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:42:33 INFO 139946820503360] #quality_metric: host=algo-1, epoch=97, train loss <loss>=-3.583342768929221\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:42:33 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:42:40 INFO 139946820503360] Epoch[98] Batch[0] avg_epoch_loss=-3.379673\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:42:40 INFO 139946820503360] #quality_metric: host=algo-1, epoch=98, batch=0 train loss <loss>=-3.379673480987549\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:42:44 INFO 139946820503360] Epoch[98] Batch[5] avg_epoch_loss=-3.571913\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:42:44 INFO 139946820503360] #quality_metric: host=algo-1, epoch=98, batch=5 train loss <loss>=-3.5719133218129477\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:42:44 INFO 139946820503360] Epoch[98] Batch [5]#011Speed: 70.39 samples/sec#011loss=-3.571913\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:42:49 INFO 139946820503360] Epoch[98] Batch[10] avg_epoch_loss=-3.858306\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:42:49 INFO 139946820503360] #quality_metric: host=algo-1, epoch=98, batch=10 train loss <loss>=-4.201978158950806\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:42:49 INFO 139946820503360] Epoch[98] Batch [10]#011Speed: 66.32 samples/sec#011loss=-4.201978\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:42:49 INFO 139946820503360] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197753.1602812, \"EndTime\": 1666197769.620069, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16459.50484275818, \"count\": 1, \"min\": 16459.50484275818, \"max\": 16459.50484275818}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:42:49 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.401925268207314 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:42:49 INFO 139946820503360] #progress_metric: host=algo-1, completed 24.75 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:42:49 INFO 139946820503360] #quality_metric: host=algo-1, epoch=98, train loss <loss>=-3.8583064296028833\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:42:49 INFO 139946820503360] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:42:49 INFO 139946820503360] Saved checkpoint to \"/opt/ml/model/state_01f4e05e-2b7c-45f6-ab5f-50d24d5bdaae-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197769.620144, \"EndTime\": 1666197769.7975838, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 177.016019821167, \"count\": 1, \"min\": 177.016019821167, \"max\": 177.016019821167}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:42:57 INFO 139946820503360] Epoch[99] Batch[0] avg_epoch_loss=-3.431725\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:42:57 INFO 139946820503360] #quality_metric: host=algo-1, epoch=99, batch=0 train loss <loss>=-3.4317245483398438\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:43:01 INFO 139946820503360] Epoch[99] Batch[5] avg_epoch_loss=-3.646341\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:43:01 INFO 139946820503360] #quality_metric: host=algo-1, epoch=99, batch=5 train loss <loss>=-3.6463406880696616\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:43:01 INFO 139946820503360] Epoch[99] Batch [5]#011Speed: 71.29 samples/sec#011loss=-3.646341\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:43:05 INFO 139946820503360] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197769.7976565, \"EndTime\": 1666197785.1499, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15352.186679840088, \"count\": 1, \"min\": 15352.186679840088, \"max\": 15352.186679840088}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:43:05 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.38488245620462 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:43:05 INFO 139946820503360] #progress_metric: host=algo-1, completed 25.0 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:43:05 INFO 139946820503360] #quality_metric: host=algo-1, epoch=99, train loss <loss>=-3.5846065521240233\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:43:05 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:43:12 INFO 139946820503360] Epoch[100] Batch[0] avg_epoch_loss=-3.335985\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:43:12 INFO 139946820503360] #quality_metric: host=algo-1, epoch=100, batch=0 train loss <loss>=-3.3359854221343994\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:43:16 INFO 139946820503360] Epoch[100] Batch[5] avg_epoch_loss=-3.361622\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:43:16 INFO 139946820503360] #quality_metric: host=algo-1, epoch=100, batch=5 train loss <loss>=-3.3616220156351724\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:43:16 INFO 139946820503360] Epoch[100] Batch [5]#011Speed: 72.17 samples/sec#011loss=-3.361622\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:43:21 INFO 139946820503360] Epoch[100] Batch[10] avg_epoch_loss=-3.673070\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:43:21 INFO 139946820503360] #quality_metric: host=algo-1, epoch=100, batch=10 train loss <loss>=-4.046807909011841\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:43:21 INFO 139946820503360] Epoch[100] Batch [10]#011Speed: 66.77 samples/sec#011loss=-4.046808\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:43:21 INFO 139946820503360] processed a total of 672 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197785.1499639, \"EndTime\": 1666197801.3351512, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16184.782981872559, \"count\": 1, \"min\": 16184.782981872559, \"max\": 16184.782981872559}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:43:21 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=41.520242827309744 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:43:21 INFO 139946820503360] #progress_metric: host=algo-1, completed 25.25 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:43:21 INFO 139946820503360] #quality_metric: host=algo-1, epoch=100, train loss <loss>=-3.6730701489882036\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:43:21 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:43:28 INFO 139946820503360] Epoch[101] Batch[0] avg_epoch_loss=-3.039105\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:43:28 INFO 139946820503360] #quality_metric: host=algo-1, epoch=101, batch=0 train loss <loss>=-3.03910493850708\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:43:33 INFO 139946820503360] Epoch[101] Batch[5] avg_epoch_loss=-3.514506\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:43:33 INFO 139946820503360] #quality_metric: host=algo-1, epoch=101, batch=5 train loss <loss>=-3.5145061016082764\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:43:33 INFO 139946820503360] Epoch[101] Batch [5]#011Speed: 69.96 samples/sec#011loss=-3.514506\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:43:37 INFO 139946820503360] Epoch[101] Batch[10] avg_epoch_loss=-3.260881\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:43:37 INFO 139946820503360] #quality_metric: host=algo-1, epoch=101, batch=10 train loss <loss>=-2.9565315723419188\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:43:37 INFO 139946820503360] Epoch[101] Batch [10]#011Speed: 73.70 samples/sec#011loss=-2.956532\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:43:37 INFO 139946820503360] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197801.3352141, \"EndTime\": 1666197817.4976838, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16162.109851837158, \"count\": 1, \"min\": 16162.109851837158, \"max\": 16162.109851837158}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:43:37 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.0316659281518 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:43:37 INFO 139946820503360] #progress_metric: host=algo-1, completed 25.5 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:43:37 INFO 139946820503360] #quality_metric: host=algo-1, epoch=101, train loss <loss>=-3.260881315578114\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:43:37 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:43:44 INFO 139946820503360] Epoch[102] Batch[0] avg_epoch_loss=-3.292530\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:43:44 INFO 139946820503360] #quality_metric: host=algo-1, epoch=102, batch=0 train loss <loss>=-3.292530059814453\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:43:49 INFO 139946820503360] Epoch[102] Batch[5] avg_epoch_loss=-3.452622\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:43:49 INFO 139946820503360] #quality_metric: host=algo-1, epoch=102, batch=5 train loss <loss>=-3.452622095743815\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:43:49 INFO 139946820503360] Epoch[102] Batch [5]#011Speed: 71.33 samples/sec#011loss=-3.452622\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:43:54 INFO 139946820503360] Epoch[102] Batch[10] avg_epoch_loss=-3.698204\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:43:54 INFO 139946820503360] #quality_metric: host=algo-1, epoch=102, batch=10 train loss <loss>=-3.9929025173187256\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:43:54 INFO 139946820503360] Epoch[102] Batch [10]#011Speed: 64.16 samples/sec#011loss=-3.992903\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:43:54 INFO 139946820503360] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197817.4977498, \"EndTime\": 1666197834.0855663, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16587.538480758667, \"count\": 1, \"min\": 16587.538480758667, \"max\": 16587.538480758667}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:43:54 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.029832828584574 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:43:54 INFO 139946820503360] #progress_metric: host=algo-1, completed 25.75 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:43:54 INFO 139946820503360] #quality_metric: host=algo-1, epoch=102, train loss <loss>=-3.6982041055505928\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:43:54 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:44:01 INFO 139946820503360] Epoch[103] Batch[0] avg_epoch_loss=-4.239718\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:44:01 INFO 139946820503360] #quality_metric: host=algo-1, epoch=103, batch=0 train loss <loss>=-4.239718437194824\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:44:05 INFO 139946820503360] Epoch[103] Batch[5] avg_epoch_loss=-3.158356\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:44:05 INFO 139946820503360] #quality_metric: host=algo-1, epoch=103, batch=5 train loss <loss>=-3.1583564281463623\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:44:05 INFO 139946820503360] Epoch[103] Batch [5]#011Speed: 70.80 samples/sec#011loss=-3.158356\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:44:10 INFO 139946820503360] Epoch[103] Batch[10] avg_epoch_loss=-3.091263\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:44:10 INFO 139946820503360] #quality_metric: host=algo-1, epoch=103, batch=10 train loss <loss>=-3.010750579833984\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:44:10 INFO 139946820503360] Epoch[103] Batch [10]#011Speed: 69.99 samples/sec#011loss=-3.010751\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:44:10 INFO 139946820503360] processed a total of 677 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197834.0856285, \"EndTime\": 1666197850.1020186, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16016.112089157104, \"count\": 1, \"min\": 16016.112089157104, \"max\": 16016.112089157104}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:44:10 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=42.269691685948615 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:44:10 INFO 139946820503360] #progress_metric: host=algo-1, completed 26.0 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:44:10 INFO 139946820503360] #quality_metric: host=algo-1, epoch=103, train loss <loss>=-3.091262860731645\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:44:10 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:44:17 INFO 139946820503360] Epoch[104] Batch[0] avg_epoch_loss=-3.227422\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:44:17 INFO 139946820503360] #quality_metric: host=algo-1, epoch=104, batch=0 train loss <loss>=-3.227421998977661\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:44:21 INFO 139946820503360] Epoch[104] Batch[5] avg_epoch_loss=-2.851743\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:44:21 INFO 139946820503360] #quality_metric: host=algo-1, epoch=104, batch=5 train loss <loss>=-2.851743221282959\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:44:21 INFO 139946820503360] Epoch[104] Batch [5]#011Speed: 70.10 samples/sec#011loss=-2.851743\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:44:26 INFO 139946820503360] Epoch[104] Batch[10] avg_epoch_loss=-2.988608\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:44:26 INFO 139946820503360] #quality_metric: host=algo-1, epoch=104, batch=10 train loss <loss>=-3.1528451919555662\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:44:26 INFO 139946820503360] Epoch[104] Batch [10]#011Speed: 70.03 samples/sec#011loss=-3.152845\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:44:26 INFO 139946820503360] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197850.1020806, \"EndTime\": 1666197866.3401625, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16237.727403640747, \"count\": 1, \"min\": 16237.727403640747, \"max\": 16237.727403640747}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:44:26 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.337961711175154 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:44:26 INFO 139946820503360] #progress_metric: host=algo-1, completed 26.25 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:44:26 INFO 139946820503360] #quality_metric: host=algo-1, epoch=104, train loss <loss>=-2.9886077534068716\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:44:26 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:44:33 INFO 139946820503360] Epoch[105] Batch[0] avg_epoch_loss=-3.361887\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:44:33 INFO 139946820503360] #quality_metric: host=algo-1, epoch=105, batch=0 train loss <loss>=-3.361886739730835\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:44:37 INFO 139946820503360] Epoch[105] Batch[5] avg_epoch_loss=-2.907211\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:44:37 INFO 139946820503360] #quality_metric: host=algo-1, epoch=105, batch=5 train loss <loss>=-2.907211343447367\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:44:37 INFO 139946820503360] Epoch[105] Batch [5]#011Speed: 72.88 samples/sec#011loss=-2.907211\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:44:42 INFO 139946820503360] Epoch[105] Batch[10] avg_epoch_loss=-3.260269\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:44:42 INFO 139946820503360] #quality_metric: host=algo-1, epoch=105, batch=10 train loss <loss>=-3.6839387893676756\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:44:42 INFO 139946820503360] Epoch[105] Batch [10]#011Speed: 62.98 samples/sec#011loss=-3.683939\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:44:42 INFO 139946820503360] processed a total of 683 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197866.340215, \"EndTime\": 1666197882.6688774, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16328.338861465454, \"count\": 1, \"min\": 16328.338861465454, \"max\": 16328.338861465454}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:44:42 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=41.828885724983984 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:44:42 INFO 139946820503360] #progress_metric: host=algo-1, completed 26.5 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:44:42 INFO 139946820503360] #quality_metric: host=algo-1, epoch=105, train loss <loss>=-3.260269273411144\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:44:42 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:44:49 INFO 139946820503360] Epoch[106] Batch[0] avg_epoch_loss=-3.086478\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:44:49 INFO 139946820503360] #quality_metric: host=algo-1, epoch=106, batch=0 train loss <loss>=-3.0864782333374023\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:44:54 INFO 139946820503360] Epoch[106] Batch[5] avg_epoch_loss=-3.296452\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:44:54 INFO 139946820503360] #quality_metric: host=algo-1, epoch=106, batch=5 train loss <loss>=-3.296452005704244\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:44:54 INFO 139946820503360] Epoch[106] Batch [5]#011Speed: 71.03 samples/sec#011loss=-3.296452\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:44:58 INFO 139946820503360] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197882.6689384, \"EndTime\": 1666197898.0884626, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15419.211149215698, \"count\": 1, \"min\": 15419.211149215698, \"max\": 15419.211149215698}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:44:58 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.59844542889372 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:44:58 INFO 139946820503360] #progress_metric: host=algo-1, completed 26.75 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:44:58 INFO 139946820503360] #quality_metric: host=algo-1, epoch=106, train loss <loss>=-3.3777609348297117\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:44:58 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:45:05 INFO 139946820503360] Epoch[107] Batch[0] avg_epoch_loss=-3.569636\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:45:05 INFO 139946820503360] #quality_metric: host=algo-1, epoch=107, batch=0 train loss <loss>=-3.5696356296539307\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:45:09 INFO 139946820503360] Epoch[107] Batch[5] avg_epoch_loss=-3.157997\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:45:09 INFO 139946820503360] #quality_metric: host=algo-1, epoch=107, batch=5 train loss <loss>=-3.1579970916112265\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:45:09 INFO 139946820503360] Epoch[107] Batch [5]#011Speed: 70.20 samples/sec#011loss=-3.157997\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:45:14 INFO 139946820503360] Epoch[107] Batch[10] avg_epoch_loss=-3.464983\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:45:14 INFO 139946820503360] #quality_metric: host=algo-1, epoch=107, batch=10 train loss <loss>=-3.8333669185638426\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:45:14 INFO 139946820503360] Epoch[107] Batch [10]#011Speed: 64.67 samples/sec#011loss=-3.833367\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:45:14 INFO 139946820503360] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197898.088531, \"EndTime\": 1666197914.7237818, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16634.912967681885, \"count\": 1, \"min\": 16634.912967681885, \"max\": 16634.912967681885}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:45:14 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.09618564356868 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:45:14 INFO 139946820503360] #progress_metric: host=algo-1, completed 27.0 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:45:14 INFO 139946820503360] #quality_metric: host=algo-1, epoch=107, train loss <loss>=-3.4649833765896885\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:45:14 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:45:21 INFO 139946820503360] Epoch[108] Batch[0] avg_epoch_loss=-3.199731\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:45:21 INFO 139946820503360] #quality_metric: host=algo-1, epoch=108, batch=0 train loss <loss>=-3.1997311115264893\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:45:26 INFO 139946820503360] Epoch[108] Batch[5] avg_epoch_loss=-3.371449\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:45:26 INFO 139946820503360] #quality_metric: host=algo-1, epoch=108, batch=5 train loss <loss>=-3.37144943078359\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:45:26 INFO 139946820503360] Epoch[108] Batch [5]#011Speed: 74.60 samples/sec#011loss=-3.371449\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:45:30 INFO 139946820503360] Epoch[108] Batch[10] avg_epoch_loss=-3.709791\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:45:30 INFO 139946820503360] #quality_metric: host=algo-1, epoch=108, batch=10 train loss <loss>=-4.115800094604492\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:45:30 INFO 139946820503360] Epoch[108] Batch [10]#011Speed: 70.55 samples/sec#011loss=-4.115800\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:45:30 INFO 139946820503360] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197914.7238395, \"EndTime\": 1666197930.6054132, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15881.239891052246, \"count\": 1, \"min\": 15881.239891052246, \"max\": 15881.239891052246}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:45:30 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=41.054493565865045 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:45:30 INFO 139946820503360] #progress_metric: host=algo-1, completed 27.25 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:45:30 INFO 139946820503360] #quality_metric: host=algo-1, epoch=108, train loss <loss>=-3.7097906416112725\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:45:30 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:45:37 INFO 139946820503360] Epoch[109] Batch[0] avg_epoch_loss=-3.680118\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:45:37 INFO 139946820503360] #quality_metric: host=algo-1, epoch=109, batch=0 train loss <loss>=-3.6801180839538574\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:45:42 INFO 139946820503360] Epoch[109] Batch[5] avg_epoch_loss=-3.687949\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:45:42 INFO 139946820503360] #quality_metric: host=algo-1, epoch=109, batch=5 train loss <loss>=-3.6879493395487466\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:45:42 INFO 139946820503360] Epoch[109] Batch [5]#011Speed: 74.79 samples/sec#011loss=-3.687949\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:45:45 INFO 139946820503360] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197930.605476, \"EndTime\": 1666197945.5790272, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 14973.235607147217, \"count\": 1, \"min\": 14973.235607147217, \"max\": 14973.235607147217}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:45:45 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=41.54050449717711 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:45:45 INFO 139946820503360] #progress_metric: host=algo-1, completed 27.5 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:45:45 INFO 139946820503360] #quality_metric: host=algo-1, epoch=109, train loss <loss>=-3.697674608230591\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:45:45 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:45:52 INFO 139946820503360] Epoch[110] Batch[0] avg_epoch_loss=-3.588320\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:45:52 INFO 139946820503360] #quality_metric: host=algo-1, epoch=110, batch=0 train loss <loss>=-3.588319778442383\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:45:57 INFO 139946820503360] Epoch[110] Batch[5] avg_epoch_loss=-3.660643\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:45:57 INFO 139946820503360] #quality_metric: host=algo-1, epoch=110, batch=5 train loss <loss>=-3.660642941792806\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:45:57 INFO 139946820503360] Epoch[110] Batch [5]#011Speed: 75.39 samples/sec#011loss=-3.660643\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:46:00 INFO 139946820503360] processed a total of 619 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197945.5790987, \"EndTime\": 1666197960.55978, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 14980.350017547607, \"count\": 1, \"min\": 14980.350017547607, \"max\": 14980.350017547607}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:46:00 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=41.32055222237327 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:46:00 INFO 139946820503360] #progress_metric: host=algo-1, completed 27.75 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:46:00 INFO 139946820503360] #quality_metric: host=algo-1, epoch=110, train loss <loss>=-3.792623996734619\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:46:00 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:46:08 INFO 139946820503360] Epoch[111] Batch[0] avg_epoch_loss=-2.898588\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:46:08 INFO 139946820503360] #quality_metric: host=algo-1, epoch=111, batch=0 train loss <loss>=-2.898587703704834\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:46:12 INFO 139946820503360] Epoch[111] Batch[5] avg_epoch_loss=-3.445337\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:46:12 INFO 139946820503360] #quality_metric: host=algo-1, epoch=111, batch=5 train loss <loss>=-3.4453367392222085\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:46:12 INFO 139946820503360] Epoch[111] Batch [5]#011Speed: 71.43 samples/sec#011loss=-3.445337\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:46:16 INFO 139946820503360] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197960.5598376, \"EndTime\": 1666197976.1603203, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15599.955320358276, \"count\": 1, \"min\": 15599.955320358276, \"max\": 15599.955320358276}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:46:16 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.640864397189596 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:46:16 INFO 139946820503360] #progress_metric: host=algo-1, completed 28.0 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:46:16 INFO 139946820503360] #quality_metric: host=algo-1, epoch=111, train loss <loss>=-3.405660104751587\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:46:16 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:46:23 INFO 139946820503360] Epoch[112] Batch[0] avg_epoch_loss=-3.888652\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:46:23 INFO 139946820503360] #quality_metric: host=algo-1, epoch=112, batch=0 train loss <loss>=-3.8886518478393555\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:46:27 INFO 139946820503360] Epoch[112] Batch[5] avg_epoch_loss=-3.946443\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:46:27 INFO 139946820503360] #quality_metric: host=algo-1, epoch=112, batch=5 train loss <loss>=-3.94644304116567\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:46:27 INFO 139946820503360] Epoch[112] Batch [5]#011Speed: 77.13 samples/sec#011loss=-3.946443\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:46:31 INFO 139946820503360] processed a total of 582 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197976.1603951, \"EndTime\": 1666197991.035487, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 14874.784231185913, \"count\": 1, \"min\": 14874.784231185913, \"max\": 14874.784231185913}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:46:31 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=39.126357083178185 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:46:31 INFO 139946820503360] #progress_metric: host=algo-1, completed 28.25 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:46:31 INFO 139946820503360] #quality_metric: host=algo-1, epoch=112, train loss <loss>=-3.6771995782852174\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:46:31 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:46:38 INFO 139946820503360] Epoch[113] Batch[0] avg_epoch_loss=-3.367335\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:46:38 INFO 139946820503360] #quality_metric: host=algo-1, epoch=113, batch=0 train loss <loss>=-3.367335319519043\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:46:42 INFO 139946820503360] Epoch[113] Batch[5] avg_epoch_loss=-3.741044\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:46:42 INFO 139946820503360] #quality_metric: host=algo-1, epoch=113, batch=5 train loss <loss>=-3.7410438458124795\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:46:42 INFO 139946820503360] Epoch[113] Batch [5]#011Speed: 73.15 samples/sec#011loss=-3.741044\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:46:47 INFO 139946820503360] Epoch[113] Batch[10] avg_epoch_loss=-3.874668\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:46:47 INFO 139946820503360] #quality_metric: host=algo-1, epoch=113, batch=10 train loss <loss>=-4.035016489028931\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:46:47 INFO 139946820503360] Epoch[113] Batch [10]#011Speed: 69.13 samples/sec#011loss=-4.035016\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:46:47 INFO 139946820503360] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666197991.0355554, \"EndTime\": 1666198007.4397633, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16403.879404067993, \"count\": 1, \"min\": 16403.879404067993, \"max\": 16403.879404067993}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:46:47 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=39.38066852625741 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:46:47 INFO 139946820503360] #progress_metric: host=algo-1, completed 28.5 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:46:47 INFO 139946820503360] #quality_metric: host=algo-1, epoch=113, train loss <loss>=-3.87466777454723\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:46:47 INFO 139946820503360] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:46:47 INFO 139946820503360] Saved checkpoint to \"/opt/ml/model/state_3a734e2c-6d4c-49e3-8ee3-381561fa4499-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198007.4398406, \"EndTime\": 1666198007.616925, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 176.63216590881348, \"count\": 1, \"min\": 176.63216590881348, \"max\": 176.63216590881348}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:46:54 INFO 139946820503360] Epoch[114] Batch[0] avg_epoch_loss=-3.780515\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:46:54 INFO 139946820503360] #quality_metric: host=algo-1, epoch=114, batch=0 train loss <loss>=-3.780515193939209\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:46:59 INFO 139946820503360] Epoch[114] Batch[5] avg_epoch_loss=-3.549885\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:46:59 INFO 139946820503360] #quality_metric: host=algo-1, epoch=114, batch=5 train loss <loss>=-3.5498849550882974\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:46:59 INFO 139946820503360] Epoch[114] Batch [5]#011Speed: 71.58 samples/sec#011loss=-3.549885\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:47:02 INFO 139946820503360] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198007.6169772, \"EndTime\": 1666198022.8924026, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15275.368213653564, \"count\": 1, \"min\": 15275.368213653564, \"max\": 15275.368213653564}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:47:02 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=41.89725023336214 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:47:02 INFO 139946820503360] #progress_metric: host=algo-1, completed 28.75 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:47:02 INFO 139946820503360] #quality_metric: host=algo-1, epoch=114, train loss <loss>=-3.6167417764663696\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:47:02 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:47:10 INFO 139946820503360] Epoch[115] Batch[0] avg_epoch_loss=-3.779197\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:47:10 INFO 139946820503360] #quality_metric: host=algo-1, epoch=115, batch=0 train loss <loss>=-3.7791965007781982\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:47:14 INFO 139946820503360] Epoch[115] Batch[5] avg_epoch_loss=-3.679694\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:47:14 INFO 139946820503360] #quality_metric: host=algo-1, epoch=115, batch=5 train loss <loss>=-3.679694175720215\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:47:14 INFO 139946820503360] Epoch[115] Batch [5]#011Speed: 74.18 samples/sec#011loss=-3.679694\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:47:18 INFO 139946820503360] processed a total of 615 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198022.8924708, \"EndTime\": 1666198038.0862086, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15193.413496017456, \"count\": 1, \"min\": 15193.413496017456, \"max\": 15193.413496017456}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:47:18 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.47781992528879 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:47:18 INFO 139946820503360] #progress_metric: host=algo-1, completed 29.0 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:47:18 INFO 139946820503360] #quality_metric: host=algo-1, epoch=115, train loss <loss>=-3.4651214838027955\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:47:18 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:47:25 INFO 139946820503360] Epoch[116] Batch[0] avg_epoch_loss=-3.809688\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:47:25 INFO 139946820503360] #quality_metric: host=algo-1, epoch=116, batch=0 train loss <loss>=-3.809687852859497\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:47:29 INFO 139946820503360] Epoch[116] Batch[5] avg_epoch_loss=-3.671311\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:47:29 INFO 139946820503360] #quality_metric: host=algo-1, epoch=116, batch=5 train loss <loss>=-3.6713105042775473\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:47:29 INFO 139946820503360] Epoch[116] Batch [5]#011Speed: 72.04 samples/sec#011loss=-3.671311\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:47:34 INFO 139946820503360] Epoch[116] Batch[10] avg_epoch_loss=-3.339968\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:47:34 INFO 139946820503360] #quality_metric: host=algo-1, epoch=116, batch=10 train loss <loss>=-2.9423564195632936\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:47:34 INFO 139946820503360] Epoch[116] Batch [10]#011Speed: 72.37 samples/sec#011loss=-2.942356\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:47:34 INFO 139946820503360] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198038.0862715, \"EndTime\": 1666198054.161842, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16075.20842552185, \"count\": 1, \"min\": 16075.20842552185, \"max\": 16075.20842552185}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:47:34 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.12364304809969 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:47:34 INFO 139946820503360] #progress_metric: host=algo-1, completed 29.25 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:47:34 INFO 139946820503360] #quality_metric: host=algo-1, epoch=116, train loss <loss>=-3.339967738498341\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:47:34 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:47:41 INFO 139946820503360] Epoch[117] Batch[0] avg_epoch_loss=-3.877352\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:47:41 INFO 139946820503360] #quality_metric: host=algo-1, epoch=117, batch=0 train loss <loss>=-3.877351999282837\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:47:45 INFO 139946820503360] Epoch[117] Batch[5] avg_epoch_loss=-3.467989\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:47:45 INFO 139946820503360] #quality_metric: host=algo-1, epoch=117, batch=5 train loss <loss>=-3.4679885307947793\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:47:45 INFO 139946820503360] Epoch[117] Batch [5]#011Speed: 71.72 samples/sec#011loss=-3.467989\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:47:49 INFO 139946820503360] processed a total of 587 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198054.1619146, \"EndTime\": 1666198069.5010028, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15338.813304901123, \"count\": 1, \"min\": 15338.813304901123, \"max\": 15338.813304901123}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:47:49 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=38.268690282704945 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:47:49 INFO 139946820503360] #progress_metric: host=algo-1, completed 29.5 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:47:49 INFO 139946820503360] #quality_metric: host=algo-1, epoch=117, train loss <loss>=-3.610749530792236\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:47:49 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:47:56 INFO 139946820503360] Epoch[118] Batch[0] avg_epoch_loss=-4.034749\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:47:56 INFO 139946820503360] #quality_metric: host=algo-1, epoch=118, batch=0 train loss <loss>=-4.034748554229736\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:48:01 INFO 139946820503360] Epoch[118] Batch[5] avg_epoch_loss=-3.631433\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:48:01 INFO 139946820503360] #quality_metric: host=algo-1, epoch=118, batch=5 train loss <loss>=-3.6314327716827393\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:48:01 INFO 139946820503360] Epoch[118] Batch [5]#011Speed: 75.01 samples/sec#011loss=-3.631433\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:48:04 INFO 139946820503360] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198069.5010688, \"EndTime\": 1666198084.6365411, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15135.154008865356, \"count\": 1, \"min\": 15135.154008865356, \"max\": 15135.154008865356}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:48:04 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.96398683963398 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:48:04 INFO 139946820503360] #progress_metric: host=algo-1, completed 29.75 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:48:04 INFO 139946820503360] #quality_metric: host=algo-1, epoch=118, train loss <loss>=-3.4404128551483155\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:48:04 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:48:11 INFO 139946820503360] Epoch[119] Batch[0] avg_epoch_loss=-4.252273\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:48:11 INFO 139946820503360] #quality_metric: host=algo-1, epoch=119, batch=0 train loss <loss>=-4.252272605895996\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:48:15 INFO 139946820503360] Epoch[119] Batch[5] avg_epoch_loss=-3.697944\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:48:15 INFO 139946820503360] #quality_metric: host=algo-1, epoch=119, batch=5 train loss <loss>=-3.697943647702535\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:48:15 INFO 139946820503360] Epoch[119] Batch [5]#011Speed: 76.47 samples/sec#011loss=-3.697944\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:48:20 INFO 139946820503360] Epoch[119] Batch[10] avg_epoch_loss=-3.912389\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:48:20 INFO 139946820503360] #quality_metric: host=algo-1, epoch=119, batch=10 train loss <loss>=-4.169724321365356\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:48:20 INFO 139946820503360] Epoch[119] Batch [10]#011Speed: 70.02 samples/sec#011loss=-4.169724\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:48:20 INFO 139946820503360] processed a total of 663 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198084.6366026, \"EndTime\": 1666198100.4597359, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15822.694540023804, \"count\": 1, \"min\": 15822.694540023804, \"max\": 15822.694540023804}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:48:20 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=41.90157829347865 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:48:20 INFO 139946820503360] #progress_metric: host=algo-1, completed 30.0 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:48:20 INFO 139946820503360] #quality_metric: host=algo-1, epoch=119, train loss <loss>=-3.912389408458363\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:48:20 INFO 139946820503360] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:48:20 INFO 139946820503360] Saved checkpoint to \"/opt/ml/model/state_64e7cc89-85ad-4063-aa18-135dc5f63262-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198100.4598029, \"EndTime\": 1666198100.630854, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 170.6397533416748, \"count\": 1, \"min\": 170.6397533416748, \"max\": 170.6397533416748}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:48:27 INFO 139946820503360] Epoch[120] Batch[0] avg_epoch_loss=-3.733744\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:48:27 INFO 139946820503360] #quality_metric: host=algo-1, epoch=120, batch=0 train loss <loss>=-3.7337443828582764\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:48:32 INFO 139946820503360] Epoch[120] Batch[5] avg_epoch_loss=-3.530230\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:48:32 INFO 139946820503360] #quality_metric: host=algo-1, epoch=120, batch=5 train loss <loss>=-3.530229926109314\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:48:32 INFO 139946820503360] Epoch[120] Batch [5]#011Speed: 72.12 samples/sec#011loss=-3.530230\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:48:35 INFO 139946820503360] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198100.6309125, \"EndTime\": 1666198115.8131552, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15182.183027267456, \"count\": 1, \"min\": 15182.183027267456, \"max\": 15182.183027267456}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:48:35 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=41.16639544224886 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:48:35 INFO 139946820503360] #progress_metric: host=algo-1, completed 30.25 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:48:35 INFO 139946820503360] #quality_metric: host=algo-1, epoch=120, train loss <loss>=-3.5815814256668093\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:48:35 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:48:43 INFO 139946820503360] Epoch[121] Batch[0] avg_epoch_loss=-4.157060\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:48:43 INFO 139946820503360] #quality_metric: host=algo-1, epoch=121, batch=0 train loss <loss>=-4.157060146331787\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:48:47 INFO 139946820503360] Epoch[121] Batch[5] avg_epoch_loss=-3.819441\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:48:47 INFO 139946820503360] #quality_metric: host=algo-1, epoch=121, batch=5 train loss <loss>=-3.819440722465515\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:48:47 INFO 139946820503360] Epoch[121] Batch [5]#011Speed: 72.41 samples/sec#011loss=-3.819441\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:48:51 INFO 139946820503360] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198115.8132272, \"EndTime\": 1666198131.1275465, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15313.933610916138, \"count\": 1, \"min\": 15313.933610916138, \"max\": 15313.933610916138}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:48:51 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.61634623014744 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:48:51 INFO 139946820503360] #progress_metric: host=algo-1, completed 30.5 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:48:51 INFO 139946820503360] #quality_metric: host=algo-1, epoch=121, train loss <loss>=-3.7129985809326174\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:48:51 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:48:58 INFO 139946820503360] Epoch[122] Batch[0] avg_epoch_loss=-3.248500\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:48:58 INFO 139946820503360] #quality_metric: host=algo-1, epoch=122, batch=0 train loss <loss>=-3.248499870300293\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:49:02 INFO 139946820503360] Epoch[122] Batch[5] avg_epoch_loss=-3.615353\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:49:02 INFO 139946820503360] #quality_metric: host=algo-1, epoch=122, batch=5 train loss <loss>=-3.6153528292973838\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:49:02 INFO 139946820503360] Epoch[122] Batch [5]#011Speed: 69.63 samples/sec#011loss=-3.615353\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:49:07 INFO 139946820503360] Epoch[122] Batch[10] avg_epoch_loss=-3.782114\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:49:07 INFO 139946820503360] #quality_metric: host=algo-1, epoch=122, batch=10 train loss <loss>=-3.982228326797485\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:49:07 INFO 139946820503360] Epoch[122] Batch [10]#011Speed: 62.68 samples/sec#011loss=-3.982228\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:49:07 INFO 139946820503360] processed a total of 676 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198131.1276147, \"EndTime\": 1666198147.7851942, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16657.25326538086, \"count\": 1, \"min\": 16657.25326538086, \"max\": 16657.25326538086}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:49:07 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.58271867635404 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:49:07 INFO 139946820503360] #progress_metric: host=algo-1, completed 30.75 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:49:07 INFO 139946820503360] #quality_metric: host=algo-1, epoch=122, train loss <loss>=-3.782114419070157\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:49:07 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:49:14 INFO 139946820503360] Epoch[123] Batch[0] avg_epoch_loss=-3.183629\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:49:14 INFO 139946820503360] #quality_metric: host=algo-1, epoch=123, batch=0 train loss <loss>=-3.183628797531128\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:49:19 INFO 139946820503360] Epoch[123] Batch[5] avg_epoch_loss=-3.278247\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:49:19 INFO 139946820503360] #quality_metric: host=algo-1, epoch=123, batch=5 train loss <loss>=-3.2782465616861978\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:49:19 INFO 139946820503360] Epoch[123] Batch [5]#011Speed: 70.65 samples/sec#011loss=-3.278247\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:49:24 INFO 139946820503360] Epoch[123] Batch[10] avg_epoch_loss=-3.382610\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:49:24 INFO 139946820503360] #quality_metric: host=algo-1, epoch=123, batch=10 train loss <loss>=-3.507846212387085\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:49:24 INFO 139946820503360] Epoch[123] Batch [10]#011Speed: 68.02 samples/sec#011loss=-3.507846\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:49:24 INFO 139946820503360] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198147.7852519, \"EndTime\": 1666198164.1726217, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16387.032985687256, \"count\": 1, \"min\": 16387.032985687256, \"max\": 16387.032985687256}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:49:24 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.70268161431598 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:49:24 INFO 139946820503360] #progress_metric: host=algo-1, completed 31.0 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:49:24 INFO 139946820503360] #quality_metric: host=algo-1, epoch=123, train loss <loss>=-3.3826100392775102\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:49:24 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:49:31 INFO 139946820503360] Epoch[124] Batch[0] avg_epoch_loss=-3.521375\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:49:31 INFO 139946820503360] #quality_metric: host=algo-1, epoch=124, batch=0 train loss <loss>=-3.5213754177093506\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:49:35 INFO 139946820503360] Epoch[124] Batch[5] avg_epoch_loss=-3.400796\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:49:35 INFO 139946820503360] #quality_metric: host=algo-1, epoch=124, batch=5 train loss <loss>=-3.4007964531580606\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:49:35 INFO 139946820503360] Epoch[124] Batch [5]#011Speed: 73.99 samples/sec#011loss=-3.400796\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:49:39 INFO 139946820503360] processed a total of 609 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198164.172686, \"EndTime\": 1666198179.4113965, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15238.385677337646, \"count\": 1, \"min\": 15238.385677337646, \"max\": 15238.385677337646}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:49:39 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=39.96461315342472 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:49:39 INFO 139946820503360] #progress_metric: host=algo-1, completed 31.25 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:49:39 INFO 139946820503360] #quality_metric: host=algo-1, epoch=124, train loss <loss>=-3.30174720287323\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:49:39 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:49:46 INFO 139946820503360] Epoch[125] Batch[0] avg_epoch_loss=-3.346840\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:49:46 INFO 139946820503360] #quality_metric: host=algo-1, epoch=125, batch=0 train loss <loss>=-3.346839666366577\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:49:50 INFO 139946820503360] Epoch[125] Batch[5] avg_epoch_loss=-3.486065\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:49:50 INFO 139946820503360] #quality_metric: host=algo-1, epoch=125, batch=5 train loss <loss>=-3.4860649903615317\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:49:50 INFO 139946820503360] Epoch[125] Batch [5]#011Speed: 73.81 samples/sec#011loss=-3.486065\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:49:55 INFO 139946820503360] Epoch[125] Batch[10] avg_epoch_loss=-3.151259\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:49:55 INFO 139946820503360] #quality_metric: host=algo-1, epoch=125, batch=10 train loss <loss>=-2.749491250514984\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:49:55 INFO 139946820503360] Epoch[125] Batch [10]#011Speed: 67.92 samples/sec#011loss=-2.749491\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:49:55 INFO 139946820503360] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198179.4114609, \"EndTime\": 1666198195.5560248, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16144.17028427124, \"count\": 1, \"min\": 16144.17028427124, \"max\": 16144.17028427124}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:49:55 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.385832300349 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:49:55 INFO 139946820503360] #progress_metric: host=algo-1, completed 31.5 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:49:55 INFO 139946820503360] #quality_metric: host=algo-1, epoch=125, train loss <loss>=-3.151258744976737\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:49:55 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:50:02 INFO 139946820503360] Epoch[126] Batch[0] avg_epoch_loss=-3.174981\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:50:02 INFO 139946820503360] #quality_metric: host=algo-1, epoch=126, batch=0 train loss <loss>=-3.174981117248535\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:50:07 INFO 139946820503360] Epoch[126] Batch[5] avg_epoch_loss=-3.632607\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:50:07 INFO 139946820503360] #quality_metric: host=algo-1, epoch=126, batch=5 train loss <loss>=-3.6326066652933755\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:50:07 INFO 139946820503360] Epoch[126] Batch [5]#011Speed: 70.93 samples/sec#011loss=-3.632607\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:50:10 INFO 139946820503360] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198195.556098, \"EndTime\": 1666198210.8483446, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15291.868448257446, \"count\": 1, \"min\": 15291.868448257446, \"max\": 15291.868448257446}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:50:10 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.54416747736496 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:50:10 INFO 139946820503360] #progress_metric: host=algo-1, completed 31.75 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:50:10 INFO 139946820503360] #quality_metric: host=algo-1, epoch=126, train loss <loss>=-3.738821578025818\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:50:10 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:50:17 INFO 139946820503360] Epoch[127] Batch[0] avg_epoch_loss=-2.972049\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:50:17 INFO 139946820503360] #quality_metric: host=algo-1, epoch=127, batch=0 train loss <loss>=-2.9720489978790283\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:50:22 INFO 139946820503360] Epoch[127] Batch[5] avg_epoch_loss=-3.394716\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:50:22 INFO 139946820503360] #quality_metric: host=algo-1, epoch=127, batch=5 train loss <loss>=-3.3947156270345054\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:50:22 INFO 139946820503360] Epoch[127] Batch [5]#011Speed: 70.94 samples/sec#011loss=-3.394716\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:50:27 INFO 139946820503360] Epoch[127] Batch[10] avg_epoch_loss=-3.631018\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:50:27 INFO 139946820503360] #quality_metric: host=algo-1, epoch=127, batch=10 train loss <loss>=-3.914580249786377\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:50:27 INFO 139946820503360] Epoch[127] Batch [10]#011Speed: 68.30 samples/sec#011loss=-3.914580\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:50:27 INFO 139946820503360] processed a total of 668 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198210.8484113, \"EndTime\": 1666198227.0702617, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16221.457719802856, \"count\": 1, \"min\": 16221.457719802856, \"max\": 16221.457719802856}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:50:27 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=41.1796621495113 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:50:27 INFO 139946820503360] #progress_metric: host=algo-1, completed 32.0 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:50:27 INFO 139946820503360] #quality_metric: host=algo-1, epoch=127, train loss <loss>=-3.631017728285356\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:50:27 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:50:34 INFO 139946820503360] Epoch[128] Batch[0] avg_epoch_loss=-3.765869\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:50:34 INFO 139946820503360] #quality_metric: host=algo-1, epoch=128, batch=0 train loss <loss>=-3.765868902206421\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:50:38 INFO 139946820503360] Epoch[128] Batch[5] avg_epoch_loss=-3.560180\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:50:38 INFO 139946820503360] #quality_metric: host=algo-1, epoch=128, batch=5 train loss <loss>=-3.5601797898610434\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:50:38 INFO 139946820503360] Epoch[128] Batch [5]#011Speed: 70.09 samples/sec#011loss=-3.560180\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:50:43 INFO 139946820503360] Epoch[128] Batch[10] avg_epoch_loss=-3.845807\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:50:43 INFO 139946820503360] #quality_metric: host=algo-1, epoch=128, batch=10 train loss <loss>=-4.188558673858642\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:50:43 INFO 139946820503360] Epoch[128] Batch [10]#011Speed: 64.96 samples/sec#011loss=-4.188559\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:50:43 INFO 139946820503360] processed a total of 687 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198227.0703747, \"EndTime\": 1666198243.5472176, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16476.55463218689, \"count\": 1, \"min\": 16476.55463218689, \"max\": 16476.55463218689}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:50:43 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=41.69538519187316 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:50:43 INFO 139946820503360] #progress_metric: host=algo-1, completed 32.25 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:50:43 INFO 139946820503360] #quality_metric: host=algo-1, epoch=128, train loss <loss>=-3.8458065553144976\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:50:43 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:50:50 INFO 139946820503360] Epoch[129] Batch[0] avg_epoch_loss=-3.691445\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:50:50 INFO 139946820503360] #quality_metric: host=algo-1, epoch=129, batch=0 train loss <loss>=-3.6914453506469727\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:50:55 INFO 139946820503360] Epoch[129] Batch[5] avg_epoch_loss=-3.982910\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:50:55 INFO 139946820503360] #quality_metric: host=algo-1, epoch=129, batch=5 train loss <loss>=-3.9829099575678506\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:50:55 INFO 139946820503360] Epoch[129] Batch [5]#011Speed: 75.74 samples/sec#011loss=-3.982910\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:50:58 INFO 139946820503360] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198243.5472798, \"EndTime\": 1666198258.7833178, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15235.777854919434, \"count\": 1, \"min\": 15235.777854919434, \"max\": 15235.777854919434}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:50:58 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.82468068977116 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:50:58 INFO 139946820503360] #progress_metric: host=algo-1, completed 32.5 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:50:58 INFO 139946820503360] #quality_metric: host=algo-1, epoch=129, train loss <loss>=-4.083187532424927\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:50:58 INFO 139946820503360] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:50:58 INFO 139946820503360] Saved checkpoint to \"/opt/ml/model/state_b890fede-bdd0-4bd9-9431-ba2b61c36315-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198258.7833908, \"EndTime\": 1666198258.9771218, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 193.28951835632324, \"count\": 1, \"min\": 193.28951835632324, \"max\": 193.28951835632324}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:51:06 INFO 139946820503360] Epoch[130] Batch[0] avg_epoch_loss=-3.490146\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:51:06 INFO 139946820503360] #quality_metric: host=algo-1, epoch=130, batch=0 train loss <loss>=-3.4901463985443115\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:51:10 INFO 139946820503360] Epoch[130] Batch[5] avg_epoch_loss=-3.702215\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:51:10 INFO 139946820503360] #quality_metric: host=algo-1, epoch=130, batch=5 train loss <loss>=-3.7022149562835693\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:51:10 INFO 139946820503360] Epoch[130] Batch [5]#011Speed: 75.63 samples/sec#011loss=-3.702215\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:51:14 INFO 139946820503360] processed a total of 611 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198258.9771817, \"EndTime\": 1666198274.2756033, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15298.358678817749, \"count\": 1, \"min\": 15298.358678817749, \"max\": 15298.358678817749}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:51:14 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=39.93865985267397 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:51:14 INFO 139946820503360] #progress_metric: host=algo-1, completed 32.75 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:51:14 INFO 139946820503360] #quality_metric: host=algo-1, epoch=130, train loss <loss>=-3.7871087789535522\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:51:14 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:51:21 INFO 139946820503360] Epoch[131] Batch[0] avg_epoch_loss=-4.167021\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:51:21 INFO 139946820503360] #quality_metric: host=algo-1, epoch=131, batch=0 train loss <loss>=-4.16702127456665\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:51:26 INFO 139946820503360] Epoch[131] Batch[5] avg_epoch_loss=-3.994825\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:51:26 INFO 139946820503360] #quality_metric: host=algo-1, epoch=131, batch=5 train loss <loss>=-3.9948246081670127\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:51:26 INFO 139946820503360] Epoch[131] Batch [5]#011Speed: 69.82 samples/sec#011loss=-3.994825\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:51:29 INFO 139946820503360] processed a total of 587 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198274.27567, \"EndTime\": 1666198289.6483216, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15372.267961502075, \"count\": 1, \"min\": 15372.267961502075, \"max\": 15372.267961502075}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:51:29 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=38.18535101141168 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:51:29 INFO 139946820503360] #progress_metric: host=algo-1, completed 33.0 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:51:29 INFO 139946820503360] #quality_metric: host=algo-1, epoch=131, train loss <loss>=-3.595558524131775\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:51:29 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:51:36 INFO 139946820503360] Epoch[132] Batch[0] avg_epoch_loss=-4.044663\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:51:36 INFO 139946820503360] #quality_metric: host=algo-1, epoch=132, batch=0 train loss <loss>=-4.044663429260254\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:51:41 INFO 139946820503360] Epoch[132] Batch[5] avg_epoch_loss=-3.784733\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:51:41 INFO 139946820503360] #quality_metric: host=algo-1, epoch=132, batch=5 train loss <loss>=-3.784733295440674\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:51:41 INFO 139946820503360] Epoch[132] Batch [5]#011Speed: 71.17 samples/sec#011loss=-3.784733\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:51:45 INFO 139946820503360] processed a total of 605 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198289.6484075, \"EndTime\": 1666198305.118981, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15470.208644866943, \"count\": 1, \"min\": 15470.208644866943, \"max\": 15470.208644866943}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:51:45 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=39.107172705656744 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:51:45 INFO 139946820503360] #progress_metric: host=algo-1, completed 33.25 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:51:45 INFO 139946820503360] #quality_metric: host=algo-1, epoch=132, train loss <loss>=-3.5567399740219114\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:51:45 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:51:52 INFO 139946820503360] Epoch[133] Batch[0] avg_epoch_loss=-2.529098\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:51:52 INFO 139946820503360] #quality_metric: host=algo-1, epoch=133, batch=0 train loss <loss>=-2.5290980339050293\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:51:56 INFO 139946820503360] Epoch[133] Batch[5] avg_epoch_loss=-3.075357\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:51:56 INFO 139946820503360] #quality_metric: host=algo-1, epoch=133, batch=5 train loss <loss>=-3.0753571589787803\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:51:56 INFO 139946820503360] Epoch[133] Batch [5]#011Speed: 72.22 samples/sec#011loss=-3.075357\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:52:00 INFO 139946820503360] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198305.119048, \"EndTime\": 1666198320.344282, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15224.737644195557, \"count\": 1, \"min\": 15224.737644195557, \"max\": 15224.737644195557}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:52:00 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=41.57680158061707 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:52:00 INFO 139946820503360] #progress_metric: host=algo-1, completed 33.5 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:52:00 INFO 139946820503360] #quality_metric: host=algo-1, epoch=133, train loss <loss>=-2.817715787887573\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:52:00 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:52:07 INFO 139946820503360] Epoch[134] Batch[0] avg_epoch_loss=-3.829622\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:52:07 INFO 139946820503360] #quality_metric: host=algo-1, epoch=134, batch=0 train loss <loss>=-3.8296220302581787\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:52:12 INFO 139946820503360] Epoch[134] Batch[5] avg_epoch_loss=-3.597082\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:52:12 INFO 139946820503360] #quality_metric: host=algo-1, epoch=134, batch=5 train loss <loss>=-3.5970818599065146\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:52:12 INFO 139946820503360] Epoch[134] Batch [5]#011Speed: 70.09 samples/sec#011loss=-3.597082\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:52:17 INFO 139946820503360] Epoch[134] Batch[10] avg_epoch_loss=-3.676709\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:52:17 INFO 139946820503360] #quality_metric: host=algo-1, epoch=134, batch=10 train loss <loss>=-3.772262191772461\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:52:17 INFO 139946820503360] Epoch[134] Batch [10]#011Speed: 66.22 samples/sec#011loss=-3.772262\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:52:17 INFO 139946820503360] processed a total of 669 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198320.3443506, \"EndTime\": 1666198337.0338902, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16689.20660018921, \"count\": 1, \"min\": 16689.20660018921, \"max\": 16689.20660018921}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:52:17 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.08553330911145 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:52:17 INFO 139946820503360] #progress_metric: host=algo-1, completed 33.75 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:52:17 INFO 139946820503360] #quality_metric: host=algo-1, epoch=134, train loss <loss>=-3.676709283481945\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:52:17 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:52:24 INFO 139946820503360] Epoch[135] Batch[0] avg_epoch_loss=-3.900484\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:52:24 INFO 139946820503360] #quality_metric: host=algo-1, epoch=135, batch=0 train loss <loss>=-3.900484085083008\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:52:28 INFO 139946820503360] Epoch[135] Batch[5] avg_epoch_loss=-3.356205\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:52:28 INFO 139946820503360] #quality_metric: host=algo-1, epoch=135, batch=5 train loss <loss>=-3.356204628944397\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:52:28 INFO 139946820503360] Epoch[135] Batch [5]#011Speed: 69.88 samples/sec#011loss=-3.356205\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:52:33 INFO 139946820503360] Epoch[135] Batch[10] avg_epoch_loss=-3.339152\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:52:33 INFO 139946820503360] #quality_metric: host=algo-1, epoch=135, batch=10 train loss <loss>=-3.3186888456344605\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:52:33 INFO 139946820503360] Epoch[135] Batch [10]#011Speed: 71.89 samples/sec#011loss=-3.318689\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:52:33 INFO 139946820503360] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198337.0339653, \"EndTime\": 1666198353.2510452, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16216.766119003296, \"count\": 1, \"min\": 16216.766119003296, \"max\": 16216.766119003296}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:52:33 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=39.65010233109782 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:52:33 INFO 139946820503360] #progress_metric: host=algo-1, completed 34.0 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:52:33 INFO 139946820503360] #quality_metric: host=algo-1, epoch=135, train loss <loss>=-3.3391520001671533\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:52:33 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:52:40 INFO 139946820503360] Epoch[136] Batch[0] avg_epoch_loss=-2.854111\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:52:40 INFO 139946820503360] #quality_metric: host=algo-1, epoch=136, batch=0 train loss <loss>=-2.8541107177734375\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:52:44 INFO 139946820503360] Epoch[136] Batch[5] avg_epoch_loss=-3.503726\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:52:44 INFO 139946820503360] #quality_metric: host=algo-1, epoch=136, batch=5 train loss <loss>=-3.5037256479263306\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:52:44 INFO 139946820503360] Epoch[136] Batch [5]#011Speed: 73.53 samples/sec#011loss=-3.503726\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:52:48 INFO 139946820503360] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198353.2511072, \"EndTime\": 1666198368.4043448, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15152.91976928711, \"count\": 1, \"min\": 15152.91976928711, \"max\": 15152.91976928711}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:52:48 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=41.97181775246433 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:52:48 INFO 139946820503360] #progress_metric: host=algo-1, completed 34.25 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:52:48 INFO 139946820503360] #quality_metric: host=algo-1, epoch=136, train loss <loss>=-3.528045415878296\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:52:48 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:52:55 INFO 139946820503360] Epoch[137] Batch[0] avg_epoch_loss=-4.319435\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:52:55 INFO 139946820503360] #quality_metric: host=algo-1, epoch=137, batch=0 train loss <loss>=-4.319434642791748\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:52:59 INFO 139946820503360] Epoch[137] Batch[5] avg_epoch_loss=-3.843869\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:52:59 INFO 139946820503360] #quality_metric: host=algo-1, epoch=137, batch=5 train loss <loss>=-3.843869129816691\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:52:59 INFO 139946820503360] Epoch[137] Batch [5]#011Speed: 73.55 samples/sec#011loss=-3.843869\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:53:03 INFO 139946820503360] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198368.40442, \"EndTime\": 1666198383.4131877, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15008.412599563599, \"count\": 1, \"min\": 15008.412599563599, \"max\": 15008.412599563599}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:53:03 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=42.37594829022891 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:53:03 INFO 139946820503360] #progress_metric: host=algo-1, completed 34.5 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:53:03 INFO 139946820503360] #quality_metric: host=algo-1, epoch=137, train loss <loss>=-3.866503882408142\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:53:03 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:53:10 INFO 139946820503360] Epoch[138] Batch[0] avg_epoch_loss=-3.520058\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:53:10 INFO 139946820503360] #quality_metric: host=algo-1, epoch=138, batch=0 train loss <loss>=-3.5200581550598145\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:53:14 INFO 139946820503360] Epoch[138] Batch[5] avg_epoch_loss=-3.827453\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:53:14 INFO 139946820503360] #quality_metric: host=algo-1, epoch=138, batch=5 train loss <loss>=-3.8274534543355307\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:53:14 INFO 139946820503360] Epoch[138] Batch [5]#011Speed: 74.90 samples/sec#011loss=-3.827453\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:53:19 INFO 139946820503360] Epoch[138] Batch[10] avg_epoch_loss=-3.941974\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:53:19 INFO 139946820503360] #quality_metric: host=algo-1, epoch=138, batch=10 train loss <loss>=-4.079397773742675\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:53:19 INFO 139946820503360] Epoch[138] Batch [10]#011Speed: 66.59 samples/sec#011loss=-4.079398\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:53:19 INFO 139946820503360] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198383.413258, \"EndTime\": 1666198399.7312133, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16317.633152008057, \"count\": 1, \"min\": 16317.633152008057, \"max\": 16317.633152008057}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:53:19 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.26297042535207 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:53:19 INFO 139946820503360] #progress_metric: host=algo-1, completed 34.75 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:53:19 INFO 139946820503360] #quality_metric: host=algo-1, epoch=138, train loss <loss>=-3.9419735995205967\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:53:19 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:53:26 INFO 139946820503360] Epoch[139] Batch[0] avg_epoch_loss=-3.780430\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:53:26 INFO 139946820503360] #quality_metric: host=algo-1, epoch=139, batch=0 train loss <loss>=-3.7804300785064697\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:53:31 INFO 139946820503360] Epoch[139] Batch[5] avg_epoch_loss=-3.708889\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:53:31 INFO 139946820503360] #quality_metric: host=algo-1, epoch=139, batch=5 train loss <loss>=-3.70888884862264\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:53:31 INFO 139946820503360] Epoch[139] Batch [5]#011Speed: 74.42 samples/sec#011loss=-3.708889\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:53:34 INFO 139946820503360] processed a total of 609 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198399.731276, \"EndTime\": 1666198414.7756488, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15044.105768203735, \"count\": 1, \"min\": 15044.105768203735, \"max\": 15044.105768203735}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:53:34 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.48071248228307 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:53:34 INFO 139946820503360] #progress_metric: host=algo-1, completed 35.0 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:53:34 INFO 139946820503360] #quality_metric: host=algo-1, epoch=139, train loss <loss>=-3.8541306257247925\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:53:34 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:53:42 INFO 139946820503360] Epoch[140] Batch[0] avg_epoch_loss=-4.014859\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:53:42 INFO 139946820503360] #quality_metric: host=algo-1, epoch=140, batch=0 train loss <loss>=-4.014858722686768\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:53:46 INFO 139946820503360] Epoch[140] Batch[5] avg_epoch_loss=-3.971615\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:53:46 INFO 139946820503360] #quality_metric: host=algo-1, epoch=140, batch=5 train loss <loss>=-3.9716149965922036\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:53:46 INFO 139946820503360] Epoch[140] Batch [5]#011Speed: 74.94 samples/sec#011loss=-3.971615\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:53:50 INFO 139946820503360] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198414.7757156, \"EndTime\": 1666198430.045817, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15269.786596298218, \"count\": 1, \"min\": 15269.786596298218, \"max\": 15269.786596298218}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:53:50 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=41.32317856890306 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:53:50 INFO 139946820503360] #progress_metric: host=algo-1, completed 35.25 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:53:50 INFO 139946820503360] #quality_metric: host=algo-1, epoch=140, train loss <loss>=-3.807712268829346\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:53:50 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:53:57 INFO 139946820503360] Epoch[141] Batch[0] avg_epoch_loss=-3.754864\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:53:57 INFO 139946820503360] #quality_metric: host=algo-1, epoch=141, batch=0 train loss <loss>=-3.754863977432251\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:54:01 INFO 139946820503360] Epoch[141] Batch[5] avg_epoch_loss=-3.883804\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:54:01 INFO 139946820503360] #quality_metric: host=algo-1, epoch=141, batch=5 train loss <loss>=-3.883803606033325\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:54:01 INFO 139946820503360] Epoch[141] Batch [5]#011Speed: 72.94 samples/sec#011loss=-3.883804\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:54:05 INFO 139946820503360] processed a total of 609 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198430.045881, \"EndTime\": 1666198445.3240385, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15277.768850326538, \"count\": 1, \"min\": 15277.768850326538, \"max\": 15277.768850326538}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:54:05 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=39.86156714489353 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:54:05 INFO 139946820503360] #progress_metric: host=algo-1, completed 35.5 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:54:05 INFO 139946820503360] #quality_metric: host=algo-1, epoch=141, train loss <loss>=-3.5062943935394286\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:54:05 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:54:12 INFO 139946820503360] Epoch[142] Batch[0] avg_epoch_loss=-4.162045\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:54:12 INFO 139946820503360] #quality_metric: host=algo-1, epoch=142, batch=0 train loss <loss>=-4.162044525146484\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:54:17 INFO 139946820503360] Epoch[142] Batch[5] avg_epoch_loss=-3.993316\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:54:17 INFO 139946820503360] #quality_metric: host=algo-1, epoch=142, batch=5 train loss <loss>=-3.9933157364527383\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:54:17 INFO 139946820503360] Epoch[142] Batch [5]#011Speed: 70.04 samples/sec#011loss=-3.993316\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:54:20 INFO 139946820503360] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198445.324112, \"EndTime\": 1666198460.8466003, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15522.060632705688, \"count\": 1, \"min\": 15522.060632705688, \"max\": 15522.060632705688}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:54:20 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.45828970165927 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:54:20 INFO 139946820503360] #progress_metric: host=algo-1, completed 35.75 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:54:20 INFO 139946820503360] #quality_metric: host=algo-1, epoch=142, train loss <loss>=-3.8599563360214235\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:54:20 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:54:28 INFO 139946820503360] Epoch[143] Batch[0] avg_epoch_loss=-3.723234\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:54:28 INFO 139946820503360] #quality_metric: host=algo-1, epoch=143, batch=0 train loss <loss>=-3.723234176635742\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:54:32 INFO 139946820503360] Epoch[143] Batch[5] avg_epoch_loss=-3.531155\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:54:32 INFO 139946820503360] #quality_metric: host=algo-1, epoch=143, batch=5 train loss <loss>=-3.5311547915140786\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:54:32 INFO 139946820503360] Epoch[143] Batch [5]#011Speed: 71.22 samples/sec#011loss=-3.531155\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:54:36 INFO 139946820503360] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198460.8466692, \"EndTime\": 1666198476.3116639, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15464.37692642212, \"count\": 1, \"min\": 15464.37692642212, \"max\": 15464.37692642212}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:54:36 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.9971181487774 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:54:36 INFO 139946820503360] #progress_metric: host=algo-1, completed 36.0 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:54:36 INFO 139946820503360] #quality_metric: host=algo-1, epoch=143, train loss <loss>=-3.534465765953064\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:54:36 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:54:43 INFO 139946820503360] Epoch[144] Batch[0] avg_epoch_loss=-3.765070\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:54:43 INFO 139946820503360] #quality_metric: host=algo-1, epoch=144, batch=0 train loss <loss>=-3.7650701999664307\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:54:47 INFO 139946820503360] Epoch[144] Batch[5] avg_epoch_loss=-3.703742\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:54:47 INFO 139946820503360] #quality_metric: host=algo-1, epoch=144, batch=5 train loss <loss>=-3.7037416299184165\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:54:47 INFO 139946820503360] Epoch[144] Batch [5]#011Speed: 75.85 samples/sec#011loss=-3.703742\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:54:51 INFO 139946820503360] processed a total of 599 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198476.3117526, \"EndTime\": 1666198491.1930892, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 14880.924701690674, \"count\": 1, \"min\": 14880.924701690674, \"max\": 14880.924701690674}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:54:51 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.25262335427129 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:54:51 INFO 139946820503360] #progress_metric: host=algo-1, completed 36.25 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:54:51 INFO 139946820503360] #quality_metric: host=algo-1, epoch=144, train loss <loss>=-3.7521024465560915\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:54:51 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:54:58 INFO 139946820503360] Epoch[145] Batch[0] avg_epoch_loss=-3.854618\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:54:58 INFO 139946820503360] #quality_metric: host=algo-1, epoch=145, batch=0 train loss <loss>=-3.8546178340911865\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:55:03 INFO 139946820503360] Epoch[145] Batch[5] avg_epoch_loss=-3.614674\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:55:03 INFO 139946820503360] #quality_metric: host=algo-1, epoch=145, batch=5 train loss <loss>=-3.614673614501953\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:55:03 INFO 139946820503360] Epoch[145] Batch [5]#011Speed: 70.60 samples/sec#011loss=-3.614674\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:55:06 INFO 139946820503360] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198491.1931505, \"EndTime\": 1666198506.9005697, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15707.01789855957, \"count\": 1, \"min\": 15707.01789855957, \"max\": 15707.01789855957}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:55:06 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=39.981890091742 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:55:06 INFO 139946820503360] #progress_metric: host=algo-1, completed 36.5 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:55:06 INFO 139946820503360] #quality_metric: host=algo-1, epoch=145, train loss <loss>=-3.6633668422698973\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:55:06 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:55:14 INFO 139946820503360] Epoch[146] Batch[0] avg_epoch_loss=-4.105829\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:55:14 INFO 139946820503360] #quality_metric: host=algo-1, epoch=146, batch=0 train loss <loss>=-4.105829238891602\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:55:18 INFO 139946820503360] Epoch[146] Batch[5] avg_epoch_loss=-3.775481\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:55:18 INFO 139946820503360] #quality_metric: host=algo-1, epoch=146, batch=5 train loss <loss>=-3.7754811445871987\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:55:18 INFO 139946820503360] Epoch[146] Batch [5]#011Speed: 75.09 samples/sec#011loss=-3.775481\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:55:22 INFO 139946820503360] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198506.9006348, \"EndTime\": 1666198522.022487, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15121.54483795166, \"count\": 1, \"min\": 15121.54483795166, \"max\": 15121.54483795166}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:55:22 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=41.92666016647219 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:55:22 INFO 139946820503360] #progress_metric: host=algo-1, completed 36.75 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:55:22 INFO 139946820503360] #quality_metric: host=algo-1, epoch=146, train loss <loss>=-3.843425464630127\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:55:22 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:55:29 INFO 139946820503360] Epoch[147] Batch[0] avg_epoch_loss=-3.910742\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:55:29 INFO 139946820503360] #quality_metric: host=algo-1, epoch=147, batch=0 train loss <loss>=-3.9107420444488525\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:55:34 INFO 139946820503360] Epoch[147] Batch[5] avg_epoch_loss=-3.756107\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:55:34 INFO 139946820503360] #quality_metric: host=algo-1, epoch=147, batch=5 train loss <loss>=-3.756107211112976\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:55:34 INFO 139946820503360] Epoch[147] Batch [5]#011Speed: 70.13 samples/sec#011loss=-3.756107\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:55:37 INFO 139946820503360] processed a total of 592 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198522.022556, \"EndTime\": 1666198537.6073003, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15584.362745285034, \"count\": 1, \"min\": 15584.362745285034, \"max\": 15584.362745285034}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:55:37 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=37.98654150311067 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:55:37 INFO 139946820503360] #progress_metric: host=algo-1, completed 37.0 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:55:37 INFO 139946820503360] #quality_metric: host=algo-1, epoch=147, train loss <loss>=-3.5196917176246645\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:55:37 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:55:44 INFO 139946820503360] Epoch[148] Batch[0] avg_epoch_loss=-3.163823\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:55:44 INFO 139946820503360] #quality_metric: host=algo-1, epoch=148, batch=0 train loss <loss>=-3.163822650909424\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:55:49 INFO 139946820503360] Epoch[148] Batch[5] avg_epoch_loss=-3.242611\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:55:49 INFO 139946820503360] #quality_metric: host=algo-1, epoch=148, batch=5 train loss <loss>=-3.2426111698150635\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:55:49 INFO 139946820503360] Epoch[148] Batch [5]#011Speed: 73.55 samples/sec#011loss=-3.242611\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:55:52 INFO 139946820503360] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198537.6073737, \"EndTime\": 1666198552.6224134, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15014.6324634552, \"count\": 1, \"min\": 15014.6324634552, \"max\": 15014.6324634552}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:55:52 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=42.22521097133514 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:55:52 INFO 139946820503360] #progress_metric: host=algo-1, completed 37.25 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:55:52 INFO 139946820503360] #quality_metric: host=algo-1, epoch=148, train loss <loss>=-3.246085596084595\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:55:52 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:55:59 INFO 139946820503360] Epoch[149] Batch[0] avg_epoch_loss=-3.149051\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:55:59 INFO 139946820503360] #quality_metric: host=algo-1, epoch=149, batch=0 train loss <loss>=-3.1490511894226074\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:56:04 INFO 139946820503360] Epoch[149] Batch[5] avg_epoch_loss=-3.659376\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:56:04 INFO 139946820503360] #quality_metric: host=algo-1, epoch=149, batch=5 train loss <loss>=-3.6593756675720215\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:56:04 INFO 139946820503360] Epoch[149] Batch [5]#011Speed: 75.12 samples/sec#011loss=-3.659376\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:56:06 INFO 139946820503360] processed a total of 576 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198552.6224792, \"EndTime\": 1666198566.924085, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 14301.278591156006, \"count\": 1, \"min\": 14301.278591156006, \"max\": 14301.278591156006}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:56:06 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.27584516339754 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:56:06 INFO 139946820503360] #progress_metric: host=algo-1, completed 37.5 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:56:06 INFO 139946820503360] #quality_metric: host=algo-1, epoch=149, train loss <loss>=-3.6827527946896024\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:56:06 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:56:14 INFO 139946820503360] Epoch[150] Batch[0] avg_epoch_loss=-3.805450\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:56:14 INFO 139946820503360] #quality_metric: host=algo-1, epoch=150, batch=0 train loss <loss>=-3.805449962615967\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:56:18 INFO 139946820503360] Epoch[150] Batch[5] avg_epoch_loss=-3.877051\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:56:18 INFO 139946820503360] #quality_metric: host=algo-1, epoch=150, batch=5 train loss <loss>=-3.877050757408142\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:56:18 INFO 139946820503360] Epoch[150] Batch [5]#011Speed: 69.97 samples/sec#011loss=-3.877051\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:56:22 INFO 139946820503360] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198566.9241526, \"EndTime\": 1666198582.2858632, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15361.381769180298, \"count\": 1, \"min\": 15361.381769180298, \"max\": 15361.381769180298}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:56:22 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.1654032293574 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:56:22 INFO 139946820503360] #progress_metric: host=algo-1, completed 37.75 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:56:22 INFO 139946820503360] #quality_metric: host=algo-1, epoch=150, train loss <loss>=-3.7866429805755617\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:56:22 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:56:29 INFO 139946820503360] Epoch[151] Batch[0] avg_epoch_loss=-3.806937\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:56:29 INFO 139946820503360] #quality_metric: host=algo-1, epoch=151, batch=0 train loss <loss>=-3.8069374561309814\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:56:34 INFO 139946820503360] Epoch[151] Batch[5] avg_epoch_loss=-3.703907\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:56:34 INFO 139946820503360] #quality_metric: host=algo-1, epoch=151, batch=5 train loss <loss>=-3.7039071718851724\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:56:34 INFO 139946820503360] Epoch[151] Batch [5]#011Speed: 71.61 samples/sec#011loss=-3.703907\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:56:37 INFO 139946820503360] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198582.2859313, \"EndTime\": 1666198597.439164, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15152.925729751587, \"count\": 1, \"min\": 15152.925729751587, \"max\": 15152.925729751587}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:56:37 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=41.44388340096122 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:56:37 INFO 139946820503360] #progress_metric: host=algo-1, completed 38.0 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:56:37 INFO 139946820503360] #quality_metric: host=algo-1, epoch=151, train loss <loss>=-3.868498682975769\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:56:37 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:56:44 INFO 139946820503360] Epoch[152] Batch[0] avg_epoch_loss=-3.956427\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:56:44 INFO 139946820503360] #quality_metric: host=algo-1, epoch=152, batch=0 train loss <loss>=-3.9564270973205566\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:56:49 INFO 139946820503360] Epoch[152] Batch[5] avg_epoch_loss=-3.956821\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:56:49 INFO 139946820503360] #quality_metric: host=algo-1, epoch=152, batch=5 train loss <loss>=-3.9568214813868203\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:56:49 INFO 139946820503360] Epoch[152] Batch [5]#011Speed: 71.95 samples/sec#011loss=-3.956821\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:56:52 INFO 139946820503360] processed a total of 584 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198597.4392297, \"EndTime\": 1666198612.6859593, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15246.406316757202, \"count\": 1, \"min\": 15246.406316757202, \"max\": 15246.406316757202}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:56:52 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=38.30387477864398 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:56:52 INFO 139946820503360] #progress_metric: host=algo-1, completed 38.25 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:56:52 INFO 139946820503360] #quality_metric: host=algo-1, epoch=152, train loss <loss>=-4.459147119522095\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:56:52 INFO 139946820503360] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:56:52 INFO 139946820503360] Saved checkpoint to \"/opt/ml/model/state_33a94efd-353b-41db-9bd6-7c3b4ecac9b2-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198612.6860228, \"EndTime\": 1666198612.8755739, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 189.15820121765137, \"count\": 1, \"min\": 189.15820121765137, \"max\": 189.15820121765137}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:56:59 INFO 139946820503360] Epoch[153] Batch[0] avg_epoch_loss=-3.668189\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:56:59 INFO 139946820503360] #quality_metric: host=algo-1, epoch=153, batch=0 train loss <loss>=-3.668189287185669\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:57:04 INFO 139946820503360] Epoch[153] Batch[5] avg_epoch_loss=-3.687899\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:57:04 INFO 139946820503360] #quality_metric: host=algo-1, epoch=153, batch=5 train loss <loss>=-3.6878985166549683\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:57:04 INFO 139946820503360] Epoch[153] Batch [5]#011Speed: 72.41 samples/sec#011loss=-3.687899\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:57:09 INFO 139946820503360] Epoch[153] Batch[10] avg_epoch_loss=-4.030155\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:57:09 INFO 139946820503360] #quality_metric: host=algo-1, epoch=153, batch=10 train loss <loss>=-4.440861701965332\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:57:09 INFO 139946820503360] Epoch[153] Batch [10]#011Speed: 64.57 samples/sec#011loss=-4.440862\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:57:09 INFO 139946820503360] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198612.875639, \"EndTime\": 1666198629.24853, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16372.833967208862, \"count\": 1, \"min\": 16372.833967208862, \"max\": 16372.833967208862}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:57:09 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=39.944019553891785 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:57:09 INFO 139946820503360] #progress_metric: host=algo-1, completed 38.5 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:57:09 INFO 139946820503360] #quality_metric: host=algo-1, epoch=153, train loss <loss>=-4.030154509977861\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:57:09 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:57:16 INFO 139946820503360] Epoch[154] Batch[0] avg_epoch_loss=-2.877929\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:57:16 INFO 139946820503360] #quality_metric: host=algo-1, epoch=154, batch=0 train loss <loss>=-2.8779289722442627\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:57:20 INFO 139946820503360] Epoch[154] Batch[5] avg_epoch_loss=-3.594600\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:57:20 INFO 139946820503360] #quality_metric: host=algo-1, epoch=154, batch=5 train loss <loss>=-3.5946003595987954\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:57:20 INFO 139946820503360] Epoch[154] Batch [5]#011Speed: 74.10 samples/sec#011loss=-3.594600\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:57:24 INFO 139946820503360] processed a total of 619 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198629.2485814, \"EndTime\": 1666198644.6268814, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15378.008842468262, \"count\": 1, \"min\": 15378.008842468262, \"max\": 15378.008842468262}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:57:24 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.25202012096997 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:57:24 INFO 139946820503360] #progress_metric: host=algo-1, completed 38.75 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:57:24 INFO 139946820503360] #quality_metric: host=algo-1, epoch=154, train loss <loss>=-3.749505138397217\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:57:24 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:57:31 INFO 139946820503360] Epoch[155] Batch[0] avg_epoch_loss=-3.667726\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:57:31 INFO 139946820503360] #quality_metric: host=algo-1, epoch=155, batch=0 train loss <loss>=-3.6677258014678955\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:57:36 INFO 139946820503360] Epoch[155] Batch[5] avg_epoch_loss=-3.618104\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:57:36 INFO 139946820503360] #quality_metric: host=algo-1, epoch=155, batch=5 train loss <loss>=-3.6181037028630576\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:57:36 INFO 139946820503360] Epoch[155] Batch [5]#011Speed: 70.19 samples/sec#011loss=-3.618104\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:57:40 INFO 139946820503360] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198644.6269517, \"EndTime\": 1666198660.065164, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15437.882900238037, \"count\": 1, \"min\": 15437.882900238037, \"max\": 15437.882900238037}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:57:40 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.74367196155918 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:57:40 INFO 139946820503360] #progress_metric: host=algo-1, completed 39.0 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:57:40 INFO 139946820503360] #quality_metric: host=algo-1, epoch=155, train loss <loss>=-3.805612874031067\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:57:40 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:57:46 INFO 139946820503360] Epoch[156] Batch[0] avg_epoch_loss=-3.636248\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:57:46 INFO 139946820503360] #quality_metric: host=algo-1, epoch=156, batch=0 train loss <loss>=-3.6362478733062744\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:57:51 INFO 139946820503360] Epoch[156] Batch[5] avg_epoch_loss=-3.321013\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:57:51 INFO 139946820503360] #quality_metric: host=algo-1, epoch=156, batch=5 train loss <loss>=-3.3210132122039795\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:57:51 INFO 139946820503360] Epoch[156] Batch [5]#011Speed: 75.69 samples/sec#011loss=-3.321013\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:57:56 INFO 139946820503360] Epoch[156] Batch[10] avg_epoch_loss=-3.869247\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:57:56 INFO 139946820503360] #quality_metric: host=algo-1, epoch=156, batch=10 train loss <loss>=-4.527127695083618\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:57:56 INFO 139946820503360] Epoch[156] Batch [10]#011Speed: 65.52 samples/sec#011loss=-4.527128\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:57:56 INFO 139946820503360] processed a total of 673 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198660.0652328, \"EndTime\": 1666198676.0299861, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15964.458465576172, \"count\": 1, \"min\": 15964.458465576172, \"max\": 15964.458465576172}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:57:56 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=42.155872031668714 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:57:56 INFO 139946820503360] #progress_metric: host=algo-1, completed 39.25 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:57:56 INFO 139946820503360] #quality_metric: host=algo-1, epoch=156, train loss <loss>=-3.869247068058361\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:57:56 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:58:03 INFO 139946820503360] Epoch[157] Batch[0] avg_epoch_loss=-3.938863\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:58:03 INFO 139946820503360] #quality_metric: host=algo-1, epoch=157, batch=0 train loss <loss>=-3.9388632774353027\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:58:07 INFO 139946820503360] Epoch[157] Batch[5] avg_epoch_loss=-3.892769\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:58:07 INFO 139946820503360] #quality_metric: host=algo-1, epoch=157, batch=5 train loss <loss>=-3.892768979072571\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:58:07 INFO 139946820503360] Epoch[157] Batch [5]#011Speed: 69.69 samples/sec#011loss=-3.892769\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:58:12 INFO 139946820503360] Epoch[157] Batch[10] avg_epoch_loss=-3.863448\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:58:12 INFO 139946820503360] #quality_metric: host=algo-1, epoch=157, batch=10 train loss <loss>=-3.828263282775879\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:58:12 INFO 139946820503360] Epoch[157] Batch [10]#011Speed: 66.96 samples/sec#011loss=-3.828263\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:58:12 INFO 139946820503360] processed a total of 666 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198676.0300603, \"EndTime\": 1666198692.4427998, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16412.39595413208, \"count\": 1, \"min\": 16412.39595413208, \"max\": 16412.39595413208}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:58:12 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.578729496777854 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:58:12 INFO 139946820503360] #progress_metric: host=algo-1, completed 39.5 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:58:12 INFO 139946820503360] #quality_metric: host=algo-1, epoch=157, train loss <loss>=-3.86344820802862\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:58:12 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:58:19 INFO 139946820503360] Epoch[158] Batch[0] avg_epoch_loss=-3.912668\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:58:19 INFO 139946820503360] #quality_metric: host=algo-1, epoch=158, batch=0 train loss <loss>=-3.912667751312256\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:58:23 INFO 139946820503360] Epoch[158] Batch[5] avg_epoch_loss=-3.941243\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:58:23 INFO 139946820503360] #quality_metric: host=algo-1, epoch=158, batch=5 train loss <loss>=-3.9412426948547363\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:58:23 INFO 139946820503360] Epoch[158] Batch [5]#011Speed: 74.72 samples/sec#011loss=-3.941243\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:58:28 INFO 139946820503360] Epoch[158] Batch[10] avg_epoch_loss=-3.645660\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:58:28 INFO 139946820503360] #quality_metric: host=algo-1, epoch=158, batch=10 train loss <loss>=-3.290961515903473\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:58:28 INFO 139946820503360] Epoch[158] Batch [10]#011Speed: 69.50 samples/sec#011loss=-3.290962\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:58:28 INFO 139946820503360] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198692.442907, \"EndTime\": 1666198708.4359167, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15992.730855941772, \"count\": 1, \"min\": 15992.730855941772, \"max\": 15992.730855941772}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:58:28 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=41.26851057166931 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:58:28 INFO 139946820503360] #progress_metric: host=algo-1, completed 39.75 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:58:28 INFO 139946820503360] #quality_metric: host=algo-1, epoch=158, train loss <loss>=-3.6456603407859802\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:58:28 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:58:35 INFO 139946820503360] Epoch[159] Batch[0] avg_epoch_loss=-3.945463\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:58:35 INFO 139946820503360] #quality_metric: host=algo-1, epoch=159, batch=0 train loss <loss>=-3.945462703704834\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:58:40 INFO 139946820503360] Epoch[159] Batch[5] avg_epoch_loss=-3.725033\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:58:40 INFO 139946820503360] #quality_metric: host=algo-1, epoch=159, batch=5 train loss <loss>=-3.7250332037607827\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:58:40 INFO 139946820503360] Epoch[159] Batch [5]#011Speed: 70.06 samples/sec#011loss=-3.725033\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:58:44 INFO 139946820503360] Epoch[159] Batch[10] avg_epoch_loss=-3.600337\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:58:44 INFO 139946820503360] #quality_metric: host=algo-1, epoch=159, batch=10 train loss <loss>=-3.4507004737854006\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:58:44 INFO 139946820503360] Epoch[159] Batch [10]#011Speed: 68.93 samples/sec#011loss=-3.450700\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:58:44 INFO 139946820503360] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198708.4359798, \"EndTime\": 1666198724.9660225, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16529.757022857666, \"count\": 1, \"min\": 16529.757022857666, \"max\": 16529.757022857666}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:58:44 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=39.50429925481891 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:58:44 INFO 139946820503360] #progress_metric: host=algo-1, completed 40.0 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:58:44 INFO 139946820503360] #quality_metric: host=algo-1, epoch=159, train loss <loss>=-3.6003365083174272\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:58:44 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:58:52 INFO 139946820503360] Epoch[160] Batch[0] avg_epoch_loss=-3.191716\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:58:52 INFO 139946820503360] #quality_metric: host=algo-1, epoch=160, batch=0 train loss <loss>=-3.191716432571411\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:58:56 INFO 139946820503360] Epoch[160] Batch[5] avg_epoch_loss=-3.800898\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:58:56 INFO 139946820503360] #quality_metric: host=algo-1, epoch=160, batch=5 train loss <loss>=-3.8008981943130493\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:58:56 INFO 139946820503360] Epoch[160] Batch [5]#011Speed: 70.93 samples/sec#011loss=-3.800898\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:59:01 INFO 139946820503360] Epoch[160] Batch[10] avg_epoch_loss=-4.012694\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:59:01 INFO 139946820503360] #quality_metric: host=algo-1, epoch=160, batch=10 train loss <loss>=-4.266849994659424\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:59:01 INFO 139946820503360] Epoch[160] Batch [10]#011Speed: 69.78 samples/sec#011loss=-4.266850\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:59:01 INFO 139946820503360] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198724.9660847, \"EndTime\": 1666198741.359879, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16393.51534843445, \"count\": 1, \"min\": 16393.51534843445, \"max\": 16393.51534843445}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:59:01 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=39.161597310893434 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:59:01 INFO 139946820503360] #progress_metric: host=algo-1, completed 40.25 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:59:01 INFO 139946820503360] #quality_metric: host=algo-1, epoch=160, train loss <loss>=-4.012694467197765\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:59:01 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:59:08 INFO 139946820503360] Epoch[161] Batch[0] avg_epoch_loss=-2.733171\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:59:08 INFO 139946820503360] #quality_metric: host=algo-1, epoch=161, batch=0 train loss <loss>=-2.733170509338379\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:59:13 INFO 139946820503360] Epoch[161] Batch[5] avg_epoch_loss=-2.167898\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:59:13 INFO 139946820503360] #quality_metric: host=algo-1, epoch=161, batch=5 train loss <loss>=-2.167898178100586\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:59:13 INFO 139946820503360] Epoch[161] Batch [5]#011Speed: 70.29 samples/sec#011loss=-2.167898\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:59:16 INFO 139946820503360] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198741.359947, \"EndTime\": 1666198756.9547904, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15594.489574432373, \"count\": 1, \"min\": 15594.489574432373, \"max\": 15594.489574432373}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:59:16 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.84747294208414 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:59:16 INFO 139946820503360] #progress_metric: host=algo-1, completed 40.5 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:59:16 INFO 139946820503360] #quality_metric: host=algo-1, epoch=161, train loss <loss>=-2.367209792137146\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:59:16 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:59:24 INFO 139946820503360] Epoch[162] Batch[0] avg_epoch_loss=-3.432530\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:59:24 INFO 139946820503360] #quality_metric: host=algo-1, epoch=162, batch=0 train loss <loss>=-3.4325296878814697\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:59:28 INFO 139946820503360] Epoch[162] Batch[5] avg_epoch_loss=-3.164204\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:59:28 INFO 139946820503360] #quality_metric: host=algo-1, epoch=162, batch=5 train loss <loss>=-3.164203683535258\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:59:28 INFO 139946820503360] Epoch[162] Batch [5]#011Speed: 71.14 samples/sec#011loss=-3.164204\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:59:32 INFO 139946820503360] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198756.9548686, \"EndTime\": 1666198772.329537, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15374.289989471436, \"count\": 1, \"min\": 15374.289989471436, \"max\": 15374.289989471436}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:59:32 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.52195645152376 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:59:32 INFO 139946820503360] #progress_metric: host=algo-1, completed 40.75 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:59:32 INFO 139946820503360] #quality_metric: host=algo-1, epoch=162, train loss <loss>=-3.2733175992965697\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:59:32 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:59:39 INFO 139946820503360] Epoch[163] Batch[0] avg_epoch_loss=-3.099004\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:59:39 INFO 139946820503360] #quality_metric: host=algo-1, epoch=163, batch=0 train loss <loss>=-3.099004030227661\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:59:43 INFO 139946820503360] Epoch[163] Batch[5] avg_epoch_loss=-3.499932\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:59:43 INFO 139946820503360] #quality_metric: host=algo-1, epoch=163, batch=5 train loss <loss>=-3.499932050704956\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:59:43 INFO 139946820503360] Epoch[163] Batch [5]#011Speed: 73.61 samples/sec#011loss=-3.499932\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:59:47 INFO 139946820503360] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198772.329598, \"EndTime\": 1666198787.5076506, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15177.67310142517, \"count\": 1, \"min\": 15177.67310142517, \"max\": 15177.67310142517}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:59:47 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.98101463460518 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:59:47 INFO 139946820503360] #progress_metric: host=algo-1, completed 41.0 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:59:47 INFO 139946820503360] #quality_metric: host=algo-1, epoch=163, train loss <loss>=-3.583154296875\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:59:47 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:59:54 INFO 139946820503360] Epoch[164] Batch[0] avg_epoch_loss=-4.090836\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:59:54 INFO 139946820503360] #quality_metric: host=algo-1, epoch=164, batch=0 train loss <loss>=-4.0908355712890625\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:59:58 INFO 139946820503360] Epoch[164] Batch[5] avg_epoch_loss=-3.463204\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:59:58 INFO 139946820503360] #quality_metric: host=algo-1, epoch=164, batch=5 train loss <loss>=-3.46320370833079\u001b[0m\n",
      "\u001b[34m[10/19/2022 16:59:58 INFO 139946820503360] Epoch[164] Batch [5]#011Speed: 75.65 samples/sec#011loss=-3.463204\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:00:03 INFO 139946820503360] Epoch[164] Batch[10] avg_epoch_loss=-3.944512\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:00:03 INFO 139946820503360] #quality_metric: host=algo-1, epoch=164, batch=10 train loss <loss>=-4.522081422805786\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:00:03 INFO 139946820503360] Epoch[164] Batch [10]#011Speed: 68.50 samples/sec#011loss=-4.522081\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:00:03 INFO 139946820503360] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198787.5077097, \"EndTime\": 1666198803.5287232, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16020.588874816895, \"count\": 1, \"min\": 16020.588874816895, \"max\": 16020.588874816895}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:00:03 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.57256069485858 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:00:03 INFO 139946820503360] #progress_metric: host=algo-1, completed 41.25 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:00:03 INFO 139946820503360] #quality_metric: host=algo-1, epoch=164, train loss <loss>=-3.9445117603648794\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:00:03 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:00:10 INFO 139946820503360] Epoch[165] Batch[0] avg_epoch_loss=-3.590365\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:00:10 INFO 139946820503360] #quality_metric: host=algo-1, epoch=165, batch=0 train loss <loss>=-3.590364933013916\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:00:15 INFO 139946820503360] Epoch[165] Batch[5] avg_epoch_loss=-3.642745\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:00:15 INFO 139946820503360] #quality_metric: host=algo-1, epoch=165, batch=5 train loss <loss>=-3.642744501431783\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:00:15 INFO 139946820503360] Epoch[165] Batch [5]#011Speed: 70.05 samples/sec#011loss=-3.642745\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:00:19 INFO 139946820503360] processed a total of 621 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198803.5287871, \"EndTime\": 1666198819.2059367, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15676.867008209229, \"count\": 1, \"min\": 15676.867008209229, \"max\": 15676.867008209229}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:00:19 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=39.612218774625866 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:00:19 INFO 139946820503360] #progress_metric: host=algo-1, completed 41.5 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:00:19 INFO 139946820503360] #quality_metric: host=algo-1, epoch=165, train loss <loss>=-3.712474250793457\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:00:19 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:00:26 INFO 139946820503360] Epoch[166] Batch[0] avg_epoch_loss=-3.746796\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:00:26 INFO 139946820503360] #quality_metric: host=algo-1, epoch=166, batch=0 train loss <loss>=-3.746795892715454\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:00:30 INFO 139946820503360] Epoch[166] Batch[5] avg_epoch_loss=-3.634678\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:00:30 INFO 139946820503360] #quality_metric: host=algo-1, epoch=166, batch=5 train loss <loss>=-3.634678363800049\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:00:30 INFO 139946820503360] Epoch[166] Batch [5]#011Speed: 69.91 samples/sec#011loss=-3.634678\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:00:35 INFO 139946820503360] Epoch[166] Batch[10] avg_epoch_loss=-4.237307\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:00:35 INFO 139946820503360] #quality_metric: host=algo-1, epoch=166, batch=10 train loss <loss>=-4.9604614734649655\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:00:35 INFO 139946820503360] Epoch[166] Batch [10]#011Speed: 67.06 samples/sec#011loss=-4.960461\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:00:35 INFO 139946820503360] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198819.2060132, \"EndTime\": 1666198835.7506871, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16544.26121711731, \"count\": 1, \"min\": 16544.26121711731, \"max\": 16544.26121711731}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:00:35 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=38.92566789390144 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:00:35 INFO 139946820503360] #progress_metric: host=algo-1, completed 41.75 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:00:35 INFO 139946820503360] #quality_metric: host=algo-1, epoch=166, train loss <loss>=-4.237307050011375\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:00:35 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:00:42 INFO 139946820503360] Epoch[167] Batch[0] avg_epoch_loss=-4.247715\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:00:42 INFO 139946820503360] #quality_metric: host=algo-1, epoch=167, batch=0 train loss <loss>=-4.247714519500732\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:00:47 INFO 139946820503360] Epoch[167] Batch[5] avg_epoch_loss=-3.876244\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:00:47 INFO 139946820503360] #quality_metric: host=algo-1, epoch=167, batch=5 train loss <loss>=-3.8762442668279014\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:00:47 INFO 139946820503360] Epoch[167] Batch [5]#011Speed: 72.40 samples/sec#011loss=-3.876244\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:00:52 INFO 139946820503360] Epoch[167] Batch[10] avg_epoch_loss=-3.950164\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:00:52 INFO 139946820503360] #quality_metric: host=algo-1, epoch=167, batch=10 train loss <loss>=-4.038868093490601\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:00:52 INFO 139946820503360] Epoch[167] Batch [10]#011Speed: 63.46 samples/sec#011loss=-4.038868\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:00:52 INFO 139946820503360] processed a total of 672 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198835.7507515, \"EndTime\": 1666198852.292493, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16541.425228118896, \"count\": 1, \"min\": 16541.425228118896, \"max\": 16541.425228118896}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:00:52 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.625042121185956 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:00:52 INFO 139946820503360] #progress_metric: host=algo-1, completed 42.0 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:00:52 INFO 139946820503360] #quality_metric: host=algo-1, epoch=167, train loss <loss>=-3.9501641880382192\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:00:52 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:00:59 INFO 139946820503360] Epoch[168] Batch[0] avg_epoch_loss=-4.064355\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:00:59 INFO 139946820503360] #quality_metric: host=algo-1, epoch=168, batch=0 train loss <loss>=-4.06435489654541\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:01:03 INFO 139946820503360] Epoch[168] Batch[5] avg_epoch_loss=-4.000955\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:01:03 INFO 139946820503360] #quality_metric: host=algo-1, epoch=168, batch=5 train loss <loss>=-4.000954866409302\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:01:03 INFO 139946820503360] Epoch[168] Batch [5]#011Speed: 74.97 samples/sec#011loss=-4.000955\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:01:07 INFO 139946820503360] processed a total of 609 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198852.2925577, \"EndTime\": 1666198867.5111964, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15218.271017074585, \"count\": 1, \"min\": 15218.271017074585, \"max\": 15218.271017074585}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:01:07 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.017379391011 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:01:07 INFO 139946820503360] #progress_metric: host=algo-1, completed 42.25 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:01:07 INFO 139946820503360] #quality_metric: host=algo-1, epoch=168, train loss <loss>=-4.209727716445923\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:01:07 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:01:14 INFO 139946820503360] Epoch[169] Batch[0] avg_epoch_loss=-3.531170\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:01:14 INFO 139946820503360] #quality_metric: host=algo-1, epoch=169, batch=0 train loss <loss>=-3.531170129776001\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:01:19 INFO 139946820503360] Epoch[169] Batch[5] avg_epoch_loss=-3.594589\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:01:19 INFO 139946820503360] #quality_metric: host=algo-1, epoch=169, batch=5 train loss <loss>=-3.5945887168248496\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:01:19 INFO 139946820503360] Epoch[169] Batch [5]#011Speed: 72.36 samples/sec#011loss=-3.594589\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:01:23 INFO 139946820503360] Epoch[169] Batch[10] avg_epoch_loss=-3.796461\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:01:23 INFO 139946820503360] #quality_metric: host=algo-1, epoch=169, batch=10 train loss <loss>=-4.038706827163696\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:01:23 INFO 139946820503360] Epoch[169] Batch [10]#011Speed: 67.71 samples/sec#011loss=-4.038707\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:01:23 INFO 139946820503360] processed a total of 668 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198867.5112772, \"EndTime\": 1666198883.7416184, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16229.880094528198, \"count\": 1, \"min\": 16229.880094528198, \"max\": 16229.880094528198}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:01:23 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=41.15839584862686 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:01:23 INFO 139946820503360] #progress_metric: host=algo-1, completed 42.5 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:01:23 INFO 139946820503360] #quality_metric: host=algo-1, epoch=169, train loss <loss>=-3.796460585160689\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:01:23 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:01:31 INFO 139946820503360] Epoch[170] Batch[0] avg_epoch_loss=-2.754117\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:01:31 INFO 139946820503360] #quality_metric: host=algo-1, epoch=170, batch=0 train loss <loss>=-2.754117250442505\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:01:35 INFO 139946820503360] Epoch[170] Batch[5] avg_epoch_loss=-3.017486\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:01:35 INFO 139946820503360] #quality_metric: host=algo-1, epoch=170, batch=5 train loss <loss>=-3.0174856185913086\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:01:35 INFO 139946820503360] Epoch[170] Batch [5]#011Speed: 72.33 samples/sec#011loss=-3.017486\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:01:38 INFO 139946820503360] processed a total of 619 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198883.7416916, \"EndTime\": 1666198898.9393027, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15197.261571884155, \"count\": 1, \"min\": 15197.261571884155, \"max\": 15197.261571884155}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:01:38 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.73077820500312 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:01:38 INFO 139946820503360] #progress_metric: host=algo-1, completed 42.75 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:01:38 INFO 139946820503360] #quality_metric: host=algo-1, epoch=170, train loss <loss>=-3.166825604438782\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:01:38 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:01:46 INFO 139946820503360] Epoch[171] Batch[0] avg_epoch_loss=-3.558872\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:01:46 INFO 139946820503360] #quality_metric: host=algo-1, epoch=171, batch=0 train loss <loss>=-3.5588724613189697\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:01:50 INFO 139946820503360] Epoch[171] Batch[5] avg_epoch_loss=-3.765669\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:01:50 INFO 139946820503360] #quality_metric: host=algo-1, epoch=171, batch=5 train loss <loss>=-3.7656692266464233\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:01:50 INFO 139946820503360] Epoch[171] Batch [5]#011Speed: 71.84 samples/sec#011loss=-3.765669\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:01:54 INFO 139946820503360] processed a total of 613 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198898.939365, \"EndTime\": 1666198914.2541811, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15314.430713653564, \"count\": 1, \"min\": 15314.430713653564, \"max\": 15314.430713653564}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:01:54 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.027291390744665 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:01:54 INFO 139946820503360] #progress_metric: host=algo-1, completed 43.0 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:01:54 INFO 139946820503360] #quality_metric: host=algo-1, epoch=171, train loss <loss>=-3.800287055969238\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:01:54 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:02:01 INFO 139946820503360] Epoch[172] Batch[0] avg_epoch_loss=-3.836038\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:02:01 INFO 139946820503360] #quality_metric: host=algo-1, epoch=172, batch=0 train loss <loss>=-3.83603835105896\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:02:06 INFO 139946820503360] Epoch[172] Batch[5] avg_epoch_loss=-3.877942\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:02:06 INFO 139946820503360] #quality_metric: host=algo-1, epoch=172, batch=5 train loss <loss>=-3.877942125002543\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:02:06 INFO 139946820503360] Epoch[172] Batch [5]#011Speed: 71.67 samples/sec#011loss=-3.877942\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:02:09 INFO 139946820503360] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198914.254263, \"EndTime\": 1666198929.7810338, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15526.397466659546, \"count\": 1, \"min\": 15526.397466659546, \"max\": 15526.397466659546}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:02:09 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.64018646842317 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:02:09 INFO 139946820503360] #progress_metric: host=algo-1, completed 43.25 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:02:09 INFO 139946820503360] #quality_metric: host=algo-1, epoch=172, train loss <loss>=-3.8810242652893066\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:02:09 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:02:17 INFO 139946820503360] Epoch[173] Batch[0] avg_epoch_loss=-3.835514\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:02:17 INFO 139946820503360] #quality_metric: host=algo-1, epoch=173, batch=0 train loss <loss>=-3.8355143070220947\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:02:21 INFO 139946820503360] Epoch[173] Batch[5] avg_epoch_loss=-3.944055\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:02:21 INFO 139946820503360] #quality_metric: host=algo-1, epoch=173, batch=5 train loss <loss>=-3.9440552790959678\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:02:21 INFO 139946820503360] Epoch[173] Batch [5]#011Speed: 71.19 samples/sec#011loss=-3.944055\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:02:25 INFO 139946820503360] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198929.7811062, \"EndTime\": 1666198945.0623362, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15280.814409255981, \"count\": 1, \"min\": 15280.814409255981, \"max\": 15280.814409255981}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:02:25 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=41.09702053093487 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:02:25 INFO 139946820503360] #progress_metric: host=algo-1, completed 43.5 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:02:25 INFO 139946820503360] #quality_metric: host=algo-1, epoch=173, train loss <loss>=-3.859770131111145\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:02:25 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:02:32 INFO 139946820503360] Epoch[174] Batch[0] avg_epoch_loss=-3.817273\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:02:32 INFO 139946820503360] #quality_metric: host=algo-1, epoch=174, batch=0 train loss <loss>=-3.817272901535034\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:02:36 INFO 139946820503360] Epoch[174] Batch[5] avg_epoch_loss=-4.017854\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:02:36 INFO 139946820503360] #quality_metric: host=algo-1, epoch=174, batch=5 train loss <loss>=-4.017853736877441\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:02:36 INFO 139946820503360] Epoch[174] Batch [5]#011Speed: 71.19 samples/sec#011loss=-4.017854\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:02:40 INFO 139946820503360] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198945.0624049, \"EndTime\": 1666198960.4414706, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15378.749132156372, \"count\": 1, \"min\": 15378.749132156372, \"max\": 15378.749132156372}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:02:40 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.11996536507899 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:02:40 INFO 139946820503360] #progress_metric: host=algo-1, completed 43.75 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:02:40 INFO 139946820503360] #quality_metric: host=algo-1, epoch=174, train loss <loss>=-4.020076823234558\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:02:40 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:02:47 INFO 139946820503360] Epoch[175] Batch[0] avg_epoch_loss=-3.272335\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:02:47 INFO 139946820503360] #quality_metric: host=algo-1, epoch=175, batch=0 train loss <loss>=-3.272334575653076\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:02:52 INFO 139946820503360] Epoch[175] Batch[5] avg_epoch_loss=-3.539084\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:02:52 INFO 139946820503360] #quality_metric: host=algo-1, epoch=175, batch=5 train loss <loss>=-3.539083997408549\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:02:52 INFO 139946820503360] Epoch[175] Batch [5]#011Speed: 73.48 samples/sec#011loss=-3.539084\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:02:55 INFO 139946820503360] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198960.4415653, \"EndTime\": 1666198975.591334, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15149.367094039917, \"count\": 1, \"min\": 15149.367094039917, \"max\": 15149.367094039917}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:02:55 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=42.24566882892171 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:02:55 INFO 139946820503360] #progress_metric: host=algo-1, completed 44.0 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:02:55 INFO 139946820503360] #quality_metric: host=algo-1, epoch=175, train loss <loss>=-3.4474891424179077\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:02:55 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:03:03 INFO 139946820503360] Epoch[176] Batch[0] avg_epoch_loss=-3.495979\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:03:03 INFO 139946820503360] #quality_metric: host=algo-1, epoch=176, batch=0 train loss <loss>=-3.4959793090820312\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:03:07 INFO 139946820503360] Epoch[176] Batch[5] avg_epoch_loss=-3.327643\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:03:07 INFO 139946820503360] #quality_metric: host=algo-1, epoch=176, batch=5 train loss <loss>=-3.327643314997355\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:03:07 INFO 139946820503360] Epoch[176] Batch [5]#011Speed: 71.64 samples/sec#011loss=-3.327643\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:03:10 INFO 139946820503360] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198975.591415, \"EndTime\": 1666198990.9208658, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15329.032897949219, \"count\": 1, \"min\": 15329.032897949219, \"max\": 15329.032897949219}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:03:10 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=41.29392237264457 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:03:10 INFO 139946820503360] #progress_metric: host=algo-1, completed 44.25 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:03:10 INFO 139946820503360] #quality_metric: host=algo-1, epoch=176, train loss <loss>=-3.5218955993652346\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:03:10 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:03:18 INFO 139946820503360] Epoch[177] Batch[0] avg_epoch_loss=-4.230394\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:03:18 INFO 139946820503360] #quality_metric: host=algo-1, epoch=177, batch=0 train loss <loss>=-4.23039436340332\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:03:22 INFO 139946820503360] Epoch[177] Batch[5] avg_epoch_loss=-4.021559\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:03:22 INFO 139946820503360] #quality_metric: host=algo-1, epoch=177, batch=5 train loss <loss>=-4.02155876159668\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:03:22 INFO 139946820503360] Epoch[177] Batch [5]#011Speed: 75.07 samples/sec#011loss=-4.021559\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:03:26 INFO 139946820503360] processed a total of 614 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666198990.920934, \"EndTime\": 1666199006.3185155, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15397.268533706665, \"count\": 1, \"min\": 15397.268533706665, \"max\": 15397.268533706665}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:03:26 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=39.876572372985436 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:03:26 INFO 139946820503360] #progress_metric: host=algo-1, completed 44.5 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:03:26 INFO 139946820503360] #quality_metric: host=algo-1, epoch=177, train loss <loss>=-4.032149791717529\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:03:26 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:03:33 INFO 139946820503360] Epoch[178] Batch[0] avg_epoch_loss=-4.064518\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:03:33 INFO 139946820503360] #quality_metric: host=algo-1, epoch=178, batch=0 train loss <loss>=-4.064518451690674\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:03:37 INFO 139946820503360] Epoch[178] Batch[5] avg_epoch_loss=-3.947785\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:03:37 INFO 139946820503360] #quality_metric: host=algo-1, epoch=178, batch=5 train loss <loss>=-3.947784503300985\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:03:37 INFO 139946820503360] Epoch[178] Batch [5]#011Speed: 70.03 samples/sec#011loss=-3.947785\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:03:42 INFO 139946820503360] Epoch[178] Batch[10] avg_epoch_loss=-4.158718\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:03:42 INFO 139946820503360] #quality_metric: host=algo-1, epoch=178, batch=10 train loss <loss>=-4.4118372917175295\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:03:42 INFO 139946820503360] Epoch[178] Batch [10]#011Speed: 65.60 samples/sec#011loss=-4.411837\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:03:42 INFO 139946820503360] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666199006.3187256, \"EndTime\": 1666199022.8702126, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16551.09453201294, \"count\": 1, \"min\": 16551.09453201294, \"max\": 16551.09453201294}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:03:42 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=39.93671260348677 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:03:42 INFO 139946820503360] #progress_metric: host=algo-1, completed 44.75 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:03:42 INFO 139946820503360] #quality_metric: host=algo-1, epoch=178, train loss <loss>=-4.158717588944868\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:03:42 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:03:49 INFO 139946820503360] Epoch[179] Batch[0] avg_epoch_loss=-3.543090\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:03:49 INFO 139946820503360] #quality_metric: host=algo-1, epoch=179, batch=0 train loss <loss>=-3.5430901050567627\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:03:54 INFO 139946820503360] Epoch[179] Batch[5] avg_epoch_loss=-3.880328\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:03:54 INFO 139946820503360] #quality_metric: host=algo-1, epoch=179, batch=5 train loss <loss>=-3.8803279797236123\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:03:54 INFO 139946820503360] Epoch[179] Batch [5]#011Speed: 75.59 samples/sec#011loss=-3.880328\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:03:58 INFO 139946820503360] Epoch[179] Batch[10] avg_epoch_loss=-4.081331\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:03:58 INFO 139946820503360] #quality_metric: host=algo-1, epoch=179, batch=10 train loss <loss>=-4.3225350856781\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:03:58 INFO 139946820503360] Epoch[179] Batch [10]#011Speed: 68.28 samples/sec#011loss=-4.322535\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:03:58 INFO 139946820503360] processed a total of 666 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666199022.8702753, \"EndTime\": 1666199038.832163, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15961.602926254272, \"count\": 1, \"min\": 15961.602926254272, \"max\": 15961.602926254272}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:03:58 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=41.72490457955833 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:03:58 INFO 139946820503360] #progress_metric: host=algo-1, completed 45.0 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:03:58 INFO 139946820503360] #quality_metric: host=algo-1, epoch=179, train loss <loss>=-4.081331209702925\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:03:58 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:04:06 INFO 139946820503360] Epoch[180] Batch[0] avg_epoch_loss=-4.318244\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:04:06 INFO 139946820503360] #quality_metric: host=algo-1, epoch=180, batch=0 train loss <loss>=-4.318244457244873\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:04:10 INFO 139946820503360] Epoch[180] Batch[5] avg_epoch_loss=-3.999609\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:04:10 INFO 139946820503360] #quality_metric: host=algo-1, epoch=180, batch=5 train loss <loss>=-3.999609112739563\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:04:10 INFO 139946820503360] Epoch[180] Batch [5]#011Speed: 71.59 samples/sec#011loss=-3.999609\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:04:14 INFO 139946820503360] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666199038.8322248, \"EndTime\": 1666199054.4295394, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15597.039222717285, \"count\": 1, \"min\": 15597.039222717285, \"max\": 15597.039222717285}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:04:14 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.7766888530771 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:04:14 INFO 139946820503360] #progress_metric: host=algo-1, completed 45.25 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:04:14 INFO 139946820503360] #quality_metric: host=algo-1, epoch=180, train loss <loss>=-3.8550361156463624\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:04:14 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:04:21 INFO 139946820503360] Epoch[181] Batch[0] avg_epoch_loss=-3.706448\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:04:21 INFO 139946820503360] #quality_metric: host=algo-1, epoch=181, batch=0 train loss <loss>=-3.7064483165740967\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:04:25 INFO 139946820503360] Epoch[181] Batch[5] avg_epoch_loss=-4.136558\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:04:25 INFO 139946820503360] #quality_metric: host=algo-1, epoch=181, batch=5 train loss <loss>=-4.136558334032695\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:04:25 INFO 139946820503360] Epoch[181] Batch [5]#011Speed: 74.56 samples/sec#011loss=-4.136558\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:04:29 INFO 139946820503360] processed a total of 599 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666199054.4296176, \"EndTime\": 1666199069.3346817, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 14904.611110687256, \"count\": 1, \"min\": 14904.611110687256, \"max\": 14904.611110687256}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:04:29 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.18862470976122 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:04:29 INFO 139946820503360] #progress_metric: host=algo-1, completed 45.5 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:04:29 INFO 139946820503360] #quality_metric: host=algo-1, epoch=181, train loss <loss>=-4.21782763004303\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:04:29 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:04:36 INFO 139946820503360] Epoch[182] Batch[0] avg_epoch_loss=-3.820199\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:04:36 INFO 139946820503360] #quality_metric: host=algo-1, epoch=182, batch=0 train loss <loss>=-3.820199489593506\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:04:41 INFO 139946820503360] Epoch[182] Batch[5] avg_epoch_loss=-4.011860\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:04:41 INFO 139946820503360] #quality_metric: host=algo-1, epoch=182, batch=5 train loss <loss>=-4.011859655380249\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:04:41 INFO 139946820503360] Epoch[182] Batch [5]#011Speed: 72.49 samples/sec#011loss=-4.011860\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:04:44 INFO 139946820503360] processed a total of 608 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666199069.334754, \"EndTime\": 1666199084.7057102, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15370.582342147827, \"count\": 1, \"min\": 15370.582342147827, \"max\": 15370.582342147827}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:04:44 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=39.555341040957885 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:04:44 INFO 139946820503360] #progress_metric: host=algo-1, completed 45.75 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:04:44 INFO 139946820503360] #quality_metric: host=algo-1, epoch=182, train loss <loss>=-3.8945303916931153\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:04:44 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:04:51 INFO 139946820503360] Epoch[183] Batch[0] avg_epoch_loss=-3.756654\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:04:51 INFO 139946820503360] #quality_metric: host=algo-1, epoch=183, batch=0 train loss <loss>=-3.7566537857055664\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:04:56 INFO 139946820503360] Epoch[183] Batch[5] avg_epoch_loss=-4.019712\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:04:56 INFO 139946820503360] #quality_metric: host=algo-1, epoch=183, batch=5 train loss <loss>=-4.019712408383687\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:04:56 INFO 139946820503360] Epoch[183] Batch [5]#011Speed: 71.53 samples/sec#011loss=-4.019712\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:05:01 INFO 139946820503360] Epoch[183] Batch[10] avg_epoch_loss=-4.237220\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:05:01 INFO 139946820503360] #quality_metric: host=algo-1, epoch=183, batch=10 train loss <loss>=-4.498230075836181\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:05:01 INFO 139946820503360] Epoch[183] Batch [10]#011Speed: 66.05 samples/sec#011loss=-4.498230\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:05:01 INFO 139946820503360] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666199084.70596, \"EndTime\": 1666199101.0912287, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16384.879112243652, \"count\": 1, \"min\": 16384.879112243652, \"max\": 16384.879112243652}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:05:01 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=39.73156174326284 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:05:01 INFO 139946820503360] #progress_metric: host=algo-1, completed 46.0 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:05:01 INFO 139946820503360] #quality_metric: host=algo-1, epoch=183, train loss <loss>=-4.237220439043912\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:05:01 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:05:08 INFO 139946820503360] Epoch[184] Batch[0] avg_epoch_loss=-3.966339\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:05:08 INFO 139946820503360] #quality_metric: host=algo-1, epoch=184, batch=0 train loss <loss>=-3.966339349746704\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:05:12 INFO 139946820503360] Epoch[184] Batch[5] avg_epoch_loss=-3.847744\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:05:12 INFO 139946820503360] #quality_metric: host=algo-1, epoch=184, batch=5 train loss <loss>=-3.847744345664978\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:05:12 INFO 139946820503360] Epoch[184] Batch [5]#011Speed: 75.14 samples/sec#011loss=-3.847744\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:05:17 INFO 139946820503360] Epoch[184] Batch[10] avg_epoch_loss=-4.173731\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:05:17 INFO 139946820503360] #quality_metric: host=algo-1, epoch=184, batch=10 train loss <loss>=-4.564914751052856\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:05:17 INFO 139946820503360] Epoch[184] Batch [10]#011Speed: 70.44 samples/sec#011loss=-4.564915\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:05:17 INFO 139946820503360] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666199101.091283, \"EndTime\": 1666199117.2787733, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16187.148094177246, \"count\": 1, \"min\": 16187.148094177246, \"max\": 16187.148094177246}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:05:17 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.525726239431705 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:05:17 INFO 139946820503360] #progress_metric: host=algo-1, completed 46.25 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:05:17 INFO 139946820503360] #quality_metric: host=algo-1, epoch=184, train loss <loss>=-4.1737308935685595\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:05:17 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:05:24 INFO 139946820503360] Epoch[185] Batch[0] avg_epoch_loss=-3.700614\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:05:24 INFO 139946820503360] #quality_metric: host=algo-1, epoch=185, batch=0 train loss <loss>=-3.7006139755249023\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:05:28 INFO 139946820503360] Epoch[185] Batch[5] avg_epoch_loss=-3.950611\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:05:28 INFO 139946820503360] #quality_metric: host=algo-1, epoch=185, batch=5 train loss <loss>=-3.9506112337112427\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:05:28 INFO 139946820503360] Epoch[185] Batch [5]#011Speed: 71.25 samples/sec#011loss=-3.950611\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:05:33 INFO 139946820503360] Epoch[185] Batch[10] avg_epoch_loss=-3.864321\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:05:33 INFO 139946820503360] #quality_metric: host=algo-1, epoch=185, batch=10 train loss <loss>=-3.760773038864136\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:05:33 INFO 139946820503360] Epoch[185] Batch [10]#011Speed: 66.45 samples/sec#011loss=-3.760773\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:05:33 INFO 139946820503360] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666199117.278846, \"EndTime\": 1666199133.7341833, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16455.07550239563, \"count\": 1, \"min\": 16455.07550239563, \"max\": 16455.07550239563}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:05:33 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=39.74435493780729 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:05:33 INFO 139946820503360] #progress_metric: host=algo-1, completed 46.5 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:05:33 INFO 139946820503360] #quality_metric: host=algo-1, epoch=185, train loss <loss>=-3.864321145144376\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:05:33 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:05:40 INFO 139946820503360] Epoch[186] Batch[0] avg_epoch_loss=-3.875624\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:05:40 INFO 139946820503360] #quality_metric: host=algo-1, epoch=186, batch=0 train loss <loss>=-3.8756237030029297\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:05:45 INFO 139946820503360] Epoch[186] Batch[5] avg_epoch_loss=-3.030234\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:05:45 INFO 139946820503360] #quality_metric: host=algo-1, epoch=186, batch=5 train loss <loss>=-3.0302344957987466\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:05:45 INFO 139946820503360] Epoch[186] Batch [5]#011Speed: 70.27 samples/sec#011loss=-3.030234\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:05:50 INFO 139946820503360] Epoch[186] Batch[10] avg_epoch_loss=-3.405332\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:05:50 INFO 139946820503360] #quality_metric: host=algo-1, epoch=186, batch=10 train loss <loss>=-3.8554484367370607\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:05:50 INFO 139946820503360] Epoch[186] Batch [10]#011Speed: 67.38 samples/sec#011loss=-3.855448\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:05:50 INFO 139946820503360] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666199133.7342477, \"EndTime\": 1666199150.1829827, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16448.46248626709, \"count\": 1, \"min\": 16448.46248626709, \"max\": 16448.46248626709}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:05:50 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=39.57797511115603 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:05:50 INFO 139946820503360] #progress_metric: host=algo-1, completed 46.75 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:05:50 INFO 139946820503360] #quality_metric: host=algo-1, epoch=186, train loss <loss>=-3.4053317416797984\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:05:50 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:05:57 INFO 139946820503360] Epoch[187] Batch[0] avg_epoch_loss=-3.388420\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:05:57 INFO 139946820503360] #quality_metric: host=algo-1, epoch=187, batch=0 train loss <loss>=-3.3884198665618896\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:06:01 INFO 139946820503360] Epoch[187] Batch[5] avg_epoch_loss=-2.977296\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:06:01 INFO 139946820503360] #quality_metric: host=algo-1, epoch=187, batch=5 train loss <loss>=-2.9772955973943076\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:06:01 INFO 139946820503360] Epoch[187] Batch [5]#011Speed: 73.75 samples/sec#011loss=-2.977296\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:06:05 INFO 139946820503360] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666199150.1830359, \"EndTime\": 1666199165.4711752, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 15287.842273712158, \"count\": 1, \"min\": 15287.842273712158, \"max\": 15287.842273712158}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:06:05 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=41.33979944240866 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:06:05 INFO 139946820503360] #progress_metric: host=algo-1, completed 47.0 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:06:05 INFO 139946820503360] #quality_metric: host=algo-1, epoch=187, train loss <loss>=-3.2393099784851076\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:06:05 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:06:12 INFO 139946820503360] Epoch[188] Batch[0] avg_epoch_loss=-4.450290\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:06:12 INFO 139946820503360] #quality_metric: host=algo-1, epoch=188, batch=0 train loss <loss>=-4.450290203094482\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:06:16 INFO 139946820503360] Epoch[188] Batch[5] avg_epoch_loss=-3.797153\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:06:16 INFO 139946820503360] #quality_metric: host=algo-1, epoch=188, batch=5 train loss <loss>=-3.797152598698934\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:06:16 INFO 139946820503360] Epoch[188] Batch [5]#011Speed: 73.83 samples/sec#011loss=-3.797153\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:06:21 INFO 139946820503360] Epoch[188] Batch[10] avg_epoch_loss=-4.033518\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:06:21 INFO 139946820503360] #quality_metric: host=algo-1, epoch=188, batch=10 train loss <loss>=-4.317157506942749\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:06:21 INFO 139946820503360] Epoch[188] Batch [10]#011Speed: 69.62 samples/sec#011loss=-4.317158\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:06:21 INFO 139946820503360] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666199165.471237, \"EndTime\": 1666199181.4946883, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16023.082256317139, \"count\": 1, \"min\": 16023.082256317139, \"max\": 16023.082256317139}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:06:21 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.691068197782656 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:06:21 INFO 139946820503360] #progress_metric: host=algo-1, completed 47.25 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:06:21 INFO 139946820503360] #quality_metric: host=algo-1, epoch=188, train loss <loss>=-4.033518466082486\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:06:21 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:06:28 INFO 139946820503360] Epoch[189] Batch[0] avg_epoch_loss=-3.551701\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:06:28 INFO 139946820503360] #quality_metric: host=algo-1, epoch=189, batch=0 train loss <loss>=-3.5517008304595947\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:06:33 INFO 139946820503360] Epoch[189] Batch[5] avg_epoch_loss=-3.535258\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:06:33 INFO 139946820503360] #quality_metric: host=algo-1, epoch=189, batch=5 train loss <loss>=-3.5352575381596885\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:06:33 INFO 139946820503360] Epoch[189] Batch [5]#011Speed: 70.08 samples/sec#011loss=-3.535258\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:06:37 INFO 139946820503360] Epoch[189] Batch[10] avg_epoch_loss=-4.012818\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:06:37 INFO 139946820503360] #quality_metric: host=algo-1, epoch=189, batch=10 train loss <loss>=-4.5858907222747805\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:06:37 INFO 139946820503360] Epoch[189] Batch [10]#011Speed: 67.51 samples/sec#011loss=-4.585891\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:06:37 INFO 139946820503360] processed a total of 663 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666199181.4947484, \"EndTime\": 1666199197.946644, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16451.597452163696, \"count\": 1, \"min\": 16451.597452163696, \"max\": 16451.597452163696}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:06:37 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=40.2998019010801 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:06:37 INFO 139946820503360] #progress_metric: host=algo-1, completed 47.5 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:06:37 INFO 139946820503360] #quality_metric: host=algo-1, epoch=189, train loss <loss>=-4.012818076393821\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:06:37 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:06:44 INFO 139946820503360] Epoch[190] Batch[0] avg_epoch_loss=-4.030847\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:06:44 INFO 139946820503360] #quality_metric: host=algo-1, epoch=190, batch=0 train loss <loss>=-4.030847072601318\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:06:49 INFO 139946820503360] Epoch[190] Batch[5] avg_epoch_loss=-3.940869\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:06:49 INFO 139946820503360] #quality_metric: host=algo-1, epoch=190, batch=5 train loss <loss>=-3.940869371096293\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:06:49 INFO 139946820503360] Epoch[190] Batch [5]#011Speed: 73.43 samples/sec#011loss=-3.940869\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:06:54 INFO 139946820503360] Epoch[190] Batch[10] avg_epoch_loss=-4.097769\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:06:54 INFO 139946820503360] #quality_metric: host=algo-1, epoch=190, batch=10 train loss <loss>=-4.286049127578735\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:06:54 INFO 139946820503360] Epoch[190] Batch [10]#011Speed: 63.19 samples/sec#011loss=-4.286049\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:06:54 INFO 139946820503360] processed a total of 677 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666199197.9467099, \"EndTime\": 1666199214.3261662, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16379.043817520142, \"count\": 1, \"min\": 16379.043817520142, \"max\": 16379.043817520142}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:06:54 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=41.333016570637604 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:06:54 INFO 139946820503360] #progress_metric: host=algo-1, completed 47.75 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:06:54 INFO 139946820503360] #quality_metric: host=algo-1, epoch=190, train loss <loss>=-4.097769260406494\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:06:54 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:07:01 INFO 139946820503360] Epoch[191] Batch[0] avg_epoch_loss=-3.417703\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:07:01 INFO 139946820503360] #quality_metric: host=algo-1, epoch=191, batch=0 train loss <loss>=-3.417703151702881\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:07:05 INFO 139946820503360] Epoch[191] Batch[5] avg_epoch_loss=-3.610536\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:07:05 INFO 139946820503360] #quality_metric: host=algo-1, epoch=191, batch=5 train loss <loss>=-3.6105358600616455\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:07:05 INFO 139946820503360] Epoch[191] Batch [5]#011Speed: 70.72 samples/sec#011loss=-3.610536\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:07:10 INFO 139946820503360] Epoch[191] Batch[10] avg_epoch_loss=-3.963736\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:07:10 INFO 139946820503360] #quality_metric: host=algo-1, epoch=191, batch=10 train loss <loss>=-4.38757586479187\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:07:10 INFO 139946820503360] Epoch[191] Batch [10]#011Speed: 67.59 samples/sec#011loss=-4.387576\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:07:10 INFO 139946820503360] processed a total of 673 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666199214.3262486, \"EndTime\": 1666199230.6110764, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16284.512042999268, \"count\": 1, \"min\": 16284.512042999268, \"max\": 16284.512042999268}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:07:10 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=41.32738780747107 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:07:10 INFO 139946820503360] #progress_metric: host=algo-1, completed 48.0 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:07:10 INFO 139946820503360] #quality_metric: host=algo-1, epoch=191, train loss <loss>=-3.9637358622117476\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:07:10 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:07:17 INFO 139946820503360] Epoch[192] Batch[0] avg_epoch_loss=-3.406363\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:07:17 INFO 139946820503360] #quality_metric: host=algo-1, epoch=192, batch=0 train loss <loss>=-3.406363010406494\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:07:22 INFO 139946820503360] Epoch[192] Batch[5] avg_epoch_loss=-3.824806\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:07:22 INFO 139946820503360] #quality_metric: host=algo-1, epoch=192, batch=5 train loss <loss>=-3.8248055378595986\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:07:22 INFO 139946820503360] Epoch[192] Batch [5]#011Speed: 70.97 samples/sec#011loss=-3.824806\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:07:26 INFO 139946820503360] Epoch[192] Batch[10] avg_epoch_loss=-3.620172\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:07:26 INFO 139946820503360] #quality_metric: host=algo-1, epoch=192, batch=10 train loss <loss>=-3.3746114015579223\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:07:26 INFO 139946820503360] Epoch[192] Batch [10]#011Speed: 69.38 samples/sec#011loss=-3.374611\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:07:26 INFO 139946820503360] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666199230.611137, \"EndTime\": 1666199246.9605048, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 16349.0469455719, \"count\": 1, \"min\": 16349.0469455719, \"max\": 16349.0469455719}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:07:26 INFO 139946820503360] #throughput_metric: host=algo-1, train throughput=39.45163787394223 records/second\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:07:26 INFO 139946820503360] #progress_metric: host=algo-1, completed 48.25 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:07:26 INFO 139946820503360] #quality_metric: host=algo-1, epoch=192, train loss <loss>=-3.620171839540655\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:07:26 INFO 139946820503360] loss did not improve\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:07:26 INFO 139946820503360] Loading parameters from best epoch (152)\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666199246.960564, \"EndTime\": 1666199247.0508542, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.deserialize.time\": {\"sum\": 89.9648666381836, \"count\": 1, \"min\": 89.9648666381836, \"max\": 89.9648666381836}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:07:27 INFO 139946820503360] stopping training now\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:07:27 INFO 139946820503360] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:07:27 INFO 139946820503360] Final loss: -4.459147119522095 (occurred at epoch 152)\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:07:27 INFO 139946820503360] #quality_metric: host=algo-1, train final_loss <loss>=-4.459147119522095\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.8/site-packages/algorithm/run_worker.py:347: DeprecationWarning: The 'warn' function is deprecated, use 'warning' instead\n",
      "  logging.warn(\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:07:27 WARNING 139946820503360] You are using large values for `context_length` and/or `prediction_length`. The following step may take some time. If the step crashes, use an instance with more memory or reduce these two parameters.\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:07:27 INFO 139946820503360] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:07:27 WARNING 139946820503360] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:07:27 INFO 139946820503360] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666199247.050926, \"EndTime\": 1666199252.3815987, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 5329.742670059204, \"count\": 1, \"min\": 5329.742670059204, \"max\": 5329.742670059204}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:07:33 INFO 139946820503360] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666199252.3816655, \"EndTime\": 1666199253.2812433, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"finalize.time\": {\"sum\": 6229.435682296753, \"count\": 1, \"min\": 6229.435682296753, \"max\": 6229.435682296753}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:07:33 INFO 139946820503360] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:07:33 INFO 139946820503360] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666199253.2813025, \"EndTime\": 1666199253.397996, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.serialize.time\": {\"sum\": 116.65511131286621, \"count\": 1, \"min\": 116.65511131286621, \"max\": 116.65511131286621}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:07:33 INFO 139946820503360] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:07:33 INFO 139946820503360] #memory_usage::<batchbuffer> = 83.69873046875 mb\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:07:33 INFO 139946820503360] Evaluating model accuracy on testset using 100 samples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666199253.3980434, \"EndTime\": 1666199253.4035504, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.bind.time\": {\"sum\": 0.025987625122070312, \"count\": 1, \"min\": 0.025987625122070312, \"max\": 0.025987625122070312}}}\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.8/site-packages/numpy/ma/core.py:2794: UserWarning: Warning: converting a masked element to nan.\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666199253.4035978, \"EndTime\": 1666199354.7319345, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.score.time\": {\"sum\": 101328.4101486206, \"count\": 1, \"min\": 101328.4101486206, \"max\": 101328.4101486206}}}\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:09:14 INFO 139946820503360] #test_score (algo-1, RMSE): 1.2466856959864956\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:09:14 INFO 139946820503360] #test_score (algo-1, mean_absolute_QuantileLoss): 59924.505764324516\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:09:14 INFO 139946820503360] #test_score (algo-1, mean_wQuantileLoss): 1.000542739670148\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:09:14 INFO 139946820503360] #test_score (algo-1, wQuantileLoss[0.1]): 0.20091203062302534\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:09:14 INFO 139946820503360] #test_score (algo-1, wQuantileLoss[0.2]): 0.40111006030543794\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:09:14 INFO 139946820503360] #test_score (algo-1, wQuantileLoss[0.3]): 0.6011151921433796\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:09:14 INFO 139946820503360] #test_score (algo-1, wQuantileLoss[0.4]): 0.8009969021696943\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:09:14 INFO 139946820503360] #test_score (algo-1, wQuantileLoss[0.5]): 1.0008156214438477\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:09:14 INFO 139946820503360] #test_score (algo-1, wQuantileLoss[0.6]): 1.2006446410813532\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:09:14 INFO 139946820503360] #test_score (algo-1, wQuantileLoss[0.7]): 1.4003967124746162\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:09:14 INFO 139946820503360] #test_score (algo-1, wQuantileLoss[0.8]): 1.599928056774745\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:09:14 INFO 139946820503360] #test_score (algo-1, wQuantileLoss[0.9]): 1.7989654400152346\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:09:14 INFO 139946820503360] #quality_metric: host=algo-1, test RMSE <loss>=1.2466856959864956\u001b[0m\n",
      "\u001b[34m[10/19/2022 17:09:14 INFO 139946820503360] #quality_metric: host=algo-1, test mean_wQuantileLoss <loss>=1.000542739670148\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1666199354.7320042, \"EndTime\": 1666199354.9485602, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"setuptime\": {\"sum\": 8.779764175415039, \"count\": 1, \"min\": 8.779764175415039, \"max\": 8.779764175415039}, \"totaltime\": {\"sum\": 3169650.083065033, \"count\": 1, \"min\": 3169650.083065033, \"max\": 3169650.083065033}}}\u001b[0m\n",
      "\n",
      "2022-10-19 17:09:20 Uploading - Uploading generated training model\n",
      "2022-10-19 17:09:41 Completed - Training job completed\n",
      "ProfilerReport-1666195902: NoIssuesFound\n",
      "Training seconds: 3368\n",
      "Billable seconds: 3368\n",
      "CPU times: user 7.18 s, sys: 720 ms, total: 7.9 s\n",
      "Wall time: 58min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_channels = {\"train\": \"{}/train_start/\".format(s3_data_path), \"test\": \"{}/test_start/\".format(s3_data_path)}\n",
    "\n",
    "estimator.fit(inputs = data_channels, wait = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231addd0-680d-4b4e-b39c-05c789ad1b2a",
   "metadata": {},
   "source": [
    "**Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5bc04415-f0b6-432c-a782-cac1f899fb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepar_model_utils import DeepARPredictor\n",
    "from deepar_model_utils import get_station_data\n",
    "from deepar_model_utils import prep_station_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e393c0dc-17fc-44e0-8871-314a8c4191b8",
   "metadata": {},
   "source": [
    "Reference for following code: https://stackoverflow.com/questions/56255154/how-to-use-a-pretrained-model-from-s3-to-predict-some-data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7a371a-9762-4a9c-8c95-5902dee2143a",
   "metadata": {},
   "source": [
    "TO DO:\n",
    "- upload data to s3 bucket\n",
    "- test to make sure series to predict is correct\n",
    "- predict + plot predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63b658f5-ec6e-4565-9d9d-aa16880d69d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_start_model = sagemaker.model.Model(\n",
    "    model_data = \"{}/deepar-poc-start-2022-10-19-16-11-42-997/output/model.tar.gz\".format(s3_output_path),\n",
    "    image_uri = image_name,\n",
    "    role = role)\n",
    "\n",
    "#predictor = trip_start_model.deploy(initial_instance_count = 1, instance_type = \"ml.m5.large\", predictor_cls = DeepARPredictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db456ed4-4c3b-4387-acc2-fcb436aa664b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_file = \"s3://{}/model_trips_start_station_20208029_20220831.csv\".format(s3_bucket)\n",
    "\n",
    "trips_start = pd.read_csv(start_file, parse_dates = True)\n",
    "trips_start.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2f822f-58ef-4528-b172-e083bbd24a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "station = 177\n",
    "freq = \"15min\" # group and sum trips by a set increment\n",
    "max_date = \"2022-08-28 23:45:00\" # make sure all series end at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cb81d6-6edb-4eec-9e2d-4a5bcfaf3eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_start_all_group = prep_station_data(trips_start, \"start station id\", \"starttime\")\n",
    "print(sum(trips_start_all_group[\"size\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1bd7c2-89a5-4213-a747-566bb18acfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.predict(ts = get_station_data(trips_start_all_group, \"start station id\", \"starttime\", station, freq, max_date), quantiles = [0.10, 0.5, 0.90]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c151d835-66fd-4be5-8b04-75911d0d202d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
